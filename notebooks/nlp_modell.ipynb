{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Datei geladen: G:/Meine Ablage/reddit/reddit_posts_original.csv\n",
      "🔹 Spalten: ['post_id', 'crypto', 'search_term', 'subreddit', 'title', 'author', 'date', 'time', 'score', 'num_comments', 'selftext']\n",
      "post_id         object\n",
      "crypto          object\n",
      "search_term     object\n",
      "subreddit       object\n",
      "title           object\n",
      "author          object\n",
      "date            object\n",
      "time            object\n",
      "score            int64\n",
      "num_comments     int64\n",
      "selftext        object\n",
      "dtype: object\n",
      "   post_id   crypto search_term       subreddit  \\\n",
      "0  1j0dz4r  Bitcoin     bitcoin  CryptoCurrency   \n",
      "1  1j0dghm  Bitcoin     bitcoin  CryptoCurrency   \n",
      "2  1j0cx0g  Bitcoin     bitcoin  CryptoCurrency   \n",
      "3  1j0cnlh  Bitcoin     bitcoin  CryptoCurrency   \n",
      "4  1j08i0y  Bitcoin     bitcoin  CryptoCurrency   \n",
      "\n",
      "                                               title               author  \\\n",
      "0  BlackRock Adds Its Bitcoin ETF to Model Portfo...             diwalost   \n",
      "1  x-post: As the free-float of coins is low, is ...        3fkgf9fmd980e   \n",
      "2  Companies Building on the Bitcoin Lightning Ne...            kirtash93   \n",
      "3  Michael Saylor Says 'Sell A Kidney, But Keep T...                KIG45   \n",
      "4  Teaching Bitcoin on Wall Street at $30 in 2012...  rizzobitcoinhistory   \n",
      "\n",
      "         date      time  score  num_comments selftext  \n",
      "0  2025-02-28  18:01:02     13             8      NaN  \n",
      "1  2025-02-28  17:39:22      0             1      NaN  \n",
      "2  2025-02-28  17:16:50     63            33      NaN  \n",
      "3  2025-02-28  17:06:18     34            44      NaN  \n",
      "4  2025-02-28  14:09:15    426            23      NaN  \n",
      "\n",
      "📌 Datei geladen: G:/Meine Ablage/reddit/reddit_comments_original.csv\n",
      "🔹 Spalten: ['post_id', 'comment_id', 'author', 'date', 'time', 'score', 'selftext']\n",
      "post_id       object\n",
      "comment_id    object\n",
      "author        object\n",
      "date          object\n",
      "time          object\n",
      "score          int64\n",
      "selftext      object\n",
      "dtype: object\n",
      "   post_id comment_id         author        date      time  score  \\\n",
      "0  1j0dz4r    mfafvxb       partymsl  2025-02-28  18:04:39      6   \n",
      "1  1j0dz4r    mfah0xz  coinfeeds-bot  2025-02-28  18:10:08      2   \n",
      "2  1j0dz4r    mfb16ty     DonasAskan  2025-02-28  19:43:55      1   \n",
      "3  1j0dz4r    mfakfdl       Deeujian  2025-02-28  18:26:05     -2   \n",
      "4  1j0dz4r    mfag4zu       diwalost  2025-02-28  18:05:51      1   \n",
      "\n",
      "                                            selftext  \n",
      "0      So... who said institutions are capitulating?  \n",
      "1  tldr; BlackRock, the world's largest asset man...  \n",
      "2             How does this only have 3 upvotes lol?  \n",
      "3  1-2% is nothing for Blackrock but they decided...  \n",
      "4                            They are not, retail is  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import os\n",
    "import psaw as ps\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from psaw import PushshiftAPI\n",
    "from praw.exceptions import APIException\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# 📌 Pfade zu den CSV-Dateien\n",
    "DRIVE_PATH = \"G:/Meine Ablage/reddit/\"\n",
    "POSTS_CSV = os.path.join(DRIVE_PATH, \"reddit_posts_original.csv\")\n",
    "COMMENTS_CSV = os.path.join(DRIVE_PATH, \"reddit_comments_original.csv\")\n",
    "\n",
    "# 🔹 **Funktion zum Laden der CSV-Dateien**\n",
    "def load_csv(filepath):\n",
    "    \"\"\"Lädt eine CSV-Datei mit `|` als Trennzeichen und Debugging-Infos\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"❌ Datei nicht gefunden: {filepath}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.read_csv(filepath, sep=\"|\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "\n",
    "    print(f\"\\n📌 Datei geladen: {filepath}\")\n",
    "    print(f\"🔹 Spalten: {df.columns.tolist()}\")\n",
    "    print(df.dtypes)\n",
    "    print(df.head())\n",
    "\n",
    "    return df\n",
    "\n",
    "# 📌 **Daten laden**\n",
    "df_posts = load_csv(POSTS_CSV)\n",
    "df_comments = load_csv(COMMENTS_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df_posts, df_comments, comment_threshold=500, min_length=5):\n",
    "    # 1. Duplikate entfernen\n",
    "    df_posts = df_posts.drop_duplicates(subset=[\"post_id\"])\n",
    "    df_comments = df_comments.drop_duplicates(subset=[\"comment_id\"])\n",
    "    \n",
    "    # 2. Fehlende Werte behandeln\n",
    "    df_posts['selftext'] = df_posts['selftext'].fillna('')  # Fehlende Posttexte auffüllen\n",
    "    df_comments['selftext'] = df_comments['selftext'].fillna('')  # Fehlende Kommentare auffüllen\n",
    "\n",
    "    # 3. Entferne Nutzer (Bots) mit übermäßigen Kommentaren\n",
    "    comment_counts = df_comments[\"author\"].value_counts()\n",
    "    frequent_users = comment_counts[comment_counts > comment_threshold].index  # Nutzer über Grenze\n",
    "    df_comments = df_comments[~df_comments[\"author\"].isin(frequent_users)]\n",
    "\n",
    "    # 4. Entferne zu kurze Kommentare\n",
    "    df_comments = df_comments[df_comments['selftext'].str.len() >= min_length]\n",
    "\n",
    "    print(f\"✅ Daten bereinigt: {df_comments.shape[0]} Kommentare übrig (nach Spam-Filter & Länge > {min_length}).\")\n",
    "\n",
    "    return df_posts, df_comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Daten bereinigt: 312146 Kommentare übrig (nach Spam-Filter & Länge > 5).\n",
      "Bereinigte Posts: 9175\n",
      "Bereinigte Kommentare: 312146\n"
     ]
    }
   ],
   "source": [
    "# Bereinigen der Daten\n",
    "df_posts_clean, df_comments_clean = clean_data(df_posts, df_comments, comment_threshold=500, min_length=5)\n",
    "\n",
    "\n",
    "# Überprüfen, wie viele Einträge übrig sind\n",
    "print(f\"Bereinigte Posts: {len(df_posts_clean)}\")\n",
    "print(f\"Bereinigte Kommentare: {len(df_comments_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Verwende Gerät: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔍 Analysiere Sentiments: 100%|██████████| 287/287 [02:41<00:00,  1.77it/s]\n",
      "🔍 Analysiere Sentiments: 100%|██████████| 9755/9755 [36:15<00:00,  4.48it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sentiment-Analyse abgeschlossen: 9175 Posts & 312146 Kommentare bewertet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# GPU nutzen, falls verfügbar sonst weglassen\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🚀 Verwende Gerät: {device}\")\n",
    "\n",
    "# 🔹 CryptoBERT-Modell laden\n",
    "MODEL_NAME = \"ElKulako/cryptobert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()  # Setzt das Modell in den Evaluationsmodus\n",
    "\n",
    "# 🔹 Funktion zur Sentiment-Analyse (Optimiert für Batch-Prozesse)\n",
    "def analyze_sentiment_batch(texts, batch_size=32):\n",
    "    \"\"\"Effiziente GPU-gestützte Sentiment-Analyse mit CryptoBERT für eine Liste von Texten.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Ersetze leere Einträge durch \"neutral\"\n",
    "    texts = [t if isinstance(t, str) and t.strip() != \"\" else \"neutral\" for t in texts]\n",
    "\n",
    "    # Batchweise Verarbeitung\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"🔍 Analysiere Sentiments\"):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "\n",
    "        # Tokenisierung (mit Padding für Performance)\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, max_length=512, padding=True).to(device)\n",
    "\n",
    "        # Vorhersage mit CryptoBERT\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        scores = F.softmax(outputs.logits, dim=1)\n",
    "        labels = [\"bearish\", \"neutral\", \"bullish\"] \n",
    "\n",
    "        # Ergebnisse speichern\n",
    "        for i in range(len(batch_texts)):\n",
    "            sentiment = labels[torch.argmax(scores[i]).item()]\n",
    "            confidence = scores[i].max().item()\n",
    "            results.append((sentiment, confidence))\n",
    "\n",
    "    return results\n",
    "\n",
    "# 🔹 Sentiment für **Posts** berechnen\n",
    "tqdm.pandas()  # Fortschrittsanzeige aktivieren\n",
    "df_posts_clean[\"full_text\"] = df_posts_clean[\"title\"] + \" \" + df_posts_clean[\"selftext\"].fillna(\"\")\n",
    "df_posts_clean[\"full_text\"] = df_posts_clean[\"full_text\"].str.strip()\n",
    "\n",
    "# Spalten 'title' und 'selftext' droppen\n",
    "df_posts_clean.drop(columns=[\"title\", \"selftext\"], inplace=True)\n",
    "\n",
    "df_posts_clean[[\"sentiment\", \"sentiment_confidence\"]] = pd.DataFrame(\n",
    "    analyze_sentiment_batch(df_posts_clean[\"full_text\"].tolist()), index=df_posts_clean.index\n",
    ")\n",
    "\n",
    "# 🔹 Sentiment für **Kommentare** berechnen\n",
    "df_comments_clean[\"full_text\"] = df_comments_clean[\"selftext\"].fillna(\"\")\n",
    "\n",
    "# Spalte 'selftext' droppen\n",
    "df_comments_clean.drop(columns=[\"selftext\"], inplace=True)\n",
    "\n",
    "df_comments_clean[[\"sentiment\", \"sentiment_confidence\"]] = pd.DataFrame(\n",
    "    analyze_sentiment_batch(df_comments_clean[\"full_text\"].tolist()), index=df_comments_clean.index\n",
    ")\n",
    "\n",
    "# 🔹 Ergebnisse anzeigen\n",
    "print(f\"✅ Sentiment-Analyse abgeschlossen: {len(df_posts_clean)} Posts & {len(df_comments_clean)} Kommentare bewertet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = df_posts_clean.copy()\n",
    "comments = df_comments_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Merged Dataset: 321321 Einträge (Posts + Kommentare)\n",
      "   post_id comment_id  type        date      time   crypto search_term  \\\n",
      "0  1j0dz4r       None  post  2025-02-28  18:01:02  Bitcoin     bitcoin   \n",
      "1  1j0dghm       None  post  2025-02-28  17:39:22  Bitcoin     bitcoin   \n",
      "2  1j0cx0g       None  post  2025-02-28  17:16:50  Bitcoin     bitcoin   \n",
      "3  1j0cnlh       None  post  2025-02-28  17:06:18  Bitcoin     bitcoin   \n",
      "4  1j08i0y       None  post  2025-02-28  14:09:15  Bitcoin     bitcoin   \n",
      "\n",
      "        subreddit               author  \\\n",
      "0  CryptoCurrency             diwalost   \n",
      "1  CryptoCurrency        3fkgf9fmd980e   \n",
      "2  CryptoCurrency            kirtash93   \n",
      "3  CryptoCurrency                KIG45   \n",
      "4  CryptoCurrency  rizzobitcoinhistory   \n",
      "\n",
      "                                           full_text  score sentiment  \\\n",
      "0  BlackRock Adds Its Bitcoin ETF to Model Portfo...     13   neutral   \n",
      "1  x-post: As the free-float of coins is low, is ...      0   bullish   \n",
      "2  Companies Building on the Bitcoin Lightning Ne...     63   bullish   \n",
      "3  Michael Saylor Says 'Sell A Kidney, But Keep T...     34   neutral   \n",
      "4  Teaching Bitcoin on Wall Street at $30 in 2012...    426   bullish   \n",
      "\n",
      "   sentiment_confidence  \n",
      "0              0.602603  \n",
      "1              0.520766  \n",
      "2              0.669048  \n",
      "3              0.644649  \n",
      "4              0.803598  \n"
     ]
    }
   ],
   "source": [
    "# 🔹 **Relevante Spalten für den Merge**\n",
    "posts = posts[[\"post_id\", \"crypto\", \"search_term\", \"subreddit\", \"author\", \"date\", \"time\", \"score\", \"full_text\", \"sentiment\", \"sentiment_confidence\"]]\n",
    "comments = comments[[\"post_id\", \"comment_id\", \"author\", \"date\", \"time\", \"score\", \"full_text\", \"sentiment\", \"sentiment_confidence\"]]\n",
    "\n",
    "# 🔹 **Kommentare erben `crypto`, `search_term` und `subreddit` vom Post**\n",
    "comments = comments.merge(df_posts[[\"post_id\", \"crypto\", \"search_term\", \"subreddit\"]], on=\"post_id\", how=\"left\")\n",
    "\n",
    "# 🔹 `type`-Spalte für Unterscheidung hinzufügen\n",
    "posts[\"comment_id\"] = None  # Posts haben keine comment_id\n",
    "posts[\"type\"] = \"post\"\n",
    "comments[\"type\"] = \"comment\"\n",
    "\n",
    "# 🔹 **Gemeinsame Spalten für den Merge**\n",
    "common_columns = [\n",
    "    \"post_id\", \"comment_id\", \"type\", \"date\", \"time\", \"crypto\",\n",
    "    \"search_term\", \"subreddit\", \"author\",\"full_text\",\"score\", \"sentiment\", \"sentiment_confidence\",\n",
    "]\n",
    "\n",
    "# 🔹 **Merging der Daten (Posts + Kommentare)**\n",
    "df_merged = pd.concat([posts[common_columns], comments[common_columns]], ignore_index=True)\n",
    "\n",
    "# 🔍 Debugging: Überprüfung der Größe\n",
    "print(f\"📌 Merged Dataset: {df_merged.shape[0]} Einträge (Posts + Kommentare)\")\n",
    "\n",
    "# 🔹 Überprüfen, ob alles korrekt normalisiert wurde\n",
    "print(df_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>score</th>\n",
       "      <th>full_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1j0dz4r</td>\n",
       "      <td>mfb16ty</td>\n",
       "      <td>DonasAskan</td>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>19:43:55</td>\n",
       "      <td>1</td>\n",
       "      <td>How does this only have 3 upvotes lol?</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.816071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1j0dz4r</td>\n",
       "      <td>mfakfdl</td>\n",
       "      <td>Deeujian</td>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>18:26:05</td>\n",
       "      <td>-2</td>\n",
       "      <td>1-2% is nothing for Blackrock but they decided...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.578827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1j0dz4r</td>\n",
       "      <td>mfag4zu</td>\n",
       "      <td>diwalost</td>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>18:05:51</td>\n",
       "      <td>1</td>\n",
       "      <td>They are not, retail is</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.691663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1j0dz4r</td>\n",
       "      <td>mfbrf7n</td>\n",
       "      <td>Bear-Bull-Pig</td>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>21:46:12</td>\n",
       "      <td>1</td>\n",
       "      <td>People don't like Blackrock</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.493956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1j0dz4r</td>\n",
       "      <td>mfao0qa</td>\n",
       "      <td>diwalost</td>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>18:42:51</td>\n",
       "      <td>1</td>\n",
       "      <td>They filled for ETF when we were in bear marke...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.619266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id comment_id         author        date      time  score  \\\n",
       "2  1j0dz4r    mfb16ty     DonasAskan  2025-02-28  19:43:55      1   \n",
       "3  1j0dz4r    mfakfdl       Deeujian  2025-02-28  18:26:05     -2   \n",
       "4  1j0dz4r    mfag4zu       diwalost  2025-02-28  18:05:51      1   \n",
       "5  1j0dz4r    mfbrf7n  Bear-Bull-Pig  2025-02-28  21:46:12      1   \n",
       "6  1j0dz4r    mfao0qa       diwalost  2025-02-28  18:42:51      1   \n",
       "\n",
       "                                           full_text sentiment  \\\n",
       "2             How does this only have 3 upvotes lol?   neutral   \n",
       "3  1-2% is nothing for Blackrock but they decided...   neutral   \n",
       "4                            They are not, retail is   neutral   \n",
       "5                        People don't like Blackrock   neutral   \n",
       "6  They filled for ETF when we were in bear marke...   neutral   \n",
       "\n",
       "   sentiment_confidence  \n",
       "2              0.816071  \n",
       "3              0.578827  \n",
       "4              0.691663  \n",
       "5              0.493956  \n",
       "6              0.619266  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setze den Pfad zu deinem Google Drive Ordner\n",
    "DRIVE_PATH = \"G:/Meine Ablage/reddit/\"\n",
    "POSTS_CSV = os.path.join(DRIVE_PATH, \"reddit_posts.csv\")\n",
    "COMMENTS_CSV = os.path.join(DRIVE_PATH, \"reddit_comments.csv\")\n",
    "MERGED_CSV = os.path.join(DRIVE_PATH, \"reddit_merged.csv\")\n",
    "ORIGINAL_POSTS_CSV = os.path.join(DRIVE_PATH, \"reddit_posts_original.csv\")\n",
    "ORIGINAL_COMMENTS_CSV = os.path.join(DRIVE_PATH, \"reddit_comments_original.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(df_new, filename, key_column):\n",
    "    \"\"\"Hängt neue Daten an eine bestehende CSV an & entfernt Duplikate.\"\"\"\n",
    "    file_path = os.path.join(DRIVE_PATH, filename)\n",
    "\n",
    "    try:\n",
    "        # Falls Datei existiert, alte Daten einlesen\n",
    "        if os.path.exists(file_path):\n",
    "            df_existing = pd.read_csv(file_path, sep=\"|\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "            \n",
    "            # 🔹 Daten zusammenführen & Duplikate nach `key_column` entfernen (neuere Werte behalten)\n",
    "            df_combined = pd.concat([df_existing, df_new], ignore_index=True).drop_duplicates(subset=[key_column], keep=\"last\")\n",
    "        else:\n",
    "            df_combined = df_new  # Falls keine Datei existiert, neue Daten direkt nutzen\n",
    "\n",
    "        # 🔹 CSV speichern\n",
    "        df_combined.to_csv(\n",
    "            file_path,\n",
    "            index=False,\n",
    "            sep=\"|\",\n",
    "            encoding=\"utf-8-sig\",\n",
    "            lineterminator=\"\\n\"\n",
    "        )\n",
    "        print(f\"✅ Datei erfolgreich aktualisiert: {file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Speichern der Datei {filename}: {e}\")\n",
    "\n",
    "def export_to_drive(df_posts_clean, df_comments_clean, df_merged,df_posts, df_comments):\n",
    "    \"\"\"Speichert Posts, Kommentare & die gemergte Datei mit Duplikat-Prüfung.\"\"\"\n",
    "    try:\n",
    "        append_to_csv(df_posts_clean, \"reddit_posts.csv\", key_column=\"post_id\")\n",
    "        append_to_csv(df_comments_clean, \"reddit_comments.csv\", key_column=\"comment_id\")\n",
    "        append_to_csv(df_merged, \"reddit_merged.csv\", key_column=\"comment_id\")  # Falls Kommentare entscheidend sind\n",
    "        append_to_csv(df_posts, \"reddit_posts_original.csv\", key_column=\"post_id\")\n",
    "        append_to_csv(df_comments, \"reddit_comments_original.csv\", key_column=\"comment_id\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Export: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Datei erfolgreich aktualisiert: G:/Meine Ablage/reddit/reddit_posts.csv\n",
      "✅ Datei erfolgreich aktualisiert: G:/Meine Ablage/reddit/reddit_comments.csv\n",
      "✅ Datei erfolgreich aktualisiert: G:/Meine Ablage/reddit/reddit_merged.csv\n",
      "✅ Datei erfolgreich aktualisiert: G:/Meine Ablage/reddit/reddit_posts_original.csv\n",
      "✅ Datei erfolgreich aktualisiert: G:/Meine Ablage/reddit/reddit_comments_original.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Export-Funktion aufrufen\n",
    "export_to_drive(df_posts_clean, df_comments_clean, df_merged,df_posts, df_comments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crypto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Scraper f√ºr vergangene Daten "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import psaw as ps\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".env geladen? True\n"
     ]
    }
   ],
   "source": [
    "# Lade die .env-Datei\n",
    "dotenv_loaded = load_dotenv(\"zugang_reddit.env\")  # Falls die Datei anders hei√üt, anpassen\n",
    "# Pr√ºfe, ob die Datei geladen wurde\n",
    "print(f\".env geladen? {dotenv_loaded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit API erfolgreich verbunden!\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"USER_AGENT\")\n",
    ")\n",
    "\n",
    "print(\"Reddit API erfolgreich verbunden!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: [AMA] Giveaway with Portal to Bitcoin: Making bridges, wrapped coins, and external custody obsolete - Feb 12, Score: 30\n",
      "Title: Daily Crypto Discussion - February 13, 2025 (GMT+0), Score: 11\n",
      "Title: Poor vs Rich vs Crypto Bro, Score: 866\n",
      "Title: 6 men kidnapped Chicago family, forcing $15M crypto transfer, Score: 168\n",
      "Title: No, no. He's got a point, Score: 3488\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    subreddit = reddit.subreddit(\"CryptoCurrency\")\n",
    "    for post in subreddit.hot(limit=5):\n",
    "        print(f\"Title: {post.title}, Score: {post.score}\")\n",
    "except Exception as e:\n",
    "    print(f\"Fehler beim Abrufen der Subreddit-Daten: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cryptos und Subreddits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_terms = {\n",
    "    # üîπ Top Coins\n",
    "    \"Bitcoin\": [\"bitcoin\", \"btc\", \"sats\", \"bitcoin network\"],\n",
    "    \"Ethereum\": [\"ethereum\", \"eth\", \"ether\", \"ethereum 2.0\", \"eth 2.0\"],\n",
    "    \"Wrapped Ethereum\": [\"wrapped ethereum\", \"weth\"],\n",
    "    \"Solana\": [\"solana\", \"sol\", \"sol coin\"],\n",
    "    \"Avalanche\": [\"avalanche\", \"avax\"],\n",
    "    \"Polkadot\": [\"polkadot\", \"dot\"],\n",
    "    \"Near Protocol\": [\"near protocol\", \"near\"],\n",
    "    \"Polygon\": [\"polygon\", \"matic\"],\n",
    "    \"XRP\": [\"xrp\", \"ripple\"],\n",
    "    \"Cardano\": [\"cardano\", \"ada\"],\n",
    "    \"Cronos\": [\"cronos\", \"cro\"],\n",
    "    \"Vulcan Forged PYR\": [\"vulcan forged\", \"pyr\"],\n",
    "    \"Chiliz\": [\"chiliz\", \"chz\"],\n",
    "    \"Illuvium\": [\"illuvium\", \"ilv\"],\n",
    "    \"Ronin\": [\"ronin\", \"ron\"],\n",
    "    \"Band Protocol\": [\"band protocol\", \"band\"],\n",
    "    \"Optimism\": [\"optimism\", \"op\"],\n",
    "    \"Celestia\": [\"celestia\", \"tia\"],\n",
    "    \"Numerai\": [\"numerai\", \"nmr\"],\n",
    "    \"Aethir\": [\"aethir\", \"ath\"],\n",
    "    \"Sui\": [\"sui\"],\n",
    "    \"Hyperliquid\": [\"hyperliquid\", \"hyp\"],\n",
    "    \"Robinhood Coin\": [\"robinhood\", \"hood\"],\n",
    "    \"Trump Coin\": [\"trump coin\"],\n",
    "    \"USD Coin\": [\"usd coin\", \"usdc\"],\n",
    "    \"Binance Coin\": [\"binance\", \"bnb\"],\n",
    "    \"Litecoin\": [\"litecoin\", \"ltc\"],\n",
    "    \"Dogecoin\": [\"dogecoin\", \"doge\"],\n",
    "    \"Tron\": [\"tron\", \"trx\"],\n",
    "    \"Aave\": [\"aave\"],\n",
    "    \"Hedera\": [\"hedera\", \"hbar\"],\n",
    "    \"Filecoin\": [\"filecoin\", \"fil\"],\n",
    "    \"Cosmos\": [\"cosmos\", \"atom\"],\n",
    "    \"Gala\": [\"gala\"],\n",
    "    \"The Sandbox\": [\"sandbox\", \"sand\"],\n",
    "    \"Audius\": [\"audius\", \"audio\"],\n",
    "    \"Render\": [\"render\", \"rndr\"],\n",
    "    \"Kusama\": [\"kusama\", \"ksm\"],\n",
    "    \"VeChain\": [\"vechain\", \"vet\"],\n",
    "    \"Chainlink\": [\"chainlink\", \"link\"],\n",
    "    \"Berachain\": [\"berachain\", \"bera\"],\n",
    "    \"TestCoin\": [\"testcoin\", \"test\"],\n",
    "\n",
    "    # üîπ Meme-Coins\n",
    "    \"Shiba Inu\": [\"shiba inu\", \"shib\"],\n",
    "    \"Pepe\": [\"pepe\"],\n",
    "    \"Floki Inu\": [\"floki inu\", \"floki\"],\n",
    "    \"Bonk\": [\"bonk\"],\n",
    "    \"Wojak\": [\"wojak\"],\n",
    "    \"Mog Coin\": [\"mog\"],\n",
    "    \"Doge Killer (Leash)\": [\"leash\"],\n",
    "    \"Baby Doge Coin\": [\"baby doge\", \"babydoge\"],\n",
    "    \"Degen\": [\"degen\"],\n",
    "    \"Toshi\": [\"toshi\"],\n",
    "    \"Fartcoin\": [\"fartcoin\"],\n",
    "    \"Banana\": [\"banana\"],\n",
    "    \"Kabosu\": [\"kabosu\"],\n",
    "    \"Husky\": [\"husky\"],\n",
    "    \"Samoyedcoin\": [\"samoyedcoin\", \"samo\"],\n",
    "    \"Milkbag\": [\"milkbag\"],\n",
    "\n",
    "    # üîπ New Coins\n",
    "    \"Arbitrum\": [\"arbitrum\", \"arb\"],\n",
    "    \"Starknet\": [\"starknet\", \"strk\"],\n",
    "    \"Injective Protocol\": [\"injective\", \"inj\"],\n",
    "    \"Sei Network\": [\"sei\"],\n",
    "    \"Aptos\": [\"aptos\", \"apt\"],\n",
    "    \"EigenLayer\": [\"eigenlayer\", \"eigen\"],\n",
    "    \"Mantle\": [\"mantle\", \"mnt\"],\n",
    "    \"Immutable X\": [\"immutable x\", \"imx\"],\n",
    "    \"Ondo Finance\": [\"ondo\"],\n",
    "    \"Worldcoin\": [\"worldcoin\", \"wld\"],\n",
    "    \"Aerodrome\": [\"aerodrome\", \"aero\"],\n",
    "    \"Jupiter\": [\"jupiter\", \"jup\"],\n",
    "    \"THORChain\": [\"thorchain\", \"rune\"],\n",
    "    \"Pendle\": [\"pendle\"],\n",
    "    \"Kujira\": [\"kujira\", \"kuji\"],\n",
    "    \"Noble\": [\"noble\"],\n",
    "    \"Stride\": [\"stride\", \"strd\"],\n",
    "    \"Dymension\": [\"dymension\", \"dym\"],\n",
    "    \"Seamless Protocol\": [\"seamless\", \"seam\"],\n",
    "    \"Blast\": [\"blast\"],\n",
    "    \"Merlin\": [\"merlin\"],\n",
    "    \"Tapioca\": [\"tapioca\"],\n",
    "    \"Arcadia Finance\": [\"arcadia\"],\n",
    "    \"Notcoin\": [\"notcoin\", \"not\"],\n",
    "    \"Omni Network\": [\"omni\"],\n",
    "    \"LayerZero\": [\"layerzero\", \"lz\"],\n",
    "    \"ZetaChain\": [\"zetachain\", \"zeta\"],\n",
    "    \"Friend.tech\": [\"friendtech\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = [\n",
    "    \"CryptoCurrency\",  # Allgemeine Diskussionen √ºber Kryptow√§hrungen\n",
    "    \"CryptoMarkets\",   # Diskussionen √ºber den Kryptomarkt und Preisbewegungen\n",
    "    \"CryptoTrading\",   # Fokus auf Trading-Strategien und Analysen\n",
    "    \"Altcoin\",         # Diskussionen √ºber Altcoins (alle Kryptow√§hrungen au√üer Bitcoin)\n",
    "    \"DeFi\",            # Decentralized Finance (DeFi) und Projekte\n",
    "    \"BitcoinBeginners\",# F√ºr Anf√§nger in der Krypto-Welt\n",
    "    \"cryptotechnology\", # Fokus auf die zugrunde liegende Blockchain-Technologie\n",
    "    \"cryptocurrencies\", # Allgemeine Diskussionen √ºber Kryptow√§hrungen\n",
    "    \"Satoshistreetsbets\", # Krypto-Wetten und Spekulationen\n",
    "    \"Binance\",        # Diskussionen √ºber die Binance-Plattform  \n",
    "    \"Bitcoin\",\n",
    "    \"ethtrader\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping Funktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîπ Funktion f√ºr neue Posts mit Pagination\n",
    "def get_all_new_posts(subreddit, start_timestamp, end_timestamp):\n",
    "    all_posts = []\n",
    "    after = None\n",
    "\n",
    "    while True:\n",
    "        print(f\"üîÑ Lade neue Posts aus r/{subreddit}... (Aktuell: {len(all_posts)})\")\n",
    "\n",
    "        try:\n",
    "            new_posts = list(reddit.subreddit(subreddit).new(limit=100, params={\"after\": after}))\n",
    "        except praw.exceptions.APIException as e:\n",
    "            print(f\"‚ö†Ô∏è API-Fehler: {e}\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "\n",
    "        if not new_posts:\n",
    "            break\n",
    "\n",
    "        for post in new_posts:\n",
    "            if start_timestamp <= post.created_utc <= end_timestamp:\n",
    "                all_posts.append(post)\n",
    "\n",
    "        after = new_posts[-1].fullname\n",
    "        time.sleep(2)  # Wartezeit f√ºr Rate-Limit\n",
    "\n",
    "    print(f\"‚úÖ Abgeschlossen: {len(all_posts)} neue Posts aus r/{subreddit} geladen!\")\n",
    "    return all_posts\n",
    "\n",
    "# üîπ Haupt-Scraper\n",
    "def scrape_reddit(start_date, end_date):\n",
    "    start_timestamp = int(start_date.timestamp())\n",
    "    end_timestamp = int(end_date.timestamp())\n",
    "\n",
    "    posts_data = []\n",
    "    comments_data = []\n",
    "\n",
    "    for subreddit_name in subreddits:\n",
    "        print(f\"üîé Suche in r/{subreddit_name}...\")\n",
    "\n",
    "        # üîπ ALLE neuen Posts holen\n",
    "        new_posts = get_all_new_posts(subreddit_name, start_timestamp, end_timestamp)\n",
    "\n",
    "        for post in new_posts:\n",
    "            detected_cryptos = [\n",
    "                crypto for crypto, terms in crypto_terms.items() \n",
    "                if any(term in post.title.lower() for term in terms)\n",
    "            ]\n",
    "\n",
    "            # üìù **Speicherung der Posts**\n",
    "            posts_data.append({\n",
    "                \"post_id\": post.id,\n",
    "                \"subreddit\": subreddit_name,\n",
    "                \"title\": post.title.lower(),\n",
    "                \"selftext\": (post.selftext or \"\").lower(),\n",
    "                \"author\": str(post.author),\n",
    "                \"created_utc\": datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"score\": post.score,\n",
    "                \"num_comments\": post.num_comments,\n",
    "                \"detected_crypto\": detected_cryptos\n",
    "            })\n",
    "\n",
    "            # üîπ Kommentare abrufen (ALLE speichern, nicht nur mit Krypto)\n",
    "            try:\n",
    "                time.sleep(1)  # Wartezeit f√ºr API-Limit\n",
    "                post.comments.replace_more(limit=5)  # Begrenzung auf relevante Kommentare\n",
    "\n",
    "                for comment in post.comments.list():\n",
    "                    comment_body = (comment.body or \"\").lower()\n",
    "\n",
    "                    detected_cryptos_comment = [\n",
    "                        crypto for crypto, terms in crypto_terms.items() \n",
    "                        if any(term in comment_body for term in terms)\n",
    "                    ]\n",
    "\n",
    "                    # üí¨ **Speicherung der Kommentare**\n",
    "                    comments_data.append({\n",
    "                        \"comment_id\": comment.id,\n",
    "                        \"post_id\": post.id,\n",
    "                        \"subreddit\": subreddit_name,\n",
    "                        \"author\": str(comment.author),\n",
    "                        \"created_utc\": datetime.utcfromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        \"score\": comment.score,\n",
    "                        \"selftext\": comment_body,\n",
    "                        \"detected_crypto\": detected_cryptos_comment\n",
    "                    })\n",
    "\n",
    "            except praw.exceptions.APIException as e:\n",
    "                print(f\"‚ö†Ô∏è API-Fehler beim Abrufen von Kommentaren: {e}\")\n",
    "                time.sleep(30)  # Exponentielle Backoff-Strategie\n",
    "\n",
    "    # üîπ DataFrames erstellen\n",
    "    df_posts = pd.DataFrame(posts_data)\n",
    "    df_comments = pd.DataFrame(comments_data)\n",
    "\n",
    "    print(f\"‚úÖ Scrape abgeschlossen: {len(df_posts)} Posts und {len(df_comments)} Kommentare gespeichert.\")\n",
    "\n",
    "    return df_posts, df_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starte Scraping-Prozess...\n",
      "üîé Suche in r/CryptoCurrency...\n",
      "üîÑ Lade neue Posts aus r/CryptoCurrency... (Aktuell: 0)\n",
      "üîÑ Lade neue Posts aus r/CryptoCurrency... (Aktuell: 100)\n",
      "üîÑ Lade neue Posts aus r/CryptoCurrency... (Aktuell: 200)\n",
      "üîÑ Lade neue Posts aus r/CryptoCurrency... (Aktuell: 300)\n",
      "üîÑ Lade neue Posts aus r/CryptoCurrency... (Aktuell: 400)\n",
      "üîÑ Lade neue Posts aus r/CryptoCurrency... (Aktuell: 500)\n",
      "üîÑ Lade neue Posts aus r/CryptoCurrency... (Aktuell: 600)\n",
      "üîÑ Lade neue Posts aus r/CryptoCurrency... (Aktuell: 700)\n",
      "üîÑ Lade neue Posts aus r/CryptoCurrency... (Aktuell: 800)\n",
      "üîÑ Lade neue Posts aus r/CryptoCurrency... (Aktuell: 805)\n",
      "‚úÖ Abgeschlossen: 805 neue Posts aus r/CryptoCurrency geladen!\n",
      "üîé Suche in r/CryptoMarkets...\n",
      "üîÑ Lade neue Posts aus r/CryptoMarkets... (Aktuell: 0)\n",
      "üîÑ Lade neue Posts aus r/CryptoMarkets... (Aktuell: 99)\n",
      "üîÑ Lade neue Posts aus r/CryptoMarkets... (Aktuell: 199)\n",
      "üîÑ Lade neue Posts aus r/CryptoMarkets... (Aktuell: 299)\n",
      "üîÑ Lade neue Posts aus r/CryptoMarkets... (Aktuell: 399)\n",
      "üîÑ Lade neue Posts aus r/CryptoMarkets... (Aktuell: 499)\n",
      "üîÑ Lade neue Posts aus r/CryptoMarkets... (Aktuell: 599)\n",
      "üîÑ Lade neue Posts aus r/CryptoMarkets... (Aktuell: 699)\n",
      "üîÑ Lade neue Posts aus r/CryptoMarkets... (Aktuell: 799)\n",
      "üîÑ Lade neue Posts aus r/CryptoMarkets... (Aktuell: 891)\n",
      "‚úÖ Abgeschlossen: 891 neue Posts aus r/CryptoMarkets geladen!\n",
      "üîé Suche in r/CryptoTrading...\n",
      "üîÑ Lade neue Posts aus r/CryptoTrading... (Aktuell: 0)\n",
      "üîÑ Lade neue Posts aus r/CryptoTrading... (Aktuell: 0)\n",
      "üîÑ Lade neue Posts aus r/CryptoTrading... (Aktuell: 0)\n",
      "üîÑ Lade neue Posts aus r/CryptoTrading... (Aktuell: 0)\n",
      "üîÑ Lade neue Posts aus r/CryptoTrading... (Aktuell: 0)\n",
      "üîÑ Lade neue Posts aus r/CryptoTrading... (Aktuell: 0)\n",
      "üîÑ Lade neue Posts aus r/CryptoTrading... (Aktuell: 0)\n",
      "üîÑ Lade neue Posts aus r/CryptoTrading... (Aktuell: 0)\n",
      "‚úÖ Abgeschlossen: 0 neue Posts aus r/CryptoTrading geladen!\n",
      "üîé Suche in r/Altcoin...\n",
      "üîÑ Lade neue Posts aus r/Altcoin... (Aktuell: 0)\n",
      "üîÑ Lade neue Posts aus r/Altcoin... (Aktuell: 100)\n",
      "üîÑ Lade neue Posts aus r/Altcoin... (Aktuell: 200)\n",
      "üîÑ Lade neue Posts aus r/Altcoin... (Aktuell: 300)\n",
      "üîÑ Lade neue Posts aus r/Altcoin... (Aktuell: 370)\n",
      "üîÑ Lade neue Posts aus r/Altcoin... (Aktuell: 370)\n",
      "üîÑ Lade neue Posts aus r/Altcoin... (Aktuell: 370)\n",
      "üîÑ Lade neue Posts aus r/Altcoin... (Aktuell: 370)\n",
      "üîÑ Lade neue Posts aus r/Altcoin... (Aktuell: 370)\n",
      "üîÑ Lade neue Posts aus r/Altcoin... (Aktuell: 370)\n",
      "üîÑ Lade neue Posts aus r/Altcoin... (Aktuell: 370)\n",
      "‚úÖ Abgeschlossen: 370 neue Posts aus r/Altcoin geladen!\n",
      "üîé Suche in r/DeFi...\n",
      "üîÑ Lade neue Posts aus r/DeFi... (Aktuell: 0)\n",
      "üîÑ Lade neue Posts aus r/DeFi... (Aktuell: 100)\n",
      "üîÑ Lade neue Posts aus r/DeFi... (Aktuell: 200)\n",
      "üîÑ Lade neue Posts aus r/DeFi... (Aktuell: 300)\n",
      "üîÑ Lade neue Posts aus r/DeFi... (Aktuell: 400)\n",
      "üîÑ Lade neue Posts aus r/DeFi... (Aktuell: 500)\n",
      "üîÑ Lade neue Posts aus r/DeFi... (Aktuell: 600)\n",
      "üîÑ Lade neue Posts aus r/DeFi... (Aktuell: 652)\n",
      "‚úÖ Abgeschlossen: 652 neue Posts aus r/DeFi geladen!\n",
      "üîé Suche in r/BitcoinBeginners...\n",
      "üîÑ Lade neue Posts aus r/BitcoinBeginners... (Aktuell: 0)\n",
      "üîÑ Lade neue Posts aus r/BitcoinBeginners... (Aktuell: 100)\n",
      "üîÑ Lade neue Posts aus r/BitcoinBeginners... (Aktuell: 200)\n",
      "üîÑ Lade neue Posts aus r/BitcoinBeginners... (Aktuell: 300)\n",
      "üîÑ Lade neue Posts aus r/BitcoinBeginners... (Aktuell: 400)\n",
      "üîÑ Lade neue Posts aus r/BitcoinBeginners... (Aktuell: 500)\n",
      "üîÑ Lade neue Posts aus r/BitcoinBeginners... (Aktuell: 600)\n",
      "üîÑ Lade neue Posts aus r/BitcoinBeginners... (Aktuell: 700)\n",
      "üîÑ Lade neue Posts aus r/BitcoinBeginners... (Aktuell: 800)\n",
      "üîÑ Lade neue Posts aus r/BitcoinBeginners... (Aktuell: 855)\n",
      "‚úÖ Abgeschlossen: 855 neue Posts aus r/BitcoinBeginners geladen!\n",
      "üîé Suche in r/cryptotechnology...\n",
      "üîÑ Lade neue Posts aus r/cryptotechnology... (Aktuell: 0)\n",
      "üîÑ Lade neue Posts aus r/cryptotechnology... (Aktuell: 56)\n",
      "üîÑ Lade neue Posts aus r/cryptotechnology... (Aktuell: 56)\n",
      "üîÑ Lade neue Posts aus r/cryptotechnology... (Aktuell: 56)\n",
      "‚úÖ Abgeschlossen: 56 neue Posts aus r/cryptotechnology geladen!\n",
      "üîé Suche in r/cryptocurrencies...\n",
      "üîÑ Lade neue Posts aus r/cryptocurrencies... (Aktuell: 0)\n",
      "üîÑ Lade neue Posts aus r/cryptocurrencies... (Aktuell: 99)\n",
      "üîÑ Lade neue Posts aus r/cryptocurrencies... (Aktuell: 99)\n",
      "üîÑ Lade neue Posts aus r/cryptocurrencies... (Aktuell: 99)\n",
      "üîÑ Lade neue Posts aus r/cryptocurrencies... (Aktuell: 99)\n",
      "üîÑ Lade neue Posts aus r/cryptocurrencies... (Aktuell: 99)\n",
      "üîÑ Lade neue Posts aus r/cryptocurrencies... (Aktuell: 99)\n",
      "üîÑ Lade neue Posts aus r/cryptocurrencies... (Aktuell: 99)\n",
      "‚úÖ Abgeschlossen: 99 neue Posts aus r/cryptocurrencies geladen!\n",
      "üîé Suche in r/Satoshistreetsbets...\n",
      "üîÑ Lade neue Posts aus r/Satoshistreetsbets... (Aktuell: 0)\n",
      "üîÑ Lade neue Posts aus r/Satoshistreetsbets... (Aktuell: 0)\n",
      "‚úÖ Abgeschlossen: 0 neue Posts aus r/Satoshistreetsbets geladen!\n",
      "üîé Suche in r/Binance...\n",
      "üîÑ Lade neue Posts aus r/Binance... (Aktuell: 0)\n",
      "üîÑ Lade neue Posts aus r/Binance... (Aktuell: 15)\n",
      "üîÑ Lade neue Posts aus r/Binance... (Aktuell: 15)\n",
      "üîÑ Lade neue Posts aus r/Binance... (Aktuell: 15)\n",
      "üîÑ Lade neue Posts aus r/Binance... (Aktuell: 15)\n",
      "üîÑ Lade neue Posts aus r/Binance... (Aktuell: 15)\n",
      "üîÑ Lade neue Posts aus r/Binance... (Aktuell: 15)\n",
      "üîÑ Lade neue Posts aus r/Binance... (Aktuell: 15)\n",
      "üîÑ Lade neue Posts aus r/Binance... (Aktuell: 15)\n",
      "‚úÖ Abgeschlossen: 15 neue Posts aus r/Binance geladen!\n",
      "üîé Suche in r/Bitcoin...\n",
      "üîÑ Lade neue Posts aus r/Bitcoin... (Aktuell: 0)\n",
      "üîÑ Lade neue Posts aus r/Bitcoin... (Aktuell: 94)\n",
      "üîÑ Lade neue Posts aus r/Bitcoin... (Aktuell: 194)\n",
      "üîÑ Lade neue Posts aus r/Bitcoin... (Aktuell: 294)\n",
      "üîÑ Lade neue Posts aus r/Bitcoin... (Aktuell: 394)\n",
      "üîÑ Lade neue Posts aus r/Bitcoin... (Aktuell: 494)\n",
      "üîÑ Lade neue Posts aus r/Bitcoin... (Aktuell: 594)\n",
      "üîÑ Lade neue Posts aus r/Bitcoin... (Aktuell: 694)\n",
      "üîÑ Lade neue Posts aus r/Bitcoin... (Aktuell: 794)\n",
      "üîÑ Lade neue Posts aus r/Bitcoin... (Aktuell: 894)\n",
      "üîÑ Lade neue Posts aus r/Bitcoin... (Aktuell: 980)\n",
      "‚úÖ Abgeschlossen: 980 neue Posts aus r/Bitcoin geladen!\n",
      "üîé Suche in r/ethtrader...\n",
      "üîÑ Lade neue Posts aus r/ethtrader... (Aktuell: 0)\n",
      "üîÑ Lade neue Posts aus r/ethtrader... (Aktuell: 98)\n",
      "üîÑ Lade neue Posts aus r/ethtrader... (Aktuell: 198)\n",
      "üîÑ Lade neue Posts aus r/ethtrader... (Aktuell: 298)\n",
      "üîÑ Lade neue Posts aus r/ethtrader... (Aktuell: 398)\n",
      "üîÑ Lade neue Posts aus r/ethtrader... (Aktuell: 498)\n",
      "üîÑ Lade neue Posts aus r/ethtrader... (Aktuell: 598)\n",
      "üîÑ Lade neue Posts aus r/ethtrader... (Aktuell: 698)\n",
      "üîÑ Lade neue Posts aus r/ethtrader... (Aktuell: 798)\n",
      "üîÑ Lade neue Posts aus r/ethtrader... (Aktuell: 898)\n",
      "üîÑ Lade neue Posts aus r/ethtrader... (Aktuell: 916)\n",
      "‚úÖ Abgeschlossen: 916 neue Posts aus r/ethtrader geladen!\n",
      "‚úÖ Scrape abgeschlossen: 5639 Posts und 184758 Kommentare gespeichert.\n"
     ]
    }
   ],
   "source": [
    "# üîπ Prozess f√ºr den einmaligen Scrape\n",
    "start_of_period = datetime(2024, 11, 1)\n",
    "now = datetime.now()\n",
    "\n",
    "print(\"üöÄ Starte Scraping-Prozess...\")\n",
    "df_posts, df_comments = scrape_reddit(start_of_period, now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df_posts, df_comments, comment_threshold=500):# Anpassbarer Schwellenwert f√ºr Kommentare pro Nutzer\n",
    "    # 1. Duplikate entfernen\n",
    "    df_posts = df_posts.drop_duplicates(subset=[\"post_id\"])\n",
    "    df_comments = df_comments.drop_duplicates(subset=[\"comment_id\"])\n",
    "    \n",
    "    # 2. Fehlende Werte behandeln\n",
    "    df_posts['selftext'] = df_posts['selftext'].fillna('')  # Fehlende Posttexte auff√ºllen\n",
    "    df_comments['body'] = df_comments['body'].fillna('')  # Fehlende Kommentare auff√ºllen\n",
    "    \n",
    "    # 3. Zeitstempel konvertieren\n",
    "    df_posts['created_utc'] = pd.to_datetime(df_posts['created_utc'])\n",
    "    df_comments['created_utc'] = pd.to_datetime(df_comments['created_utc'])\n",
    "\n",
    "    # 4. Datum & Uhrzeit in separate Spalten aufteilen (Daten normalisieren)\n",
    "    df_posts[\"date\"] = df_posts[\"created_utc\"].dt.date  # YYYY-MM-DD\n",
    "    df_posts[\"time\"] = df_posts[\"created_utc\"].dt.time  # HH:MM:SS\n",
    "\n",
    "    df_comments[\"date\"] = df_comments[\"created_utc\"].dt.date\n",
    "    df_comments[\"time\"] = df_comments[\"created_utc\"].dt.time\n",
    "\n",
    "    # 5. Original `created_utc`-Spalte entfernen\n",
    "    df_posts.drop(columns=[\"created_utc\"], inplace=True)\n",
    "    df_comments.drop(columns=[\"created_utc\"], inplace=True)\n",
    "\n",
    "    # 6. Filterung nach Qualit√§t (Spam oder irrelevante Daten entfernen)\n",
    "    df_posts = df_posts[df_posts['score'] > 0]  # Posts mit negativem Score entfernen\n",
    "    df_comments = df_comments[df_comments['score'] > 0]  # Kommentare mit negativem Score entfernen\n",
    "\n",
    "    # 7. Entferne bekannte Bot-Accounts\n",
    "    bot_accounts = [\"AutoModerator\", \"coinfeeds-bot\", \"devCheckingIn\"]\n",
    "    df_comments = df_comments[~df_comments[\"author\"].isin(bot_accounts)]\n",
    "\n",
    "    # 8. Entferne Nutzer mit √ºberm√§√üigen Kommentaren\n",
    "    comment_counts = df_comments[\"author\"].value_counts()\n",
    "    frequent_users = comment_counts[comment_counts > comment_threshold].index  # Nutzer √ºber Grenze\n",
    "    df_comments = df_comments[~df_comments[\"author\"].isin(frequent_users)]\n",
    "\n",
    "    print(f\"‚úÖ Daten bereinigt: {df_comments.shape[0]} Kommentare √ºbrig (nach Spam-Filter).\")\n",
    "\n",
    "    return df_posts, df_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'body'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\dennis\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'body'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Bereinigen der Daten\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_posts_clean, df_comments_clean \u001b[38;5;241m=\u001b[39m \u001b[43mclean_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_posts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_comments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# √úberpr√ºfen, wie viele Eintr√§ge √ºbrig sind\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBereinigte Posts: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_posts_clean)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[76], line 8\u001b[0m, in \u001b[0;36mclean_data\u001b[1;34m(df_posts, df_comments, comment_threshold)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 2. Fehlende Werte behandeln\u001b[39;00m\n\u001b[0;32m      7\u001b[0m df_posts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselftext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_posts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselftext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Fehlende Posttexte auff√ºllen\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m df_comments[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_comments\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbody\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Fehlende Kommentare auff√ºllen\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 3. Zeitstempel konvertieren\u001b[39;00m\n\u001b[0;32m     11\u001b[0m df_posts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_utc\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_posts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_utc\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\dennis\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\dennis\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'body'"
     ]
    }
   ],
   "source": [
    "# Bereinigen der Daten\n",
    "df_posts_clean, df_comments_clean = clean_data(df_posts, df_comments)\n",
    "\n",
    "# √úberpr√ºfen, wie viele Eintr√§ge √ºbrig sind\n",
    "print(f\"Bereinigte Posts: {len(df_posts_clean)}\")\n",
    "print(f\"Bereinigte Kommentare: {len(df_comments_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fuer das Sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_NAME)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# üîπ Sicherstellen, dass die Spalte \"body\" existiert\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf_reddit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFehler: Die CSV-Datei enth√§lt keine \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-Spalte mit Kommentaren!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# üîπ Funktion zur Sentiment-Analyse mit CryptoBERT\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# üîπ CryptoBERT-Modell laden\n",
    "MODEL_NAME = \"ElKulako/cryptobert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# üîπ Sicherstellen, dass die Spalten \"title\" und \"body\" existieren\n",
    "if \"title\" not in df_posts_clean.columns or \"body\" not in df_comments_clean.columns:\n",
    "    raise ValueError(\"Fehler: Die CSV-Dateien enthalten nicht die erforderlichen Spalten ('title' f√ºr Posts, 'body' f√ºr Kommentare)!\")\n",
    "\n",
    "# üîπ Funktion zur Sentiment-Analyse mit CryptoBERT\n",
    "def analyze_sentiment(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return \"neutral\", 0.0  # Leere Eintr√§ge sind neutral\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    scores = F.softmax(outputs.logits, dim=1)[0]\n",
    "    labels = [\"bearish\", \"neutral\", \"bullish\"]  # CryptoBERT nutzt diese Labels\n",
    "    sentiment = labels[torch.argmax(scores).item()]\n",
    "    confidence = scores.max().item()\n",
    "\n",
    "    return sentiment, confidence\n",
    "\n",
    "# üîπ Sentiment f√ºr **Posts** berechnen (auf Basis des Titels und ggf. Selbsttextes)\n",
    "df_posts_clean[\"sentiment\"], df_posts_clean[\"sentiment_confidence\"] = zip(\n",
    "    *df_posts_clean[\"title\"].map(analyze_sentiment)\n",
    ")\n",
    "\n",
    "# Falls es einen Selbsttext gibt, kombinieren wir diesen mit dem Titel f√ºr eine genauere Analyse\n",
    "df_posts_clean[\"full_text\"] = df_posts_clean[\"title\"] + \" \" + df_posts_clean[\"selftext\"].fillna(\"\")\n",
    "df_posts_clean[\"sentiment\"], df_posts_clean[\"sentiment_confidence\"] = zip(\n",
    "    *df_posts_clean[\"full_text\"].map(analyze_sentiment)\n",
    ")\n",
    "\n",
    "# üîπ Sentiment f√ºr **Kommentare** berechnen\n",
    "df_comments_clean[\"sentiment\"], df_comments_clean[\"sentiment_confidence\"] = zip(\n",
    "    *df_comments_clean[\"body\"].map(analyze_sentiment)\n",
    ")\n",
    "\n",
    "# üîπ Ergebnisse anzeigen\n",
    "print(f\"‚úÖ Sentiment-Analyse abgeschlossen: {len(df_posts_clean)} Posts & {len(df_comments_clean)} Kommentare bewertet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîπ Spalte \"type\" hinzuf√ºgen (damit klar ist, ob es ein Post oder Kommentar ist)\n",
    "df_posts_clean[\"type\"] = \"post\"\n",
    "df_comments_clean[\"type\"] = \"comment\"\n",
    "\n",
    "# üîπ Mergen von Posts & Kommentaren \n",
    "df_merged = pd.concat([df_posts_clean, df_comments_clean], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setze den Pfad zu deinem Google Drive Ordner\n",
    "DRIVE_PATH = \"G:/Meine Ablage/reddit/\"\n",
    "POSTS_CSV = os.path.join(DRIVE_PATH, \"reddit_posts.csv\")\n",
    "COMMENTS_CSV = os.path.join(DRIVE_PATH, \"reddit_comments.csv\")\n",
    "MERGED_CSV = os.path.join(DRIVE_PATH, \"reddit_merged.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktion zum Export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(df_new, filename, key_column):\n",
    "    \"\"\"H√§ngt neue Daten an eine bestehende CSV an & entfernt Duplikate.\"\"\"\n",
    "    file_path = os.path.join(DRIVE_PATH, filename)\n",
    "\n",
    "    try:\n",
    "        # Falls Datei existiert, alte Daten einlesen\n",
    "        if os.path.exists(file_path):\n",
    "            df_existing = pd.read_csv(file_path, sep=\"|\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "            \n",
    "            # üîπ Daten zusammenf√ºhren & Duplikate nach `key_column` entfernen (neuere Werte behalten)\n",
    "            df_combined = pd.concat([df_existing, df_new], ignore_index=True).drop_duplicates(subset=[key_column], keep=\"last\")\n",
    "        else:\n",
    "            df_combined = df_new  # Falls keine Datei existiert, neue Daten direkt nutzen\n",
    "\n",
    "        # üîπ CSV speichern\n",
    "        df_combined.to_csv(\n",
    "            file_path,\n",
    "            index=False,\n",
    "            sep=\"|\",\n",
    "            encoding=\"utf-8-sig\",\n",
    "            lineterminator=\"\\n\"\n",
    "        )\n",
    "        print(f\"‚úÖ Datei erfolgreich aktualisiert: {file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Speichern der Datei {filename}: {e}\")\n",
    "\n",
    "def export_to_drive(df_posts, df_comments, df_merged):\n",
    "    \"\"\"Speichert Posts, Kommentare & die gemergte Datei mit Duplikat-Pr√ºfung.\"\"\"\n",
    "    try:\n",
    "        append_to_csv(df_posts, \"reddit_posts.csv\", key_column=\"post_id\")\n",
    "        append_to_csv(df_comments, \"reddit_comments.csv\", key_column=\"comment_id\")\n",
    "        append_to_csv(df_merged, \"reddit_merged.csv\", key_column=\"comment_id\")  # Falls Kommentare entscheidend sind\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Export: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datei erfolgreich gespeichert: G:/Meine Ablage/reddit/reddit_posts.csv\n",
      "‚úÖ Datei erfolgreich gespeichert: G:/Meine Ablage/reddit/reddit_comments.csv\n",
      "‚úÖ Datei erfolgreich gespeichert: G:/Meine Ablage/reddit/reddit_merged.csv\n"
     ]
    }
   ],
   "source": [
    "# üîπ Export-Funktion aufrufen\n",
    "export_to_drive(df_posts_clean, df_comments_clean, df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dennis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

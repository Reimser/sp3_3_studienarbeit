{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import os\n",
    "import psaw as ps\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from psaw import PushshiftAPI\n",
    "from praw.exceptions import APIException\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".env geladen? True\n"
     ]
    }
   ],
   "source": [
    "# Lade die .env-Datei\n",
    "dotenv_loaded = load_dotenv(\"zugang_reddit.env\")  # Falls die Datei anders heißt, anpassen\n",
    "# Prüfe, ob die Datei geladen wurde\n",
    "print(f\".env geladen? {dotenv_loaded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit API erfolgreich verbunden!\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"USER_AGENT\")\n",
    ")\n",
    "\n",
    "print(\"Reddit API erfolgreich verbunden!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U.S. Crypto Task Force to Focus on Delivering National Bitcoin (BTC) Reserve 1739803032.0\n",
      "A story in two pictures 1739802752.0\n",
      "A curated list of Bitcoin payment processors 1739801652.0\n",
      "best way to invest a bigger sum into bnitcoin long term 1739792662.0\n",
      "Metaplanet Buys Another ¥4.0 Billion Worth of Bitcoin (BTC) 1739787689.0\n"
     ]
    }
   ],
   "source": [
    "for post in reddit.subreddit(\"CryptoCurrency\").search(\"Bitcoin\", sort=\"new\", limit=5):\n",
    "    print(post.title, post.created_utc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cryptos und Subreddits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_terms = {\n",
    "    # 🔹 Top Coins\n",
    "    \"Ethereum\": [\"ethereum\", \"eth\", \"ether\", \"ethereum 2.0\", \"eth 2.0\"],\n",
    "    \"Wrapped Ethereum\": [\"wrapped ethereum\", \"weth\"],\n",
    "    \"Solana\": [\"solana\", \"sol\", \"sol coin\"],\n",
    "    \"Avalanche\": [\"avalanche\", \"avax\"],\n",
    "    \"Polkadot\": [\"polkadot\", \"dot\"],\n",
    "    \"Near Protocol\": [\"near protocol\", \"near\"],\n",
    "    \"Polygon\": [\"polygon\", \"matic\"],\n",
    "    \"XRP\": [\"xrp\", \"ripple\"],\n",
    "    \"Cardano\": [\"cardano\", \"ada\"],\n",
    "    \"Cronos\": [\"cronos\", \"cro\"],\n",
    "    \"Vulcan Forged PYR\": [\"vulcan forged\", \"pyr\"],\n",
    "    \"Chiliz\": [\"chiliz\", \"chz\"],\n",
    "    \"Illuvium\": [\"illuvium\", \"ilv\"],\n",
    "    \"Ronin\": [\"ronin\", \"ron\"],\n",
    "    \"Band Protocol\": [\"band protocol\", \"band\"],\n",
    "    \"Optimism\": [\"optimism\", \"op\"],\n",
    "    \"Celestia\": [\"celestia\", \"tia\"],\n",
    "    \"Numerai\": [\"numerai\", \"nmr\"],\n",
    "    \"Aethir\": [\"aethir\", \"ath\"],\n",
    "    \"Sui\": [\"sui\"],\n",
    "    \"Hyperliquid\": [\"hyperliquid\", \"hyp\"],\n",
    "    \"Robinhood Coin\": [\"robinhood\", \"hood\"],\n",
    "    \"Trump Coin\": [\"trump coin\"],\n",
    "    \"USD Coin\": [\"usd coin\", \"usdc\"],\n",
    "    \"Binance Coin\": [\"binance\", \"bnb\"],\n",
    "    \"Litecoin\": [\"litecoin\", \"ltc\"],\n",
    "    \"Dogecoin\": [\"dogecoin\", \"doge\"],\n",
    "    \"Tron\": [\"tron\", \"trx\"],\n",
    "    \"Aave\": [\"aave\"],\n",
    "    \"Hedera\": [\"hedera\", \"hbar\"],\n",
    "    \"Filecoin\": [\"filecoin\", \"fil\"],\n",
    "    \"Cosmos\": [\"cosmos\", \"atom\"],\n",
    "    \"Gala\": [\"gala\"],\n",
    "    \"The Sandbox\": [\"sandbox\", \"sand\"],\n",
    "    \"Audius\": [\"audius\", \"audio\"],\n",
    "    \"Render\": [\"render\", \"rndr\"],\n",
    "    \"Kusama\": [\"kusama\", \"ksm\"],\n",
    "    \"VeChain\": [\"vechain\", \"vet\"],\n",
    "    \"Chainlink\": [\"chainlink\", \"link\"],\n",
    "    \"Berachain\": [\"berachain\", \"bera\"],\n",
    "    \"TestCoin\": [\"testcoin\", \"test\"],\n",
    "\n",
    "    # 🔹 Meme-Coins\n",
    "    \"Shiba Inu\": [\"shiba inu\", \"shib\"],\n",
    "    \"Pepe\": [\"pepe\"],\n",
    "    \"Floki Inu\": [\"floki inu\", \"floki\"],\n",
    "    \"Bonk\": [\"bonk\"],\n",
    "    \"Wojak\": [\"wojak\"],\n",
    "    \"Mog Coin\": [\"mog\"],\n",
    "    \"Doge Killer (Leash)\": [\"leash\"],\n",
    "    \"Baby Doge Coin\": [\"baby doge\", \"babydoge\"],\n",
    "    \"Degen\": [\"degen\"],\n",
    "    \"Toshi\": [\"toshi\"],\n",
    "    \"Fartcoin\": [\"fartcoin\"],\n",
    "    \"Banana\": [\"banana\"],\n",
    "    \"Kabosu\": [\"kabosu\"],\n",
    "    \"Husky\": [\"husky\"],\n",
    "    \"Samoyedcoin\": [\"samoyedcoin\", \"samo\"],\n",
    "    \"Milkbag\": [\"milkbag\"],\n",
    "\n",
    "    # 🔹 New Coins\n",
    "    \"Arbitrum\": [\"arbitrum\", \"arb\"],\n",
    "    \"Starknet\": [\"starknet\", \"strk\"],\n",
    "    \"Injective Protocol\": [\"injective\", \"inj\"],\n",
    "    \"Sei Network\": [\"sei\"],\n",
    "    \"Aptos\": [\"aptos\", \"apt\"],\n",
    "    \"EigenLayer\": [\"eigenlayer\", \"eigen\"],\n",
    "    \"Mantle\": [\"mantle\", \"mnt\"],\n",
    "    \"Immutable X\": [\"immutable x\", \"imx\"],\n",
    "    \"Ondo Finance\": [\"ondo\"],\n",
    "    \"Worldcoin\": [\"worldcoin\", \"wld\"],\n",
    "    \"Aerodrome\": [\"aerodrome\", \"aero\"],\n",
    "    \"Jupiter\": [\"jupiter\", \"jup\"],\n",
    "    \"THORChain\": [\"thorchain\", \"rune\"],\n",
    "    \"Pendle\": [\"pendle\"],\n",
    "    \"Kujira\": [\"kujira\", \"kuji\"],\n",
    "    \"Noble\": [\"noble\"],\n",
    "    \"Stride\": [\"stride\", \"strd\"],\n",
    "    \"Dymension\": [\"dymension\", \"dym\"],\n",
    "    \"Seamless Protocol\": [\"seamless\", \"seam\"],\n",
    "    \"Blast\": [\"blast\"],\n",
    "    \"Merlin\": [\"merlin\"],\n",
    "    \"Tapioca\": [\"tapioca\"],\n",
    "    \"Arcadia Finance\": [\"arcadia\"],\n",
    "    \"Notcoin\": [\"notcoin\", \"not\"],\n",
    "    \"Omni Network\": [\"omni\"],\n",
    "    \"LayerZero\": [\"layerzero\", \"lz\"],\n",
    "    \"ZetaChain\": [\"zetachain\", \"zeta\"],\n",
    "    \"Friend.tech\": [\"friendtech\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = [\n",
    "    \"CryptoCurrency\",  # Allgemeine Diskussionen über Kryptowährungen\n",
    "    \"CryptoMarkets\",   # Diskussionen über den Kryptomarkt und Preisbewegungen\n",
    "    \"CryptoTrading\",   # Fokus auf Trading-Strategien und Analysen\n",
    "    \"Altcoin\",         # Diskussionen über Altcoins (alle Kryptowährungen außer Bitcoin)\n",
    "    \"DeFi\",            # Decentralized Finance (DeFi) und Projekte\n",
    "    \"BitcoinBeginners\",# Für Anfänger in der Krypto-Welt\n",
    "    \"cryptotechnology\", # Fokus auf die zugrunde liegende Blockchain-Technologie\n",
    "    \"cryptocurrencies\", # Allgemeine Diskussionen über Kryptowährungen\n",
    "    \"Satoshistreetsbets\", # Krypto-Wetten und Spekulationen\n",
    "    \"Binance\",        # Diskussionen über die Binance-Plattform  \n",
    "    \"Bitcoin\",\n",
    "    \"ethtrader\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping Funktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m api \u001b[38;5;241m=\u001b[39m \u001b[43mPushshiftAPI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 🔹 Scraper für historische Reddit-Daten mit vollständigem Kommentarabruf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_reddit\u001b[39m(start_date, end_date, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\dennis\\lib\\site-packages\\psaw\\PushshiftAPI.py:326\u001b[0m, in \u001b[0;36mPushshiftAPI.__init__\u001b[1;34m(self, r, *args, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, r\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    290\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    Helper class for interacting with the PushShift API for searching public reddit archival data.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;124;03m    :type shards_down_behavior: str, optional\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr \u001b[38;5;241m=\u001b[39m r\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_search_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_search\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\dennis\\lib\\site-packages\\psaw\\PushshiftAPI.py:94\u001b[0m, in \u001b[0;36mPushshiftAPIMinimal.__init__\u001b[1;34m(self, max_retries, max_sleep, backoff, rate_limit_per_minute, max_results_per_request, detect_local_tz, utc_offset_secs, domain, https_proxy, shards_down_behavior)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rate_limit_per_minute \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnecting to /meta endpoint to learn rate limit.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmeta\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m     rate_limit_per_minute \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserver_ratelimit_per_minute\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     96\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserver_ratelimit_per_minute: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m rate_limit_per_minute)\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\dennis\\lib\\site-packages\\psaw\\PushshiftAPI.py:181\u001b[0m, in \u001b[0;36mPushshiftAPIMinimal._get\u001b[1;34m(self, url, payload)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    180\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to connect to pushshift.io. Retrying after backoff.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 181\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_impose_rate_limit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\dennis\\lib\\site-packages\\psaw\\PushshiftAPI.py:151\u001b[0m, in \u001b[0;36mPushshiftAPIMinimal._impose_rate_limit\u001b[1;34m(self, nth_request)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interval \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    150\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImposing rate limit, sleeping for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m interval)\n\u001b[1;32m--> 151\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterval\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "api = PushshiftAPI()\n",
    "\n",
    "# 🔹 Scraper für historische Reddit-Daten mit vollständigem Kommentarabruf\n",
    "def scrape_reddit(start_date, end_date, mode=\"initial\"):\n",
    "    start_timestamp = int(start_date.timestamp())\n",
    "    end_timestamp = int(end_date.timestamp())\n",
    "\n",
    "    posts = []\n",
    "    comments = []\n",
    "    post_ids = set()\n",
    "\n",
    "    for crypto_name, search_terms in crypto_terms.items():\n",
    "        for subreddit_name in subreddits:\n",
    "            print(f\"🔍 Suche nach {crypto_name} in r/{subreddit_name}...\")\n",
    "\n",
    "            try:\n",
    "                # 🟢 Pushshift für historische Posts nutzen\n",
    "                submissions = list(api.search_submissions(\n",
    "                    after=start_timestamp,\n",
    "                    before=end_timestamp,\n",
    "                    subreddit=subreddit_name,\n",
    "                    q=\"|\".join(search_terms),\n",
    "                    filter=[\"id\", \"title\", \"selftext\", \"author\", \"created_utc\", \"score\", \"num_comments\"],\n",
    "                    limit=5000\n",
    "                ))\n",
    "\n",
    "                # 🛑 Falls Pushshift keine Posts liefert, Fallback auf `praw.search()`\n",
    "                if not submissions:\n",
    "                    print(f\"⚠️ Keine Pushshift-Daten für r/{subreddit_name}, verwende `praw.search()`...\")\n",
    "                    subreddit = reddit.subreddit(subreddit_name)\n",
    "                    submissions = list(subreddit.search(\"|\".join(search_terms), sort=\"new\", time_filter=\"all\", limit=500))\n",
    "\n",
    "                for post in submissions:\n",
    "                    post_id = post.id\n",
    "                    if post_id not in post_ids:\n",
    "                        post_ids.add(post_id)\n",
    "                        created_dt = datetime.utcfromtimestamp(post.created_utc)\n",
    "\n",
    "                        posts.append({\n",
    "                            'post_id': post_id,\n",
    "                            'crypto': crypto_name,\n",
    "                            'search_term': next((term for term in search_terms if term in post.title.lower() or term in post.selftext.lower()), None),\n",
    "                            'subreddit': subreddit_name,\n",
    "                            'title': post.title,\n",
    "                            'author': str(post.author),\n",
    "                            'date': created_dt.date().isoformat(),\n",
    "                            'time': created_dt.time().isoformat(),\n",
    "                            'score': post.score,\n",
    "                            'num_comments': post.num_comments,\n",
    "                            'selftext': post.selftext\n",
    "                        })\n",
    "                        print(f\"✅ Post gefunden: {post.title}\")\n",
    "\n",
    "                        # 🟢 Vollständige Kommentarabfrage mit praw\n",
    "                        try:\n",
    "                            submission = reddit.submission(id=post_id)\n",
    "                            submission.comments.replace_more(limit=0)  # Entfernt Strukturkommentare\n",
    "\n",
    "                            for comment in submission.comments.list():\n",
    "                                created_dt = datetime.utcfromtimestamp(comment.created_utc)\n",
    "                                comments.append({\n",
    "                                    'post_id': post_id,\n",
    "                                    'comment_id': comment.id,\n",
    "                                    'author': str(comment.author),\n",
    "                                    'date': created_dt.date().isoformat(),\n",
    "                                    'time': created_dt.time().isoformat(),\n",
    "                                    'score': comment.score,\n",
    "                                    'selftext': comment.body\n",
    "                                })\n",
    "\n",
    "                        except APIException as e:\n",
    "                            print(f\"⚠️ Fehler beim Abrufen der Kommentare: {e}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ API-Fehler: {e}. Warte 60 Sekunden und versuche es erneut...\")\n",
    "                time.sleep(60)\n",
    "\n",
    "    df_posts = pd.DataFrame(posts)\n",
    "    df_comments = pd.DataFrame(comments)\n",
    "\n",
    "    print(f\"✅ Scrape abgeschlossen: {len(df_posts)} Posts & {len(df_comments)} Kommentare gefunden.\")\n",
    "    return df_posts, df_comments\n",
    "\n",
    "# 🔹 Starte den Scraper für die letzten 3 Monate\n",
    "start_of_period = datetime(2024, 11, 1, tzinfo=timezone.utc)\n",
    "now = datetime.now(timezone.utc)\n",
    "df_posts, df_comments = scrape_reddit(start_of_period, now)\n",
    "\n",
    "print(\"✅ Daten erfolgreich gespeichert & bereit für weitere Analysen.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df_posts, df_comments, comment_threshold=500):\n",
    "    # 1. Duplikate entfernen\n",
    "    df_posts = df_posts.drop_duplicates(subset=[\"post_id\"])\n",
    "    df_comments = df_comments.drop_duplicates(subset=[\"comment_id\"])\n",
    "\n",
    "    # 2. Fehlende Werte behandeln\n",
    "    df_posts['selftext'] = df_posts['selftext'].fillna('')\n",
    "    df_comments['body'] = df_comments['body'].fillna('')\n",
    "\n",
    "    # 8. Entferne Nutzer mit übermäßigen Kommentaren\n",
    "    comment_counts = df_comments[\"author\"].value_counts()\n",
    "    frequent_users = comment_counts[comment_counts > comment_threshold].index\n",
    "    df_comments = df_comments[~df_comments[\"author\"].isin(frequent_users)]\n",
    "\n",
    "    print(f\"✅ Daten bereinigt: {df_comments.shape[0]} Kommentare übrig (nach Spam-Filter).\")\n",
    "\n",
    "    return df_posts, df_comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts_clean, df_comments_clean = clean_data(df_posts, df_comments, comment_threshold=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔹 GPU nutzen, falls verfügbar\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🚀 Verwende Gerät: {device}\")\n",
    "\n",
    "# 🔹 CryptoBERT-Modell laden\n",
    "MODEL_NAME = \"ElKulako/cryptobert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()  # Setzt das Modell in den Evaluationsmodus\n",
    "\n",
    "# 🔹 Funktion zur Sentiment-Analyse (Optimiert für Batch-Prozesse)\n",
    "def analyze_sentiment_batch(texts, batch_size=32):\n",
    "    \"\"\"Effiziente GPU-gestützte Sentiment-Analyse mit CryptoBERT für eine Liste von Texten.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Ersetze leere Einträge durch \"neutral\"\n",
    "    texts = [t if isinstance(t, str) and t.strip() != \"\" else \"neutral\" for t in texts]\n",
    "\n",
    "    # Batchweise Verarbeitung\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"🔍 Analysiere Sentiments\"):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "\n",
    "        # Tokenisierung (mit Padding für Performance)\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, max_length=512, padding=True).to(device)\n",
    "\n",
    "        # Vorhersage mit CryptoBERT\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        scores = F.softmax(outputs.logits, dim=1)\n",
    "        labels = [\"bearish\", \"neutral\", \"bullish\"] \n",
    "\n",
    "        # Ergebnisse speichern\n",
    "        for i in range(len(batch_texts)):\n",
    "            sentiment = labels[torch.argmax(scores[i]).item()]\n",
    "            confidence = scores[i].max().item()\n",
    "            results.append((sentiment, confidence))\n",
    "\n",
    "    return results\n",
    "\n",
    "# 🔹 Sentiment für **Posts** berechnen\n",
    "tqdm.pandas()  # Fortschrittsanzeige aktivieren\n",
    "df_posts_clean[\"full_text\"] = df_posts_clean[\"title\"] + \" \" + df_posts_clean[\"selftext\"].fillna(\"\")\n",
    "df_posts_clean[[\"sentiment\", \"sentiment_confidence\"]] = pd.DataFrame(\n",
    "    analyze_sentiment_batch(df_posts_clean[\"full_text\"].tolist()), index=df_posts_clean.index\n",
    ")\n",
    "\n",
    "# 🔹 Sentiment für **Kommentare** berechnen\n",
    "df_comments_clean[\"full_text\"] = df_comments_clean[\"body\"].fillna(\"\")\n",
    "df_comments_clean[[\"sentiment\", \"sentiment_confidence\"]] = pd.DataFrame(\n",
    "    analyze_sentiment_batch(df_comments_clean[\"full_text\"].tolist()), index=df_comments_clean.index\n",
    ")\n",
    "\n",
    "# 🔹 Ergebnisse anzeigen\n",
    "print(f\"✅ Sentiment-Analyse abgeschlossen: {len(df_posts_clean)} Posts & {len(df_comments_clean)} Kommentare bewertet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔹 DataFrames zusammenführen\n",
    "df_merged = df_posts_clean.merge(df_comments_clean, on=\"post_id\", how=\"left\", suffixes=(\"_post\", \"_comment\"))\n",
    "\n",
    "# 🔹 Ergebnisse anzeigen\n",
    "print(f\"✅ DataFrames zusammengeführt: {df_merged.shape[0]} Zeilen insgesamt.\")\n",
    "\n",
    "# 🔹 Optional: Null-Werte in Kommentar-Spalten auffüllen (falls nötig)\n",
    "df_merged.fillna({\"comment_id\": \"\", \"author_comment\": \"\", \"date_comment\": \"\", \"time_comment\": \"\", \n",
    "                  \"score_comment\": 0, \"body\": \"\", \"sentiment_comment\": \"\", \"sentiment_confidence_comment\": 0.0}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setze den Pfad zu deinem Google Drive Ordner\n",
    "DRIVE_PATH = \"G:/Meine Ablage/reddit/\"\n",
    "POSTS_CSV = os.path.join(DRIVE_PATH, \"reddit_posts.csv\")\n",
    "COMMENTS_CSV = os.path.join(DRIVE_PATH, \"reddit_comments.csv\")\n",
    "MERGED_CSV = os.path.join(DRIVE_PATH, \"reddit_merged.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktion zum Export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(df_new, filename, key_column):\n",
    "    \"\"\"Hängt neue Daten an eine bestehende CSV an & entfernt Duplikate.\"\"\"\n",
    "    file_path = os.path.join(DRIVE_PATH, filename)\n",
    "\n",
    "    try:\n",
    "        # Falls Datei existiert, alte Daten einlesen\n",
    "        if os.path.exists(file_path):\n",
    "            df_existing = pd.read_csv(file_path, sep=\"|\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "            \n",
    "            # 🔹 Daten zusammenführen & Duplikate nach `key_column` entfernen (neuere Werte behalten)\n",
    "            df_combined = pd.concat([df_existing, df_new], ignore_index=True).drop_duplicates(subset=[key_column], keep=\"last\")\n",
    "        else:\n",
    "            df_combined = df_new  # Falls keine Datei existiert, neue Daten direkt nutzen\n",
    "\n",
    "        # 🔹 CSV speichern\n",
    "        df_combined.to_csv(\n",
    "            file_path,\n",
    "            index=False,\n",
    "            sep=\"|\",\n",
    "            encoding=\"utf-8-sig\",\n",
    "            lineterminator=\"\\n\"\n",
    "        )\n",
    "        print(f\"✅ Datei erfolgreich aktualisiert: {file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Speichern der Datei {filename}: {e}\")\n",
    "\n",
    "def export_to_drive(df_posts, df_comments, df_merged):\n",
    "    \"\"\"Speichert Posts, Kommentare & die gemergte Datei mit Duplikat-Prüfung.\"\"\"\n",
    "    try:\n",
    "        append_to_csv(df_posts, \"reddit_posts.csv\", key_column=\"post_id\")\n",
    "        append_to_csv(df_comments, \"reddit_comments.csv\", key_column=\"comment_id\")\n",
    "        append_to_csv(df_merged, \"reddit_merged.csv\", key_column=\"comment_id\")  # Falls Kommentare entscheidend sind\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Export: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Datei erfolgreich aktualisiert: G:/Meine Ablage/reddit/reddit_posts.csv\n",
      "✅ Datei erfolgreich aktualisiert: G:/Meine Ablage/reddit/reddit_comments.csv\n",
      "✅ Datei erfolgreich aktualisiert: G:/Meine Ablage/reddit/reddit_merged.csv\n"
     ]
    }
   ],
   "source": [
    "# 🔹 Export-Funktion aufrufen\n",
    "export_to_drive(df_posts_clean, df_comments_clean, df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_merged\u001b[49m\u001b[38;5;241m.\u001b[39msentiment\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_merged' is not defined"
     ]
    }
   ],
   "source": [
    "df_merged.sentiment.value_counts()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dennis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

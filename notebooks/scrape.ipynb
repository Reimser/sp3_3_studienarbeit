{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Einleitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Dokumentation beschreibt die Entwicklung einer datengetriebenen Pipeline zur Analyse der Stimmungslage (Sentiment) von Kryptow√§hrungen basierend auf Reddit-Daten. Ziel ist es, durch automatisierte Datenerfassung, Verarbeitung und Analyse wertvolle Einblicke in Markttrends zu gewinnen. Die Implementierung umfasst die w√∂chentliche Erfassung von Reddit-Posts und -Kommentaren, deren Bereinigung und anschlie√üende Sentiment-Analyse. Die Ergebnisse werden durch kontinuierliche Integrationsprozesse mittels Jenkins automatisiert verarbeitet und in einem interaktiven Dashboard mit Streamlit visualisiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevanz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kryptow√§hrungen unterliegen starken Kursschwankungen, die oft durch √∂ffentliche Meinungen und Diskussionen in sozialen Netzwerken begleitet werden. Eine systematische Analyse dieser Stimmungen kann helfen:\n",
    "- Markttrends fr√ºhzeitig zu erkennen,\n",
    "- Investitionsentscheidungen zu unterst√ºtzen,\n",
    "- Risiken besser zu bewerten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zielsetzung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Automatisierte Erfassung und Speicherung von relevanten Reddit-Diskussionen √ºber Kryptow√§hrungen.\n",
    "- Bereinigung und Vorverarbeitung der gesammelten Daten, um eine hohe Datenqualit√§t zu gew√§hrleisten.\n",
    "- Anwendung von Sentiment-Analyseverfahren zur Kategorisierung und Quantifizierung von Stimmungen.\n",
    "- Automatisierte Bereitstellung der Ergebnisse in einem Dashboard zur kontinuierlichen √úberwachung."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è Einrichtung des Git-Repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorgehensweise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Erstellung eines √∂ffentlichen Repositories**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "git init\n",
    "git remote add origin <URL>\n",
    "git add .\n",
    "git commit -m \"Initial commit\"\n",
    "git push -u origin main\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. **Erzeugung eines Personal Access Tokens f√ºr Jenkins**\n",
    "   - Navigiere zu [GitHub Personal Access Tokens](https://github.com/settings/tokens)\n",
    "   - Erstelle ein Token mit den erforderlichen Berechtigungen f√ºr **Repo** und **Workflows**\n",
    "   - Speichere das Token sicher und hinterlege es, du brauchst es sp√§ter um Authentifizierungsprozesse f√ºr CI/CD zu erm√∂glichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è Einrichtung der Reddit API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorgehensweise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Registrierung einer Anwendung auf Reddit**\n",
    "   - Besuche die [Reddit Developer Console](https://www.reddit.com/prefs/apps)\n",
    "   - Erstelle eine neue Anwendung und w√§hle den Typ **script**\n",
    "   - Trage eine beliebige **App-Name**, **Beschreibung** und eine **Redirect URL** (z. B. `http://localhost:8080`) ein\n",
    "   - Notiere dir die generierte **Client ID** und **Client Secret**\n",
    "\n",
    "2. **Umgebungsvariablen setzen**\n",
    "   - Erstelle eine `.env` Datei und speichere die Anmeldeinformationen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "CLIENT_ID='deine_client_id'\n",
    "CLIENT_SECRET='dein_client_secret'\n",
    "USER_AGENT='dein_user_agent'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è Einrichtung des Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Datenverarbeitung und Analyse interaktiv durchzuf√ºhren, wird ein **Jupyter Notebook** verwendet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation der ben√∂tigten Python-Bibliotheken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zun√§chst m√ºssen alle relevanten Pakete installiert werden:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash \n",
    "pip install praw pandas psaw python-dotenv torch transformers tqdm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laden und Initialisieren der Reddit API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das folgende Skript l√§dt die API-Zugangsdaten aus der `.env` Datei und stellt eine Verbindung zur Reddit API her:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiere die ben√∂tigten Bibliotheken\n",
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import os\n",
    "import psaw as ps\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from psaw import PushshiftAPI\n",
    "from praw.exceptions import APIException\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".env geladen? True\n"
     ]
    }
   ],
   "source": [
    "# Lade die .env-Datei\n",
    "dotenv_loaded = load_dotenv(\"zugang_reddit.env\")  # Falls die Datei anders hei√üt, anpassen\n",
    "print(f\".env geladen? {dotenv_loaded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit API erfolgreich verbunden!\n"
     ]
    }
   ],
   "source": [
    "# Verbindung zur Reddit API\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"USER_AGENT\")\n",
    ")\n",
    "\n",
    "print(\"Reddit API erfolgreich verbunden!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testabfrage von Reddit-Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Verbindung zu √ºberpr√ºfen, werden die f√ºnf hei√üesten Posts aus dem Subreddit `CryptoCurrency` abgerufen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Daily Crypto Discussion - February 28, 2025 (GMT+0), Score: 23, URL: https://www.reddit.com/r/CryptoCurrency/comments/1izudyx/daily_crypto_discussion_february_28_2025_gmt0/\n",
      "Title: Thanks Crypto, Score: 4940, URL: https://i.redd.it/67mfttb6mple1.jpeg\n",
      "Title: Genuinely, what happened to the \"Crypto President\"?, Score: 266, URL: https://www.reddit.com/r/CryptoCurrency/comments/1izz8iy/genuinely_what_happened_to_the_crypto_president/\n",
      "Title: US Act to Ban Political Crypto Tokens like TRUMP, Score: 1790, URL: https://crypto-economy.com/us-act-to-ban-political-crypto-tokens-like-trump/\n",
      "Title: Mining Bitcoin is gambling with extra steps, Score: 1774, URL: https://i.redd.it/ea2h5o82fple1.png\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    subreddit = reddit.subreddit(\"CryptoCurrency\")\n",
    "    for post in subreddit.hot(limit=5):\n",
    "        print(f\"Title: {post.title}, Score: {post.score}, URL: {post.url}\")\n",
    "except Exception as e:\n",
    "    print(f\"Fehler beim Abrufen der Posts: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieser Codeblock stellt sicher, dass die Verbindung zur Reddit API erfolgreich funktioniert. Falls es zu Fehlern kommt, sollten die API-Zugangsdaten √ºberpr√ºft werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition der Kryptow√§hrungen und Subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die relevante Diskussion zu Kryptow√§hrungen zu analysieren, wird eine Liste von Kryptow√§hrungen mit ihren Symbolen sowie eine Auswahl an Subreddits definiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende Liste enth√§lt die wichtigsten Kryptow√§hrungen, die in der Analyse ber√ºcksichtigt werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_terms = {\n",
    "    # üîπ Top Coins\n",
    "    \"Bitcoin\": [\"bitcoin\", \"btc\"],\n",
    "    \"Ethereum\": [\"ethereum\", \"eth\"],\n",
    "    \"Wrapped Ethereum\": [\"wrapped ethereum\", \"weth\"],\n",
    "    \"Solana\": [\"solana\", \"sol\"],\n",
    "    \"Avalanche\": [\"avalanche\", \"avax\"],\n",
    "    \"Polkadot\": [\"polkadot\", \"dot\"],\n",
    "    \"Near Protocol\": [\"near protocol\", \"near\"],\n",
    "    \"Polygon\": [\"polygon\", \"matic\"],\n",
    "    \"XRP\": [\"xrp\", \"ripple\"],\n",
    "    \"Cardano\": [\"cardano\", \"ada\"],\n",
    "    \"Cronos\": [\"cronos\", \"cro\"],\n",
    "    \"Vulcan Forged PYR\": [\"vulcan forged\", \"pyr\"],\n",
    "    \"Chiliz\": [\"chiliz\", \"chz\"],\n",
    "    \"Illuvium\": [\"illuvium\", \"ilv\"],\n",
    "    \"Ronin\": [\"ronin\", \"ron\"],\n",
    "    \"Band Protocol\": [\"band protocol\", \"band\"],\n",
    "    \"Optimism\": [\"optimism\", \"op\"],\n",
    "    \"Celestia\": [\"celestia\", \"tia\"],\n",
    "    \"Numerai\": [\"numerai\", \"nmr\"],\n",
    "    \"Aethir\": [\"aethir\", \"ath\"],\n",
    "    \"Sui\": [\"sui\"],\n",
    "    \"Hyperliquid\": [\"hyperliquid\", \"hyp\"],\n",
    "    \"Robinhood Coin\": [\"robinhood\", \"hood\"],\n",
    "    \"Trump Coin\": [\"trump coin\"],\n",
    "    \"USD Coin\": [\"usd coin\", \"usdc\"],\n",
    "    \"Binance Coin\": [\"binance\", \"bnb\"],\n",
    "    \"Litecoin\": [\"litecoin\", \"ltc\"],\n",
    "    \"Dogecoin\": [\"dogecoin\", \"doge\"],\n",
    "    \"Tron\": [\"tron\", \"trx\"],\n",
    "    \"Aave\": [\"aave\"],\n",
    "    \"Hedera\": [\"hedera\", \"hbar\"],\n",
    "    \"Filecoin\": [\"filecoin\", \"fil\"],\n",
    "    \"Cosmos\": [\"cosmos\", \"atom\"],\n",
    "    \"Gala\": [\"gala\"],\n",
    "    \"The Sandbox\": [\"sandbox\", \"sand\"],\n",
    "    \"Audius\": [\"audius\", \"audio\"],\n",
    "    \"Render\": [\"render\", \"rndr\"],\n",
    "    \"Kusama\": [\"kusama\", \"ksm\"],\n",
    "    \"VeChain\": [\"vechain\", \"vet\"],\n",
    "    \"Chainlink\": [\"chainlink\", \"link\"],\n",
    "    \"Berachain\": [\"berachain\", \"bera\"],\n",
    "    \"TestCoin\": [\"testcoin\", \"test\"],\n",
    "\n",
    "    # üîπ Meme-Coins\n",
    "    \"Shiba Inu\": [\"shiba inu\", \"shib\"],\n",
    "    \"Pepe\": [\"pepe\"],\n",
    "    \"Floki Inu\": [\"floki inu\", \"floki\"],\n",
    "    \"Bonk\": [\"bonk\"],\n",
    "    \"Wojak\": [\"wojak\"],\n",
    "    \"Mog Coin\": [\"mog\"],\n",
    "    \"Doge Killer (Leash)\": [\"leash\"],\n",
    "    \"Baby Doge Coin\": [\"baby doge\", \"babydoge\"],\n",
    "    \"Degen\": [\"degen\"],\n",
    "    \"Toshi\": [\"toshi\"],\n",
    "    \"Fartcoin\": [\"fartcoin\"],\n",
    "    \"Banana\": [\"banana\"],\n",
    "    \"Kabosu\": [\"kabosu\"],\n",
    "    \"Husky\": [\"husky\"],\n",
    "    \"Samoyedcoin\": [\"samoyedcoin\", \"samo\"],\n",
    "    \"Milkbag\": [\"milkbag\"],\n",
    "\n",
    "    # üîπ New Coins\n",
    "    \"Arbitrum\": [\"arbitrum\", \"arb\"],\n",
    "    \"Starknet\": [\"starknet\", \"strk\"],\n",
    "    \"Injective Protocol\": [\"injective\", \"inj\"],\n",
    "    \"Sei Network\": [\"sei\"],\n",
    "    \"Aptos\": [\"aptos\", \"apt\"],\n",
    "    \"EigenLayer\": [\"eigenlayer\", \"eigen\"],\n",
    "    \"Mantle\": [\"mantle\", \"mnt\"],\n",
    "    \"Immutable X\": [\"immutable x\", \"imx\"],\n",
    "    \"Ondo Finance\": [\"ondo\"],\n",
    "    \"Worldcoin\": [\"worldcoin\", \"wld\"],\n",
    "    \"Aerodrome\": [\"aerodrome\", \"aero\"],\n",
    "    \"Jupiter\": [\"jupiter\", \"jup\"],\n",
    "    \"THORChain\": [\"thorchain\", \"rune\"],\n",
    "    \"Pendle\": [\"pendle\"],\n",
    "    \"Kujira\": [\"kujira\", \"kuji\"],\n",
    "    \"Noble\": [\"noble\"],\n",
    "    \"Stride\": [\"stride\", \"strd\"],\n",
    "    \"Dymension\": [\"dymension\", \"dym\"],\n",
    "    \"Seamless Protocol\": [\"seamless\", \"seam\"],\n",
    "    \"Blast\": [\"blast\"],\n",
    "    \"Merlin\": [\"merlin\"],\n",
    "    \"Tapioca\": [\"tapioca\"],\n",
    "    \"Arcadia Finance\": [\"arcadia\"],\n",
    "    \"Notcoin\": [\"notcoin\", \"not\"],\n",
    "    \"Omni Network\": [\"omni\"],\n",
    "    \"LayerZero\": [\"layerzero\", \"lz\"],\n",
    "    \"ZetaChain\": [\"zetachain\", \"zeta\"],\n",
    "    \"Friend.tech\": [\"friendtech\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur Analyse der Diskussionen werden Subreddits verwendet, die sich intensiv mit Kryptow√§hrungen und deren Marktbewegungen besch√§ftigen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = [\n",
    "    \"CryptoCurrency\",  # Allgemeine Diskussionen √ºber Kryptow√§hrungen\n",
    "    \"CryptoMarkets\",   # Diskussionen √ºber den Kryptomarkt und Preisbewegungen\n",
    "    \"CryptoTrading\",   # Fokus auf Trading-Strategien und Analysen\n",
    "    \"Altcoin\",         # Diskussionen √ºber Altcoins (alle Kryptow√§hrungen au√üer Bitcoin)\n",
    "    \"DeFi\",            # Decentralized Finance (DeFi) und Projekte\n",
    "    \"BitcoinBeginners\",# F√ºr Anf√§nger in der Krypto-Welt\n",
    "    \"cryptotechnology\", # Fokus auf die zugrunde liegende Blockchain-Technologie\n",
    "    \"cryptocurrencies\", # Allgemeine Diskussionen √ºber Kryptow√§hrungen\n",
    "    \"Satoshistreetsbets\", # Krypto-Wetten und Spekulationen\n",
    "    \"Binance\",        # Diskussionen √ºber die Binance-Plattform  \n",
    "    \"Bitcoin\",          # Diskussionen √ºber Bitcoin\n",
    "    \"ethtrader\"     # Generelle Diskussionen √ºber Crypto\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Listen werden sp√§ter verwendet, um relevante Beitr√§ge und Kommentare aus diesen Subreddits zu extrahieren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping von Posts und Kommentaren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende Funktion erm√∂glicht es, gezielt Reddit-Posts und deren Kommentare f√ºr bestimmte Kryptow√§hrungen zu extrahieren. Dabei werden sowohl der vollst√§ndige Name als auch das K√ºrzel (case-insesitive) der Kryptow√§hrung als Suchbegriffe genutzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # üîπ Funktion zum Scrapen von Posts und Kommentaren mit Backoff\n",
    "def scrape_reddit(start_date, end_date, mode=\"initial\"):\n",
    "    start_timestamp = int(start_date.timestamp())\n",
    "    end_timestamp = int(end_date.timestamp())\n",
    "\n",
    "    posts = []\n",
    "    comments = []\n",
    "    post_ids = set()\n",
    "    request_count = 0  # Z√§hlt die Anzahl der Requests\n",
    "\n",
    "    for crypto_name, search_terms in crypto_terms.items():\n",
    "        for subreddit_name in subreddits:\n",
    "            subreddit = reddit.subreddit(subreddit_name)\n",
    "            print(f\"üîç Suche nach {crypto_name} in r/{subreddit_name}...\")\n",
    "\n",
    "            # üü¢ Suche nach allen relevanten Begriffen mit `.search()`\n",
    "            for search_term in search_terms:\n",
    "                for post in subreddit.search(query=search_term, sort=\"new\", limit=None):\n",
    "                    if start_timestamp <= post.created_utc <= end_timestamp and post.id not in post_ids:\n",
    "                        post_ids.add(post.id)\n",
    "                        created_dt = datetime.utcfromtimestamp(post.created_utc)\n",
    "\n",
    "                        posts.append({\n",
    "                            'post_id': post.id,\n",
    "                            'crypto': crypto_name,\n",
    "                            'search_term': search_term,\n",
    "                            'subreddit': subreddit_name,\n",
    "                            'title': post.title,\n",
    "                            'author': str(post.author),\n",
    "                            'date': created_dt.date().isoformat(),\n",
    "                            'time': created_dt.time().isoformat(),\n",
    "                            'score': post.score,\n",
    "                            'num_comments': post.num_comments,\n",
    "                            'selftext': post.selftext\n",
    "                        })\n",
    "                        print(f\"‚úÖ Post gefunden: {post.title} (Suchbegriff: {search_term})\")\n",
    "\n",
    "                        # üü¢ Kommentare sammeln (mit Rate-Limit-Schutz)\n",
    "                        try:\n",
    "                            post.comments.replace_more(limit=0)\n",
    "                            for comment in post.comments.list():\n",
    "                                created_dt = datetime.utcfromtimestamp(comment.created_utc)\n",
    "                                comments.append({\n",
    "                                    'post_id': post.id,\n",
    "                                    'comment_id': comment.id,\n",
    "                                    'author': str(comment.author),\n",
    "                                    'date': created_dt.date().isoformat(),\n",
    "                                    'time': created_dt.time().isoformat(),\n",
    "                                    'score': comment.score,\n",
    "                                    'selftext': comment.body\n",
    "                                })\n",
    "                        except praw.exceptions.APIException as e:\n",
    "                            if \"RATELIMIT\" in str(e):\n",
    "                                print(f\"‚ö†Ô∏è Reddit API-Limit erreicht. Warte 60 Sekunden...\")\n",
    "                                time.sleep(60)  # Wartezeit erh√∂hen\n",
    "                            else:\n",
    "                                print(f\"‚ö†Ô∏è Fehler beim Abrufen der Kommentare: {e}\")\n",
    "\n",
    "                    # üîÑ Nach jeder `post.comments.list()` Anfrage pr√ºfen, ob eine Pause n√∂tig ist\n",
    "                    request_count += 1\n",
    "                    if request_count % 50 == 0:  # Nach 50 Requests eine kurze Pause\n",
    "                        wait_time = 10  # Standard-Wartezeit\n",
    "                        print(f\"‚è≥ Warte {wait_time} Sekunden, um Rate-Limit zu vermeiden...\")\n",
    "                        time.sleep(wait_time)\n",
    "\n",
    "    # In DataFrames umwandeln\n",
    "    df_posts = pd.DataFrame(posts)\n",
    "    df_comments = pd.DataFrame(comments)\n",
    "\n",
    "    print(f\"‚úÖ Scrape abgeschlossen: {len(df_posts)} Posts & {len(df_comments)} Kommentare gefunden.\")\n",
    "    return df_posts, df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Durchf√ºhrung des Scrapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Beispielaufruf f√ºr die Funktion √ºber die letzten drei Monate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # üîπ Starte den Scraper f√ºr die letzten 3 Monate\n",
    "# start_of_period = datetime(2024, 11, 1)  # Startzeitpunkt\n",
    "# now = datetime.now()  # Aktueller Zeitpunkt\n",
    "# print(\"üöÄ Starte den Scraper f√ºr die letzten 3 Monate...\")\n",
    "# df_posts, df_comments = scrape_reddit(start_of_period, now)\n",
    "\n",
    "# print(\"‚úÖ Daten erfolgreich gespeichert & bereit f√ºr weitere Analysen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei dem w√∂chentlichen Scrape sieht der Aufruf wie folgt aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïµÔ∏è Starte den w√∂chentlichen Scrape...\n",
      "üîç Suche nach Bitcoin in r/CryptoCurrency...\n",
      "‚úÖ Post gefunden: Are gold based stablecoins the future? (Suchbegriff: bitcoin)\n",
      "‚úÖ Post gefunden: What's the future of memecoins/crypto outside of btc/eth? (Suchbegriff: bitcoin)\n",
      "‚úÖ Post gefunden: Bitcoin sees largest weekly drop in market cap since inception (Suchbegriff: bitcoin)\n",
      "‚úÖ Post gefunden: Introducing Tap-to-Pay for Crypto, Expanding Payment Accessibility - Breakdown (Suchbegriff: bitcoin)\n",
      "‚úÖ Post gefunden: Bitcoin ETFs Have Shed More Than $2.4 Billion So Far This Week (Suchbegriff: bitcoin)\n",
      "‚úÖ Post gefunden: Texas Bitcoin Reserve Bill Passes Committee With 9-0 Vote, What‚Äôs Next? (Suchbegriff: bitcoin)\n",
      "‚úÖ Post gefunden: Scam or real? Trianz.life (Suchbegriff: bitcoin)\n",
      "‚úÖ Post gefunden: Bitcoin whiplash (Suchbegriff: bitcoin)\n",
      "‚úÖ Post gefunden: Recent buyers Bitcoin (BTC) have realized $2.16B in losses (Suchbegriff: bitcoin)\n",
      "‚úÖ Post gefunden: 'Extreme fear': Crypto Fear & Greed Index hits multi-year low, as bitcoin sinks below $86,000 (Suchbegriff: bitcoin)\n",
      "‚úÖ Post gefunden: Mining Bitcoin is gambling with extra steps (Suchbegriff: bitcoin)\n",
      "‚úÖ Post gefunden: Litecoin and Dogecoin are examples why MicroStrategy could easily go bust. (Suchbegriff: bitcoin)\n",
      "‚úÖ Post gefunden: Astrology 'Pro' predicts Bitcoin Will Crash & Rebound - Might as well trust this as much as other analysts.. (Suchbegriff: bitcoin)\n",
      "‚úÖ Post gefunden: Metaplanet to Raise ¬•2.0 Billion to Buy More Bitcoin (BTC) - They Really Want To Buy The Dip (Suchbegriff: bitcoin)\n",
      "‚úÖ Post gefunden: He didn't buy the dip üíÄ (Suchbegriff: bitcoin)\n",
      "‚úÖ Post gefunden: Bitcoin Fear And Greed Index At 10, Which Is Lower Than FTX Collapse (Suchbegriff: bitcoin)\n"
     ]
    }
   ],
   "source": [
    "# Aktuelle Zeit als datetime-Objekt\n",
    "now = datetime.now()\n",
    "last_week = now - timedelta(days=7)  # 7 Tage zur√ºck\n",
    "\n",
    "print(\"üïµÔ∏è Starte den w√∂chentlichen Scrape...\")\n",
    "\n",
    "# Aufruf der Scraper-Funktion mit datetime-Objekten\n",
    "df_posts, df_comments = scrape_reddit(last_week, now, mode=\"weekly\")\n",
    "\n",
    "# Beispiel: Lokale Weiterverarbeitung\n",
    "print(\"Daten k√∂nnen jetzt bereinigt werden...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ich habe zun√§chst den einmaligen Scrape durchlaufen lassen und anschliessend den w√∂chentlichen automatisiert, dazu kommen wir sp√§ter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenbereinigung und Vorverarbeitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Funktion:\n",
    "- Entfernt Duplikate in Posts und Kommentaren.\n",
    "- Handhabt fehlende Werte, um Datenverluste zu vermeiden.\n",
    "- Konvertiert und strukturiert Zeitstempel f√ºr bessere Nachvollziehbarkeit.\n",
    "- Zu kurze Kommentare werden entfernt.\n",
    "- Entfernt Kommentare von Accounts mit  √ºberm√§√üig vielen Kommentaren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df_posts, df_comments, comment_threshold=500, min_length=5):\n",
    "    # 1. Duplikate entfernen\n",
    "    df_posts = df_posts.drop_duplicates(subset=[\"post_id\"])\n",
    "    df_comments = df_comments.drop_duplicates(subset=[\"comment_id\"])\n",
    "    \n",
    "    # 2. Fehlende Werte behandeln\n",
    "    df_posts['selftext'] = df_posts['selftext'].fillna('')  # Fehlende Posttexte auff√ºllen\n",
    "    df_comments['selftext'] = df_comments['selftext'].fillna('')  # Fehlende Kommentare auff√ºllen\n",
    "\n",
    "    # 3. Entferne Nutzer (Bots) mit √ºberm√§√üigen Kommentaren\n",
    "    comment_counts = df_comments[\"author\"].value_counts()\n",
    "    frequent_users = comment_counts[comment_counts > comment_threshold].index  # Nutzer √ºber Grenze\n",
    "    df_comments = df_comments[~df_comments[\"author\"].isin(frequent_users)]\n",
    "\n",
    "    # 4. Entferne zu kurze Kommentare\n",
    "    df_comments = df_comments[df_comments['selftext'].str.len() >= min_length]\n",
    "\n",
    "    print(f\"‚úÖ Daten bereinigt: {df_comments.shape[0]} Kommentare √ºbrig (nach Spam-Filter & L√§nge > {min_length}).\")\n",
    "\n",
    "    return df_posts, df_comments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anwendung der Bereinigungsfunktion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der Extraktion der Daten wird die Bereinigungsfunktion auf die Posts und Kommentare angewendet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Daten bereinigt: 26232 Kommentare √ºbrig (nach Spam-Filter & L√§nge > 5).\n",
      "Bereinigte Posts: 763\n",
      "Bereinigte Kommentare: 26232\n"
     ]
    }
   ],
   "source": [
    "# Bereinigen der Daten\n",
    "df_posts_clean, df_comments_clean = clean_data(df_posts, df_comments, comment_threshold=500, min_length=5)\n",
    "\n",
    "\n",
    "# √úberpr√ºfen, wie viele Eintr√§ge √ºbrig sind\n",
    "print(f\"Bereinigte Posts: {len(df_posts_clean)}\")\n",
    "print(f\"Bereinigte Kommentare: {len(df_comments_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Schritte stellen sicher, dass nur relevante, qualitativ hochwertige Daten in der Pipeline weiterverarbeitet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment-Analyse der Kommentare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der Bereinigung der Daten wird das Sentiment f√ºr die gesammelten Kommentare mithilfe eines vortrainierten Modells analysiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verwendetes Sentiment-Analyse-Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Modell \"ElKulako/crypto-bert\" stammt von ElKulako und wurde speziell f√ºr die Analyse von Kryptow√§hrungs-Diskussionen entwickelt. Es basiert auf der BERT-Architektur und klassifiziert Texte als bullish (positiv), neutral oder bearish (negativ). Das Modell wurde auf umfangreichen Finanz- und Krypto-spezifischen Daten trainiert, wodurch es sich besonders gut f√ºr die Analyse von Reddit-Kommentaren eignet, die sich mit dem Krypto-Markt befassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Verwende Ger√§t: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîç Analysiere Sentiments: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24/24 [00:11<00:00,  2.07it/s]\n",
      "üîç Analysiere Sentiments: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 820/820 [02:56<00:00,  4.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sentiment-Analyse abgeschlossen: 763 Posts & 26232 Kommentare bewertet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# üîπ GPU nutzen, falls verf√ºgbar sonst weglassen\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Verwende Ger√§t: {device}\")\n",
    "\n",
    "# üîπ CryptoBERT-Modell laden\n",
    "MODEL_NAME = \"ElKulako/cryptobert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()  # Setzt das Modell in den Evaluationsmodus\n",
    "\n",
    "\n",
    "# üîπ Funktion zur Sentiment-Analyse (Optimiert f√ºr Batch-Prozesse)\n",
    "def analyze_sentiment_batch(texts, batch_size=32):\n",
    "    \"\"\"Effiziente GPU-gest√ºtzte Sentiment-Analyse mit CryptoBERT f√ºr eine Liste von Texten.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Ersetze leere Eintr√§ge durch \"neutral\"\n",
    "    texts = [t if isinstance(t, str) and t.strip() != \"\" else \"neutral\" for t in texts]\n",
    "\n",
    "    # Batchweise Verarbeitung\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"üîç Analysiere Sentiments\"):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "\n",
    "        # Tokenisierung (mit Padding f√ºr Performance)\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, max_length=512, padding=True).to(device)\n",
    "\n",
    "        # Vorhersage mit CryptoBERT\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        scores = F.softmax(outputs.logits, dim=1)\n",
    "        labels = [\"bearish\", \"neutral\", \"bullish\"] \n",
    "\n",
    "        # Ergebnisse speichern\n",
    "        for i in range(len(batch_texts)):\n",
    "            sentiment = labels[torch.argmax(scores[i]).item()]\n",
    "            confidence = scores[i].max().item()\n",
    "            results.append((sentiment, confidence))\n",
    "\n",
    "    return results\n",
    "\n",
    "# üîπ Sentiment f√ºr **Posts** berechnen\n",
    "tqdm.pandas()  # Fortschrittsanzeige aktivieren\n",
    "df_posts_clean[\"full_text\"] = df_posts_clean[\"title\"] + \" \" + df_posts_clean[\"selftext\"].fillna(\"\")\n",
    "df_posts_clean[[\"sentiment\", \"sentiment_confidence\"]] = pd.DataFrame(\n",
    "    analyze_sentiment_batch(df_posts[\"full_text\"].tolist()), index=df_posts.index\n",
    ")\n",
    "\n",
    "# üîπ Sentiment f√ºr **Kommentare** berechnen\n",
    "df_comments_clean[\"full_text\"] = df_comments_clean[\"selftext\"].fillna(\"\")\n",
    "df_comments_clean[[\"sentiment\", \"sentiment_confidence\"]] = pd.DataFrame(\n",
    "    analyze_sentiment_batch(df_comments_clean[\"full_text\"].tolist()), index=df_comments_clean.index\n",
    ")\n",
    "\n",
    "# üîπ Ergebnisse anzeigen\n",
    "print(f\"‚úÖ Sentiment-Analyse abgeschlossen: {len(df_posts_clean)} Posts & {len(df_comments_clean)} Kommentare bewertet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem die Sentiment-Analyse durchgef√ºhrt wurde, werden die bereinigten Posts und Kommentare zusammengef√ºhrt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = df_posts_clean.copy()\n",
    "comments = df_comments_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>crypto</th>\n",
       "      <th>search_term</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>selftext</th>\n",
       "      <th>full_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1izdutr</td>\n",
       "      <td>Bitcoin</td>\n",
       "      <td>bitcoin</td>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>He didn't buy the dip üíÄ</td>\n",
       "      <td>rizzobitcoinhistory</td>\n",
       "      <td>2025-02-27</td>\n",
       "      <td>11:44:17</td>\n",
       "      <td>181</td>\n",
       "      <td>26</td>\n",
       "      <td></td>\n",
       "      <td>He didn't buy the dip üíÄ</td>\n",
       "      <td>bullish</td>\n",
       "      <td>0.635427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1iza5sg</td>\n",
       "      <td>Bitcoin</td>\n",
       "      <td>bitcoin</td>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>Nearly 4% of the World Owns Bitcoin - Just the...</td>\n",
       "      <td>kirtash93</td>\n",
       "      <td>2025-02-27</td>\n",
       "      <td>07:16:01</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td>Nearly 4% of the World Owns Bitcoin - Just the...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.581964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1iz9hgb</td>\n",
       "      <td>Bitcoin</td>\n",
       "      <td>bitcoin</td>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>Bitcoin Fear And Greed Index At 10, Which Is L...</td>\n",
       "      <td>Funnyurolith61</td>\n",
       "      <td>2025-02-27</td>\n",
       "      <td>06:28:00</td>\n",
       "      <td>212</td>\n",
       "      <td>70</td>\n",
       "      <td></td>\n",
       "      <td>Bitcoin Fear And Greed Index At 10, Which Is L...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.651733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1iz9fz2</td>\n",
       "      <td>Bitcoin</td>\n",
       "      <td>bitcoin</td>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>BlackRock Bitcoin Fund Sees $420M Outflow as E...</td>\n",
       "      <td>Flygate</td>\n",
       "      <td>2025-02-27</td>\n",
       "      <td>06:25:07</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>BlackRock Bitcoin Fund Sees $420M Outflow as E...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.699549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1iz8vb1</td>\n",
       "      <td>Bitcoin</td>\n",
       "      <td>bitcoin</td>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>In the lavatory</td>\n",
       "      <td>thecryptos</td>\n",
       "      <td>2025-02-27</td>\n",
       "      <td>05:48:10</td>\n",
       "      <td>279</td>\n",
       "      <td>23</td>\n",
       "      <td></td>\n",
       "      <td>In the lavatory</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.385122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id   crypto search_term       subreddit  \\\n",
       "0  1izdutr  Bitcoin     bitcoin  CryptoCurrency   \n",
       "1  1iza5sg  Bitcoin     bitcoin  CryptoCurrency   \n",
       "2  1iz9hgb  Bitcoin     bitcoin  CryptoCurrency   \n",
       "3  1iz9fz2  Bitcoin     bitcoin  CryptoCurrency   \n",
       "4  1iz8vb1  Bitcoin     bitcoin  CryptoCurrency   \n",
       "\n",
       "                                               title               author  \\\n",
       "0                            He didn't buy the dip üíÄ  rizzobitcoinhistory   \n",
       "1  Nearly 4% of the World Owns Bitcoin - Just the...            kirtash93   \n",
       "2  Bitcoin Fear And Greed Index At 10, Which Is L...       Funnyurolith61   \n",
       "3  BlackRock Bitcoin Fund Sees $420M Outflow as E...              Flygate   \n",
       "4                                    In the lavatory           thecryptos   \n",
       "\n",
       "         date      time  score  num_comments selftext  \\\n",
       "0  2025-02-27  11:44:17    181            26            \n",
       "1  2025-02-27  07:16:01      0             6            \n",
       "2  2025-02-27  06:28:00    212            70            \n",
       "3  2025-02-27  06:25:07      5             2            \n",
       "4  2025-02-27  05:48:10    279            23            \n",
       "\n",
       "                                           full_text sentiment  \\\n",
       "0                           He didn't buy the dip üíÄ    bullish   \n",
       "1  Nearly 4% of the World Owns Bitcoin - Just the...   neutral   \n",
       "2  Bitcoin Fear And Greed Index At 10, Which Is L...   neutral   \n",
       "3  BlackRock Bitcoin Fund Sees $420M Outflow as E...   neutral   \n",
       "4                                   In the lavatory    neutral   \n",
       "\n",
       "   sentiment_confidence  \n",
       "0              0.635427  \n",
       "1              0.581964  \n",
       "2              0.651733  \n",
       "3              0.699549  \n",
       "4              0.385122  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Merged Dataset: 26995 Eintr√§ge (Posts + Kommentare)\n",
      "   post_id comment_id  type        date      time   crypto search_term  \\\n",
      "0  1izdutr       None  post  2025-02-27  11:44:17  Bitcoin     bitcoin   \n",
      "1  1iza5sg       None  post  2025-02-27  07:16:01  Bitcoin     bitcoin   \n",
      "2  1iz9hgb       None  post  2025-02-27  06:28:00  Bitcoin     bitcoin   \n",
      "3  1iz9fz2       None  post  2025-02-27  06:25:07  Bitcoin     bitcoin   \n",
      "4  1iz8vb1       None  post  2025-02-27  05:48:10  Bitcoin     bitcoin   \n",
      "\n",
      "        subreddit               author  \\\n",
      "0  CryptoCurrency  rizzobitcoinhistory   \n",
      "1  CryptoCurrency            kirtash93   \n",
      "2  CryptoCurrency       Funnyurolith61   \n",
      "3  CryptoCurrency              Flygate   \n",
      "4  CryptoCurrency           thecryptos   \n",
      "\n",
      "                                           full_text  score sentiment  \\\n",
      "0                           He didn't buy the dip üíÄ     181   bullish   \n",
      "1  Nearly 4% of the World Owns Bitcoin - Just the...      0   neutral   \n",
      "2  Bitcoin Fear And Greed Index At 10, Which Is L...    212   neutral   \n",
      "3  BlackRock Bitcoin Fund Sees $420M Outflow as E...      5   neutral   \n",
      "4                                   In the lavatory     279   neutral   \n",
      "\n",
      "   sentiment_confidence  \n",
      "0              0.635427  \n",
      "1              0.581964  \n",
      "2              0.651733  \n",
      "3              0.699549  \n",
      "4              0.385122  \n"
     ]
    }
   ],
   "source": [
    "# üîπ **Relevante Spalten f√ºr den Merge**\n",
    "posts = posts[[\"post_id\", \"crypto\", \"search_term\", \"subreddit\", \"author\", \"date\", \"time\", \"score\", \"full_text\", \"sentiment\", \"sentiment_confidence\"]]\n",
    "comments = comments[[\"post_id\", \"comment_id\", \"author\", \"date\", \"time\", \"score\", \"full_text\", \"sentiment\", \"sentiment_confidence\"]]\n",
    "\n",
    "# üîπ **Kommentare erben `crypto`, `search_term` und `subreddit` vom Post**\n",
    "comments = comments.merge(df_posts[[\"post_id\", \"crypto\", \"search_term\", \"subreddit\"]], on=\"post_id\", how=\"left\")\n",
    "\n",
    "# üîπ `type`-Spalte f√ºr Unterscheidung hinzuf√ºgen\n",
    "posts[\"comment_id\"] = None  # Posts haben keine comment_id\n",
    "posts[\"type\"] = \"post\"\n",
    "comments[\"type\"] = \"comment\"\n",
    "\n",
    "# üîπ **Gemeinsame Spalten f√ºr den Merge**\n",
    "common_columns = [\n",
    "    \"post_id\", \"comment_id\", \"type\", \"date\", \"time\", \"crypto\",\n",
    "    \"search_term\", \"subreddit\", \"author\",\"full_text\",\"score\", \"sentiment\", \"sentiment_confidence\",\n",
    "]\n",
    "\n",
    "# üîπ **Merging der Daten (Posts + Kommentare)**\n",
    "df_merged = pd.concat([posts[common_columns], comments[common_columns]], ignore_index=True)\n",
    "\n",
    "# üîç Debugging: √úberpr√ºfung der Gr√∂√üe\n",
    "print(f\"üìå Merged Dataset: {df_merged.shape[0]} Eintr√§ge (Posts + Kommentare)\")\n",
    "\n",
    "# üîπ √úberpr√ºfen, ob alles korrekt normalisiert wurde\n",
    "print(df_merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieser Schritt stellt sicher, dass alle relevanten Kommentare mit ihren zugeh√∂rigen Posts verkn√ºpft sind sodass wir neben den normalisierten Post- und Kommentar-Tabelle eine fertige Tabelle f√ºr Analysen haben.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export und Speicherung der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zweck der Speicherung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die exportierten Daten enthalten die gesammelten Reddit-Posts und Kommentare sowie deren Sentiment-Analyse. Durch die Speicherung in **Google Drive** wird eine einfache Automatisierung erm√∂glicht, sodass die Daten regelm√§√üig aktualisiert und f√ºr nachfolgende Analysen oder Machine-Learning-Prozesse verf√ºgbar sind.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementierung des Exports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den Pfad definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setze den Pfad zu deinem Google Drive Ordner\n",
    "DRIVE_PATH = \"G:/Meine Ablage/reddit/\"\n",
    "POSTS_CSV = os.path.join(DRIVE_PATH, \"reddit_posts.csv\")\n",
    "COMMENTS_CSV = os.path.join(DRIVE_PATH, \"reddit_comments.csv\")\n",
    "MERGED_CSV = os.path.join(DRIVE_PATH, \"reddit_merged.csv\")\n",
    "ORIGINAL_POSTS_CSV = os.path.join(DRIVE_PATH, \"reddit_posts_original.csv\")\n",
    "ORIGINAL_COMMENTS_CSV = os.path.join(DRIVE_PATH, \"reddit_comments_original.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende Funktion speichert die extrahierten und verarbeiteten Daten als CSV-Dateien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(df_new, filename, key_column):\n",
    "    \"\"\"H√§ngt neue Daten an eine bestehende CSV an & entfernt Duplikate.\"\"\"\n",
    "    file_path = os.path.join(DRIVE_PATH, filename)\n",
    "\n",
    "    try:\n",
    "        # Falls Datei existiert, alte Daten einlesen\n",
    "        if os.path.exists(file_path):\n",
    "            df_existing = pd.read_csv(file_path, sep=\"|\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "            \n",
    "            # üîπ Daten zusammenf√ºhren & Duplikate nach `key_column` entfernen (neuere Werte behalten)\n",
    "            df_combined = pd.concat([df_existing, df_new], ignore_index=True).drop_duplicates(subset=[key_column], keep=\"last\")\n",
    "        else:\n",
    "            df_combined = df_new  # Falls keine Datei existiert, neue Daten direkt nutzen\n",
    "\n",
    "        # üîπ CSV speichern\n",
    "        df_combined.to_csv(\n",
    "            file_path,\n",
    "            index=False,\n",
    "            sep=\"|\",\n",
    "            encoding=\"utf-8-sig\",\n",
    "            lineterminator=\"\\n\"\n",
    "        )\n",
    "        print(f\"‚úÖ Datei erfolgreich aktualisiert: {file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Speichern der Datei {filename}: {e}\")\n",
    "\n",
    "def export_to_drive(df_posts_clean, df_comments_clean, df_merged,df_posts, df_comments):\n",
    "    \"\"\"Speichert Posts, Kommentare & die gemergte Datei mit Duplikat-Pr√ºfung.\"\"\"\n",
    "    try:\n",
    "        append_to_csv(df_posts_clean, \"reddit_posts.csv\", key_column=\"post_id\")\n",
    "        append_to_csv(df_comments_clean, \"reddit_comments.csv\", key_column=\"comment_id\")\n",
    "        append_to_csv(df_merged, \"reddit_merged.csv\", key_column=\"comment_id\")  # Falls Kommentare entscheidend sind\n",
    "        append_to_csv(df_posts, \"reddit_posts_original.csv\", key_column=\"post_id\")\n",
    "        append_to_csv(df_comments, \"reddit_comments_original.csv\", key_column=\"comment_id\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Export: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export-Funktion aufrufen\n",
    "export_to_drive(df_posts_clean, df_comments_clean, df_merged,df_posts, df_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vorteile der Speicherung in Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Automatisierung**: Die exportierten Daten k√∂nnen regelm√§√üig durch Skripte oder Jenkins-Jobs aktualisiert werden.\n",
    "- **Zug√§nglichkeit**: Die gespeicherten Dateien k√∂nnen von verschiedenen Systemen oder Nutzern f√ºr Analysen oder Machine-Learning-Modelle verwendet werden.\n",
    "- **Versionskontrolle**: Historische Daten k√∂nnen gespeichert werden, um Entwicklungen √ºber die Zeit hinweg zu analysieren.\n",
    "\n",
    "Dieser Schritt stellt sicher, dass die verarbeiteten Daten langfristig verf√ºgbar bleiben und kontinuierlich f√ºr weitere Analysen genutzt werden k√∂nnen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Automatisierung mit Jenkins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einrichtung von Jenkins auf Ubuntu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um den Scraping- und Analyseprozess zu automatisieren, wird Jenkins auf einem Ubuntu-Server eingerichtet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Installation von Jenkins auf Ubuntu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ```bash\n",
    "   sudo apt update\n",
    "   sudo apt install openjdk-11-jre\n",
    "   wget -q -O - https://pkg.jenkins.io/debian/jenkins.io.key | sudo apt-key add -\n",
    "   sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ > /etc/apt/sources.list.d/jenkins.list'\n",
    "   sudo apt update\n",
    "   sudo apt install jenkins\n",
    "   sudo systemctl start jenkins\n",
    "   sudo systemctl enable jenkins\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der Installation ist Jenkins unter `http://localhost:8080` erreichbar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Erstellen eines neuen Jobs in Jenkins**\n",
    "   - √ñffne die Jenkins Web-Oberfl√§che.\n",
    "   - Erstelle einen neuen **Freestyle-Projekt**-Job.\n",
    "   - F√ºge den **GitHub Personal Access Token** ein, um Zugriff auf das Repository zu erhalten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Konfiguration des Build-Schritts**\n",
    "   Der folgende Shell-Befehl wird in den Build-Schritten des Jenkins-Jobs hinzugef√ºgt, um das Notebook auszuf√ºhren:\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Aktiviere das Python-Environment\n",
    ". /var/lib/jenkins/venv/bin/activate\n",
    "\n",
    "# Lade die .env-Datei und setze die Variablen\n",
    "set -a\n",
    ". /var/lib/jenkins/workspace/reddit_crypto_scraper/.env\n",
    "set +a\n",
    "\n",
    "# Debugging: Pr√ºfe, ob CLIENT_ID gesetzt wurde\n",
    "echo \"CLIENT_ID = $CLIENT_ID\"\n",
    "\n",
    "# Installiere Abh√§ngigkeiten aus requirements.txt\n",
    "pip install -r /var/lib/jenkins/workspace/reddit_crypto_scraper/requirements.txt\n",
    "\n",
    "# F√ºhre das Notebook mit papermill aus\n",
    "papermill /var/lib/jenkins/workspace/reddit_crypto_scraper/notebooks/reddit-skript.ipynb /var/lib/jenkins/workspace/reddit_crypto_scraper/notebooks/output.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorteile der Jenkins-Automatisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Regelm√§√üige Ausf√ºhrung**: Jenkins kann so konfiguriert werden, dass das Skript automatisch t√§glich oder w√∂chentlich ausgef√ºhrt wird.\n",
    "- **Monitoring & Logging**: Jenkins speichert Logs jeder Ausf√ºhrung und erm√∂glicht eine Fehleranalyse.\n",
    "- **Reproduzierbarkeit**: Durch das Laden der `.env`-Datei und die Installation aller Abh√§ngigkeiten ist jede Ausf√ºhrung identisch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit dieser Konfiguration ist die gesamte Sentiment-Analyse vollst√§ndig automatisiert und kann kontinuierlich aktualisiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Integrierung von Preisdaten "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Skript ruft historische Preisdaten f√ºr verschiedene Kryptow√§hrungen √ºber die CoinGecko API ab. Die Kryptow√§hrungen werden dabei direkt aus der bestehenden crypto_terms-Liste des Reddit-Scrapers √ºbernommen, um Konsistenz zu gew√§hrleisten. Die API-Abfragen erfolgen f√ºr die letzten 90 Tage, wobei ein Rate-Limit-Handling integriert wurde, um API-Sperren zu vermeiden. Die extrahierten Daten werden formatiert, in ein lesbares Datumsformat konvertiert und als CSV-Datei gespeichert. Dies erm√∂glicht eine sp√§tere Analyse der Preisentwicklungen im Zusammenhang mit den Reddit-Sentiment-Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì° Abrufen von Kursdaten f√ºr Bitcoin...\n",
      "‚úÖ Erfolgreich abgerufen: Bitcoin\n",
      "üì° Abrufen von Kursdaten f√ºr Ethereum...\n",
      "‚úÖ Erfolgreich abgerufen: Ethereum\n",
      "üì° Abrufen von Kursdaten f√ºr Wrapped Ethereum...\n",
      "‚ö†Ô∏è Fehler beim Abrufen der API f√ºr Wrapped Ethereum: 404\n",
      "üì° Abrufen von Kursdaten f√ºr Solana...\n",
      "‚úÖ Erfolgreich abgerufen: Solana\n",
      "üì° Abrufen von Kursdaten f√ºr Avalanche...\n",
      "‚ö†Ô∏è Fehler beim Abrufen der API f√ºr Avalanche: 404\n",
      "üì° Abrufen von Kursdaten f√ºr Polkadot...\n",
      "‚úÖ Erfolgreich abgerufen: Polkadot\n",
      "üì° Abrufen von Kursdaten f√ºr Near Protocol...\n",
      "‚ö†Ô∏è API-Limit erreicht f√ºr Near Protocol. Warte 60 Sekunden...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CoinGecko IDs aus der zentralen Liste generieren\n",
    "CRYPTO_LIST = {name: ids[0].replace(\" \", \"-\") for name, ids in crypto_terms.items()}\n",
    "\n",
    "# Zeitraum f√ºr die letzten 90 Tage\n",
    "DAYS = 90\n",
    "\n",
    "# Speicherpfad (Google Drive Sync-Ordner)\n",
    "DRIVE_PATH = \"G:/Meine Ablage/reddit/\"\n",
    "OUTPUT_FILE = os.path.join(DRIVE_PATH, \"crypto_prices.csv\")\n",
    "\n",
    "# Falls die Datei existiert, laden wir die bestehenden Daten\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    existing_df = pd.read_csv(OUTPUT_FILE, sep=\"|\", encoding=\"utf-8-sig\", parse_dates=[\"date\"])\n",
    "else:\n",
    "    existing_df = pd.DataFrame(columns=[\"date\", \"crypto\", \"price\"])\n",
    "\n",
    "# Liste zur Speicherung der neuen Kursdaten\n",
    "all_prices = []\n",
    "\n",
    "# API-Abfrage f√ºr jede Kryptow√§hrung\n",
    "for crypto_name, crypto_id in CRYPTO_LIST.items():\n",
    "    print(f\"üì° Abrufen von Kursdaten f√ºr {crypto_name}...\")\n",
    "    url = f\"https://api.coingecko.com/api/v3/coins/{crypto_id}/market_chart?vs_currency=usd&days={DAYS}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        prices = data[\"prices\"]  # Liste von [timestamp, price]\n",
    "\n",
    "        for price_data in prices:\n",
    "            all_prices.append({\n",
    "                \"date\": pd.to_datetime(price_data[0], unit=\"ms\").date(),  # Datum als YYYY-MM-DD\n",
    "                \"crypto\": crypto_name,\n",
    "                \"price\": price_data[1]\n",
    "            })\n",
    "\n",
    "        print(f\"‚úÖ Erfolgreich abgerufen: {crypto_name}\")\n",
    "    elif response.status_code == 429:\n",
    "        print(f\"‚ö†Ô∏è API-Limit erreicht f√ºr {crypto_name}. Warte 60 Sekunden...\")\n",
    "        time.sleep(60)\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Fehler beim Abrufen der API f√ºr {crypto_name}: {response.status_code}\")\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "# DataFrame mit neuen Daten erstellen\n",
    "df_prices = pd.DataFrame(all_prices)\n",
    "\n",
    "# Falls es bereits Daten gibt, Duplikate entfernen\n",
    "if not existing_df.empty:\n",
    "    combined_df = pd.concat([existing_df, df_prices])\n",
    "    combined_df = combined_df.drop_duplicates(subset=[\"date\", \"crypto\"], keep=\"last\")\n",
    "else:\n",
    "    combined_df = df_prices\n",
    "\n",
    "# Daten speichern (ersetzen, falls Datei schon existiert)\n",
    "combined_df.to_csv(OUTPUT_FILE, sep=\"|\", encoding=\"utf-8-sig\", index=False)\n",
    "\n",
    "print(f\"‚úÖ Kursdaten gespeichert, Duplikate entfernt: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Analyse und Dashboard mit Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einrichtung der Streamlit-App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streamlit ist ein einfaches Framework zur Erstellung interaktiver Dashboards mit Python. Die folgende Anwendung visualisiert die analysierten Sentiment-Daten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation der ben√∂tigten Pakete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install streamlit pandas gdown matplotlib seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufbau der Streamlit-App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende **`app.py`** Datei enth√§lt das vollst√§ndige Dashboard f√ºr die Visualisierung der analysierten Reddit-Daten:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import gdown\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "st.set_page_config(page_title=\"Krypto-Sentiment Dashboard\", layout=\"centered\")\n",
    "\n",
    "MERGED_CSV_ID = \"102W-f_u58Jvx9xBAv4IaYrOY6txk-XKL\"\n",
    "MERGED_CSV = \"reddit_merged.csv\"\n",
    "\n",
    "@st.cache_data\n",
    "def download_csv(file_id, output):\n",
    "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "    gdown.download(url, output, quiet=False)\n",
    "\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    if not os.path.exists(MERGED_CSV):\n",
    "        download_csv(MERGED_CSV_ID, MERGED_CSV)\n",
    "    df = pd.read_csv(MERGED_CSV, sep=\"|\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df[\"sentiment_score\"] = df[\"sentiment\"].map({\"positive\": 1, \"neutral\": 0, \"negative\": -1})\n",
    "    return df\n",
    "\n",
    "df_merged = load_data()\n",
    "\n",
    "st.title(\"üìä Krypto-Sentiment Dashboard\")\n",
    "\n",
    "if df_merged.empty:\n",
    "    st.warning(\"‚ö†Ô∏è Keine Daten verf√ºgbar. √úberpr√ºfe Google Drive oder lade neue Daten hoch.\")\n",
    "else:\n",
    "    st.subheader(\"üî• Top 10 meist erw√§hnte Kryptow√§hrungen\")\n",
    "    crypto_counts = df_merged[\"crypto\"].value_counts().head(10)\n",
    "    st.bar_chart(crypto_counts)\n",
    "\n",
    "    st.subheader(\"üí° Sentiment-Verteilung der Coins\")\n",
    "    sentiment_distribution = df_merged.groupby([\"crypto\", \"sentiment\"]).size().unstack(fill_value=0)\n",
    "    st.bar_chart(sentiment_distribution)\n",
    "\n",
    "    st.subheader(\"üìà Verh√§ltnis Positiv vs. Negativ\")\n",
    "    sentiment_ratio = df_merged[df_merged[\"sentiment\"] != \"neutral\"].groupby(\"sentiment\").size()\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.pie(sentiment_ratio, labels=sentiment_ratio.index, autopct=\"%1.1f%%\", startangle=90, colors=[\"green\", \"red\"])\n",
    "    ax.axis(\"equal\")\n",
    "    st.pyplot(fig)\n",
    "\n",
    "    st.subheader(\"üìÖ Sentiment-Entwicklung √ºber Zeit\")\n",
    "    crypto_options = df_merged[\"crypto\"].unique().tolist()\n",
    "    selected_crypto = st.selectbox(\"W√§hle eine Kryptow√§hrung:\", crypto_options, index=0)\n",
    "    df_filtered = df_merged[(df_merged[\"crypto\"] == selected_crypto) & (df_merged[\"sentiment\"] != \"neutral\")]\n",
    "    df_time = df_filtered.groupby([\"date\", \"sentiment\"]).size().unstack(fill_value=0)\n",
    "    st.line_chart(df_time)\n",
    "\n",
    "st.write(\"üîÑ Dashboard wird regelm√§√üig mit neuen Daten aktualisiert!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starten der Streamlit-App\n",
    "Die App kann mit folgendem Befehl gestartet werden:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "streamlit run app.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M√∂glichkeiten zur Erweiterung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Anwendung kann um weitere Visualisierungen und Features erg√§nzt werden:\n",
    "\n",
    "- Erweiterte Sentiment-Trends: Liniendiagramme f√ºr Langzeitanalysen.\n",
    "- Korrelationen zwischen Coins: Heatmaps zur Analyse von Korrelationen zwischen Kryptow√§hrungen.\n",
    "- Interaktive Benutzersteuerung: Mehr Auswahlm√∂glichkeiten f√ºr den Nutzer, wie Filter f√ºr bestimmte Zeitr√§ume oder Sentiment-Typen.\n",
    "- Integration weiterer Datenquellen: Kombination mit Twitter-Daten oder Finanzmarktdaten zur besseren Analyse der Marktentwicklung.\n",
    "- Verbesserte NLP-Modelle: Einsatz von FinBERT, CryptoBERT oder GPT-basierten Modellen, um Sentiment-Analysen genauer zu machen und Fake News oder Spam zu erkennen.\n",
    "- Prognosemodelle f√ºr Preisbewegungen: Nutzung von Regressionsmodellen, LSTMs oder Random Forests, um die Auswirkungen von Sentiment-Trends auf Kursbewegungen vorherzusagen.\n",
    "- Berechnung der logarithmischen Rendite: Analyse der Log-Rendite von Kryptow√§hrungen als Basis f√ºr Vorhersagemodelle und Risikomanagement.\n",
    "\n",
    "Durch die Nutzung von Matplotlib, Seaborn und Streamlit stehen zahlreiche M√∂glichkeiten f√ºr kreative Datenvisualisierung zur Verf√ºgung. Entwickler k√∂nnen das Dashboard kontinuierlich verbessern, neue NLP-Modelle integrieren und Predictive Analytics f√ºr den Kryptomarkt anwenden. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dennis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

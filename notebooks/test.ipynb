{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Datei geladen: G:/Meine Ablage/reddit/reddit_posts_original.csv\n",
      "üîπ Spalten: ['post_id', 'crypto', 'search_term', 'subreddit', 'title', 'author', 'date', 'time', 'score', 'num_comments', 'selftext']\n",
      "post_id         object\n",
      "crypto          object\n",
      "search_term     object\n",
      "subreddit       object\n",
      "title           object\n",
      "author          object\n",
      "date            object\n",
      "time            object\n",
      "score            int64\n",
      "num_comments     int64\n",
      "selftext        object\n",
      "dtype: object\n",
      "   post_id   crypto search_term       subreddit  \\\n",
      "0  1j0dz4r  Bitcoin     bitcoin  CryptoCurrency   \n",
      "1  1j0dghm  Bitcoin     bitcoin  CryptoCurrency   \n",
      "2  1j0cx0g  Bitcoin     bitcoin  CryptoCurrency   \n",
      "3  1j0cnlh  Bitcoin     bitcoin  CryptoCurrency   \n",
      "4  1j08i0y  Bitcoin     bitcoin  CryptoCurrency   \n",
      "\n",
      "                                               title               author  \\\n",
      "0  BlackRock Adds Its Bitcoin ETF to Model Portfo...             diwalost   \n",
      "1  x-post: As the free-float of coins is low, is ...        3fkgf9fmd980e   \n",
      "2  Companies Building on the Bitcoin Lightning Ne...            kirtash93   \n",
      "3  Michael Saylor Says 'Sell A Kidney, But Keep T...                KIG45   \n",
      "4  Teaching Bitcoin on Wall Street at $30 in 2012...  rizzobitcoinhistory   \n",
      "\n",
      "         date      time  score  num_comments selftext  \n",
      "0  2025-02-28  18:01:02     13             8      NaN  \n",
      "1  2025-02-28  17:39:22      0             1      NaN  \n",
      "2  2025-02-28  17:16:50     63            33      NaN  \n",
      "3  2025-02-28  17:06:18     34            44      NaN  \n",
      "4  2025-02-28  14:09:15    426            23      NaN  \n",
      "\n",
      "üìå Datei geladen: G:/Meine Ablage/reddit/reddit_comments_original.csv\n",
      "üîπ Spalten: ['post_id', 'comment_id', 'author', 'date', 'time', 'score', 'selftext']\n",
      "post_id       object\n",
      "comment_id    object\n",
      "author        object\n",
      "date          object\n",
      "time          object\n",
      "score          int64\n",
      "selftext      object\n",
      "dtype: object\n",
      "   post_id comment_id         author        date      time  score  \\\n",
      "0  1j0dz4r    mfafvxb       partymsl  2025-02-28  18:04:39      6   \n",
      "1  1j0dz4r    mfah0xz  coinfeeds-bot  2025-02-28  18:10:08      2   \n",
      "2  1j0dz4r    mfb16ty     DonasAskan  2025-02-28  19:43:55      1   \n",
      "3  1j0dz4r    mfakfdl       Deeujian  2025-02-28  18:26:05     -2   \n",
      "4  1j0dz4r    mfag4zu       diwalost  2025-02-28  18:05:51      1   \n",
      "\n",
      "                                            selftext  \n",
      "0      So... who said institutions are capitulating?  \n",
      "1  tldr; BlackRock, the world's largest asset man...  \n",
      "2             How does this only have 3 upvotes lol?  \n",
      "3  1-2% is nothing for Blackrock but they decided...  \n",
      "4                            They are not, retail is  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import os\n",
    "import psaw as ps\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from psaw import PushshiftAPI\n",
    "from praw.exceptions import APIException\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# üìå Pfade zu den CSV-Dateien\n",
    "DRIVE_PATH = \"G:/Meine Ablage/reddit/\"\n",
    "POSTS_CSV = os.path.join(DRIVE_PATH, \"reddit_posts_original.csv\")\n",
    "COMMENTS_CSV = os.path.join(DRIVE_PATH, \"reddit_comments_original.csv\")\n",
    "\n",
    "# üîπ **Funktion zum Laden der CSV-Dateien**\n",
    "def load_csv(filepath):\n",
    "    \"\"\"L√§dt eine CSV-Datei mit `|` als Trennzeichen und Debugging-Infos\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"‚ùå Datei nicht gefunden: {filepath}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.read_csv(filepath, sep=\"|\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "\n",
    "    print(f\"\\nüìå Datei geladen: {filepath}\")\n",
    "    print(f\"üîπ Spalten: {df.columns.tolist()}\")\n",
    "    print(df.dtypes)\n",
    "    print(df.head())\n",
    "\n",
    "    return df\n",
    "\n",
    "# üìå **Daten laden**\n",
    "df_posts = load_csv(POSTS_CSV)\n",
    "df_comments = load_csv(COMMENTS_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df_posts, df_comments, comment_threshold=500, min_length=5):\n",
    "    # 1. Duplikate entfernen\n",
    "    df_posts = df_posts.drop_duplicates(subset=[\"post_id\"])\n",
    "    df_comments = df_comments.drop_duplicates(subset=[\"comment_id\"])\n",
    "    \n",
    "    # 2. Fehlende Werte behandeln\n",
    "    df_posts['selftext'] = df_posts['selftext'].fillna('')  # Fehlende Posttexte auff√ºllen\n",
    "    df_comments['selftext'] = df_comments['selftext'].fillna('')  # Fehlende Kommentare auff√ºllen\n",
    "\n",
    "    # 3. Entferne Nutzer (Bots) mit √ºberm√§√üigen Kommentaren\n",
    "    comment_counts = df_comments[\"author\"].value_counts()\n",
    "    frequent_users = comment_counts[comment_counts > comment_threshold].index  # Nutzer √ºber Grenze\n",
    "    df_comments = df_comments[~df_comments[\"author\"].isin(frequent_users)]\n",
    "\n",
    "    # 4. Entferne zu kurze Kommentare\n",
    "    df_comments = df_comments[df_comments['selftext'].str.len() >= min_length]\n",
    "\n",
    "    print(f\"‚úÖ Daten bereinigt: {df_comments.shape[0]} Kommentare √ºbrig (nach Spam-Filter & L√§nge > {min_length}).\")\n",
    "\n",
    "    return df_posts, df_comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Daten bereinigt: 312146 Kommentare √ºbrig (nach Spam-Filter & L√§nge > 5).\n",
      "Bereinigte Posts: 9175\n",
      "Bereinigte Kommentare: 312146\n"
     ]
    }
   ],
   "source": [
    "# Bereinigen der Daten\n",
    "df_posts_clean, df_comments_clean = clean_data(df_posts, df_comments, comment_threshold=500, min_length=5)\n",
    "\n",
    "\n",
    "# √úberpr√ºfen, wie viele Eintr√§ge √ºbrig sind\n",
    "print(f\"Bereinigte Posts: {len(df_posts_clean)}\")\n",
    "print(f\"Bereinigte Kommentare: {len(df_comments_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Verwende Ger√§t: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîç Analysiere Sentiments: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 287/287 [02:41<00:00,  1.77it/s]\n",
      "üîç Analysiere Sentiments: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9755/9755 [36:15<00:00,  4.48it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sentiment-Analyse abgeschlossen: 9175 Posts & 312146 Kommentare bewertet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# GPU nutzen, falls verf√ºgbar sonst weglassen\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Verwende Ger√§t: {device}\")\n",
    "\n",
    "# üîπ CryptoBERT-Modell laden\n",
    "MODEL_NAME = \"ElKulako/cryptobert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()  # Setzt das Modell in den Evaluationsmodus\n",
    "\n",
    "# üîπ Funktion zur Sentiment-Analyse (Optimiert f√ºr Batch-Prozesse)\n",
    "def analyze_sentiment_batch(texts, batch_size=32):\n",
    "    \"\"\"Effiziente GPU-gest√ºtzte Sentiment-Analyse mit CryptoBERT f√ºr eine Liste von Texten.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Ersetze leere Eintr√§ge durch \"neutral\"\n",
    "    texts = [t if isinstance(t, str) and t.strip() != \"\" else \"neutral\" for t in texts]\n",
    "\n",
    "    # Batchweise Verarbeitung\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"üîç Analysiere Sentiments\"):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "\n",
    "        # Tokenisierung (mit Padding f√ºr Performance)\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, max_length=512, padding=True).to(device)\n",
    "\n",
    "        # Vorhersage mit CryptoBERT\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        scores = F.softmax(outputs.logits, dim=1)\n",
    "        labels = [\"bearish\", \"neutral\", \"bullish\"] \n",
    "\n",
    "        # Ergebnisse speichern\n",
    "        for i in range(len(batch_texts)):\n",
    "            sentiment = labels[torch.argmax(scores[i]).item()]\n",
    "            confidence = scores[i].max().item()\n",
    "            results.append((sentiment, confidence))\n",
    "\n",
    "    return results\n",
    "\n",
    "# üîπ Sentiment f√ºr **Posts** berechnen\n",
    "tqdm.pandas()  # Fortschrittsanzeige aktivieren\n",
    "df_posts_clean[\"full_text\"] = df_posts_clean[\"title\"] + \" \" + df_posts_clean[\"selftext\"].fillna(\"\")\n",
    "df_posts_clean[\"full_text\"] = df_posts_clean[\"full_text\"].str.strip()\n",
    "\n",
    "# Spalten 'title' und 'selftext' droppen\n",
    "df_posts_clean.drop(columns=[\"title\", \"selftext\"], inplace=True)\n",
    "\n",
    "df_posts_clean[[\"sentiment\", \"sentiment_confidence\"]] = pd.DataFrame(\n",
    "    analyze_sentiment_batch(df_posts_clean[\"full_text\"].tolist()), index=df_posts_clean.index\n",
    ")\n",
    "\n",
    "# üîπ Sentiment f√ºr **Kommentare** berechnen\n",
    "df_comments_clean[\"full_text\"] = df_comments_clean[\"selftext\"].fillna(\"\")\n",
    "\n",
    "# Spalte 'selftext' droppen\n",
    "df_comments_clean.drop(columns=[\"selftext\"], inplace=True)\n",
    "\n",
    "df_comments_clean[[\"sentiment\", \"sentiment_confidence\"]] = pd.DataFrame(\n",
    "    analyze_sentiment_batch(df_comments_clean[\"full_text\"].tolist()), index=df_comments_clean.index\n",
    ")\n",
    "\n",
    "# üîπ Ergebnisse anzeigen\n",
    "print(f\"‚úÖ Sentiment-Analyse abgeschlossen: {len(df_posts_clean)} Posts & {len(df_comments_clean)} Kommentare bewertet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = df_posts_clean.copy()\n",
    "comments = df_comments_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Merged Dataset: 321321 Eintr√§ge (Posts + Kommentare)\n",
      "   post_id comment_id  type        date      time   crypto search_term  \\\n",
      "0  1j0dz4r       None  post  2025-02-28  18:01:02  Bitcoin     bitcoin   \n",
      "1  1j0dghm       None  post  2025-02-28  17:39:22  Bitcoin     bitcoin   \n",
      "2  1j0cx0g       None  post  2025-02-28  17:16:50  Bitcoin     bitcoin   \n",
      "3  1j0cnlh       None  post  2025-02-28  17:06:18  Bitcoin     bitcoin   \n",
      "4  1j08i0y       None  post  2025-02-28  14:09:15  Bitcoin     bitcoin   \n",
      "\n",
      "        subreddit               author  \\\n",
      "0  CryptoCurrency             diwalost   \n",
      "1  CryptoCurrency        3fkgf9fmd980e   \n",
      "2  CryptoCurrency            kirtash93   \n",
      "3  CryptoCurrency                KIG45   \n",
      "4  CryptoCurrency  rizzobitcoinhistory   \n",
      "\n",
      "                                           full_text  score sentiment  \\\n",
      "0  BlackRock Adds Its Bitcoin ETF to Model Portfo...     13   neutral   \n",
      "1  x-post: As the free-float of coins is low, is ...      0   bullish   \n",
      "2  Companies Building on the Bitcoin Lightning Ne...     63   bullish   \n",
      "3  Michael Saylor Says 'Sell A Kidney, But Keep T...     34   neutral   \n",
      "4  Teaching Bitcoin on Wall Street at $30 in 2012...    426   bullish   \n",
      "\n",
      "   sentiment_confidence  \n",
      "0              0.602603  \n",
      "1              0.520766  \n",
      "2              0.669048  \n",
      "3              0.644649  \n",
      "4              0.803598  \n"
     ]
    }
   ],
   "source": [
    "# üîπ **Relevante Spalten f√ºr den Merge**\n",
    "posts = posts[[\"post_id\", \"crypto\", \"search_term\", \"subreddit\", \"author\", \"date\", \"time\", \"score\", \"full_text\", \"sentiment\", \"sentiment_confidence\"]]\n",
    "comments = comments[[\"post_id\", \"comment_id\", \"author\", \"date\", \"time\", \"score\", \"full_text\", \"sentiment\", \"sentiment_confidence\"]]\n",
    "\n",
    "# üîπ **Kommentare erben `crypto`, `search_term` und `subreddit` vom Post**\n",
    "comments = comments.merge(df_posts[[\"post_id\", \"crypto\", \"search_term\", \"subreddit\"]], on=\"post_id\", how=\"left\")\n",
    "\n",
    "# üîπ `type`-Spalte f√ºr Unterscheidung hinzuf√ºgen\n",
    "posts[\"comment_id\"] = None  # Posts haben keine comment_id\n",
    "posts[\"type\"] = \"post\"\n",
    "comments[\"type\"] = \"comment\"\n",
    "\n",
    "# üîπ **Gemeinsame Spalten f√ºr den Merge**\n",
    "common_columns = [\n",
    "    \"post_id\", \"comment_id\", \"type\", \"date\", \"time\", \"crypto\",\n",
    "    \"search_term\", \"subreddit\", \"author\",\"full_text\",\"score\", \"sentiment\", \"sentiment_confidence\",\n",
    "]\n",
    "\n",
    "# üîπ **Merging der Daten (Posts + Kommentare)**\n",
    "df_merged = pd.concat([posts[common_columns], comments[common_columns]], ignore_index=True)\n",
    "\n",
    "# üîç Debugging: √úberpr√ºfung der Gr√∂√üe\n",
    "print(f\"üìå Merged Dataset: {df_merged.shape[0]} Eintr√§ge (Posts + Kommentare)\")\n",
    "\n",
    "# üîπ √úberpr√ºfen, ob alles korrekt normalisiert wurde\n",
    "print(df_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>score</th>\n",
       "      <th>full_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1j0dz4r</td>\n",
       "      <td>mfb16ty</td>\n",
       "      <td>DonasAskan</td>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>19:43:55</td>\n",
       "      <td>1</td>\n",
       "      <td>How does this only have 3 upvotes lol?</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.816071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1j0dz4r</td>\n",
       "      <td>mfakfdl</td>\n",
       "      <td>Deeujian</td>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>18:26:05</td>\n",
       "      <td>-2</td>\n",
       "      <td>1-2% is nothing for Blackrock but they decided...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.578827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1j0dz4r</td>\n",
       "      <td>mfag4zu</td>\n",
       "      <td>diwalost</td>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>18:05:51</td>\n",
       "      <td>1</td>\n",
       "      <td>They are not, retail is</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.691663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1j0dz4r</td>\n",
       "      <td>mfbrf7n</td>\n",
       "      <td>Bear-Bull-Pig</td>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>21:46:12</td>\n",
       "      <td>1</td>\n",
       "      <td>People don't like Blackrock</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.493956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1j0dz4r</td>\n",
       "      <td>mfao0qa</td>\n",
       "      <td>diwalost</td>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>18:42:51</td>\n",
       "      <td>1</td>\n",
       "      <td>They filled for ETF when we were in bear marke...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.619266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id comment_id         author        date      time  score  \\\n",
       "2  1j0dz4r    mfb16ty     DonasAskan  2025-02-28  19:43:55      1   \n",
       "3  1j0dz4r    mfakfdl       Deeujian  2025-02-28  18:26:05     -2   \n",
       "4  1j0dz4r    mfag4zu       diwalost  2025-02-28  18:05:51      1   \n",
       "5  1j0dz4r    mfbrf7n  Bear-Bull-Pig  2025-02-28  21:46:12      1   \n",
       "6  1j0dz4r    mfao0qa       diwalost  2025-02-28  18:42:51      1   \n",
       "\n",
       "                                           full_text sentiment  \\\n",
       "2             How does this only have 3 upvotes lol?   neutral   \n",
       "3  1-2% is nothing for Blackrock but they decided...   neutral   \n",
       "4                            They are not, retail is   neutral   \n",
       "5                        People don't like Blackrock   neutral   \n",
       "6  They filled for ETF when we were in bear marke...   neutral   \n",
       "\n",
       "   sentiment_confidence  \n",
       "2              0.816071  \n",
       "3              0.578827  \n",
       "4              0.691663  \n",
       "5              0.493956  \n",
       "6              0.619266  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setze den Pfad zu deinem Google Drive Ordner\n",
    "DRIVE_PATH = \"G:/Meine Ablage/reddit/\"\n",
    "POSTS_CSV = os.path.join(DRIVE_PATH, \"reddit_posts.csv\")\n",
    "COMMENTS_CSV = os.path.join(DRIVE_PATH, \"reddit_comments.csv\")\n",
    "MERGED_CSV = os.path.join(DRIVE_PATH, \"reddit_merged.csv\")\n",
    "ORIGINAL_POSTS_CSV = os.path.join(DRIVE_PATH, \"reddit_posts_original.csv\")\n",
    "ORIGINAL_COMMENTS_CSV = os.path.join(DRIVE_PATH, \"reddit_comments_original.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(df_new, filename, key_column):\n",
    "    \"\"\"H√§ngt neue Daten an eine bestehende CSV an & entfernt Duplikate.\"\"\"\n",
    "    file_path = os.path.join(DRIVE_PATH, filename)\n",
    "\n",
    "    try:\n",
    "        # Falls Datei existiert, alte Daten einlesen\n",
    "        if os.path.exists(file_path):\n",
    "            df_existing = pd.read_csv(file_path, sep=\"|\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "            \n",
    "            # üîπ Daten zusammenf√ºhren & Duplikate nach `key_column` entfernen (neuere Werte behalten)\n",
    "            df_combined = pd.concat([df_existing, df_new], ignore_index=True).drop_duplicates(subset=[key_column], keep=\"last\")\n",
    "        else:\n",
    "            df_combined = df_new  # Falls keine Datei existiert, neue Daten direkt nutzen\n",
    "\n",
    "        # üîπ CSV speichern\n",
    "        df_combined.to_csv(\n",
    "            file_path,\n",
    "            index=False,\n",
    "            sep=\"|\",\n",
    "            encoding=\"utf-8-sig\",\n",
    "            lineterminator=\"\\n\"\n",
    "        )\n",
    "        print(f\"‚úÖ Datei erfolgreich aktualisiert: {file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Speichern der Datei {filename}: {e}\")\n",
    "\n",
    "def export_to_drive(df_posts_clean, df_comments_clean, df_merged,df_posts, df_comments):\n",
    "    \"\"\"Speichert Posts, Kommentare & die gemergte Datei mit Duplikat-Pr√ºfung.\"\"\"\n",
    "    try:\n",
    "        append_to_csv(df_posts_clean, \"reddit_posts.csv\", key_column=\"post_id\")\n",
    "        append_to_csv(df_comments_clean, \"reddit_comments.csv\", key_column=\"comment_id\")\n",
    "        append_to_csv(df_merged, \"reddit_merged.csv\", key_column=\"comment_id\")  # Falls Kommentare entscheidend sind\n",
    "        append_to_csv(df_posts, \"reddit_posts_original.csv\", key_column=\"post_id\")\n",
    "        append_to_csv(df_comments, \"reddit_comments_original.csv\", key_column=\"comment_id\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Export: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datei erfolgreich aktualisiert: G:/Meine Ablage/reddit/reddit_posts.csv\n",
      "‚úÖ Datei erfolgreich aktualisiert: G:/Meine Ablage/reddit/reddit_comments.csv\n",
      "‚úÖ Datei erfolgreich aktualisiert: G:/Meine Ablage/reddit/reddit_merged.csv\n",
      "‚úÖ Datei erfolgreich aktualisiert: G:/Meine Ablage/reddit/reddit_posts_original.csv\n",
      "‚úÖ Datei erfolgreich aktualisiert: G:/Meine Ablage/reddit/reddit_comments_original.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Export-Funktion aufrufen\n",
    "export_to_drive(df_posts_clean, df_comments_clean, df_merged,df_posts, df_comments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crypto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ Einleitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Dokumentation beschreibt die Entwicklung einer datengetriebenen Pipeline zur Analyse der Stimmungslage (Sentiment) von Kryptowährungen basierend auf Reddit-Daten. Ziel ist es, durch automatisierte Datenerfassung, Verarbeitung und Analyse wertvolle Einblicke in Markttrends zu gewinnen. Die Implementierung umfasst die wöchentliche Erfassung von Reddit-Posts und -Kommentaren, deren Bereinigung und anschließende Sentiment-Analyse. Die Ergebnisse werden durch kontinuierliche Integrationsprozesse mittels Jenkins automatisiert verarbeitet und in einem interaktiven Dashboard mit Streamlit visualisiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevanz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kryptowährungen unterliegen starken Kursschwankungen, die oft durch öffentliche Meinungen und Diskussionen in sozialen Netzwerken beeinflusst werden. Eine systematische Analyse dieser Stimmungen kann helfen:\n",
    "- Markttrends frühzeitig zu erkennen,\n",
    "- Investitionsentscheidungen zu unterstützen,\n",
    "- Risiken besser zu bewerten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zielsetzung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Automatisierte Erfassung und Speicherung von relevanten Reddit-Diskussionen über Kryptowährungen.\n",
    "- Bereinigung und Vorverarbeitung der gesammelten Daten, um eine hohe Datenqualität zu gewährleisten.\n",
    "- Anwendung von Sentiment-Analyseverfahren zur Kategorisierung und Quantifizierung von Stimmungen.\n",
    "- Automatisierte Bereitstellung der Ergebnisse in einem Dashboard zur kontinuierlichen Überwachung."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Einrichtung des Git-Repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorgehensweise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Erstellung eines öffentlichen Repositories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git init\n",
    "git remote add origin <URL>\n",
    "git add .\n",
    "git commit -m \"Initial commit\"\n",
    "git push -u origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. **Erzeugung eines Personal Access Tokens für Jenkins**\n",
    "   - Navigiere zu [GitHub Personal Access Tokens](https://github.com/settings/tokens)\n",
    "   - Erstelle ein Token mit den erforderlichen Berechtigungen für **Repo** und **Workflows**\n",
    "   - Speichere das Token sicher und hinterlege es, du brauchst es später um Authentifizierungsprozesse für CI/CD zu ermöglichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Einrichtung der Reddit API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorgehensweise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Registrierung einer Anwendung auf Reddit**\n",
    "   - Besuche die [Reddit Developer Console](https://www.reddit.com/prefs/apps)\n",
    "   - Erstelle eine neue Anwendung und wähle den Typ **script**\n",
    "   - Trage eine beliebige **App-Name**, **Beschreibung** und eine **Redirect URL** (z. B. `http://localhost:8080`) ein\n",
    "   - Notiere dir die generierte **Client ID** und **Client Secret**\n",
    "\n",
    "2. **Umgebungsvariablen setzen**\n",
    "   - Erstelle eine `.env` Datei und speichere die Anmeldeinformationen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID='deine_client_id'\n",
    "CLIENT_SECRET='dein_client_secret'\n",
    "USER_AGENT='dein_user_agent'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Einrichtung des Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Datenverarbeitung und Analyse interaktiv durchzuführen, wird ein **Jupyter Notebook** verwendet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation der benötigten Python-Bibliotheken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst müssen alle relevanten Pakete installiert werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install praw python-dotenv psaw transformers torch jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laden und Initialisieren der Reddit API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das folgende Skript lädt die API-Zugangsdaten aus der `.env` Datei und stellt eine Verbindung zur Reddit API her:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import psaw as ps\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".env geladen? True\n"
     ]
    }
   ],
   "source": [
    "# Lade die .env-Datei\n",
    "dotenv_loaded = load_dotenv(\"zugang_reddit.env\")  # Falls die Datei anders heißt, anpassen\n",
    "print(f\".env geladen? {dotenv_loaded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit API erfolgreich verbunden!\n"
     ]
    }
   ],
   "source": [
    "# Verbindung zur Reddit API\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"USER_AGENT\")\n",
    ")\n",
    "\n",
    "print(\"Reddit API erfolgreich verbunden!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testabfrage von Reddit-Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Verbindung zu überprüfen, werden die fünf heißesten Posts aus dem Subreddit `CryptoCurrency` abgerufen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Moon Week 58, Score: 8, URL: https://www.reddit.com/r/CryptoCurrency/comments/1iifsjr/moon_week_58/\n",
      "Title: Daily Crypto Discussion - February 10, 2025 (GMT+0), Score: 12, URL: https://www.reddit.com/r/CryptoCurrency/comments/1ilt27n/daily_crypto_discussion_february_10_2025_gmt0/\n",
      "Title: Just give it a minute, Score: 1713, URL: https://i.redd.it/2sz9fm4b17ie1.png\n",
      "Title: Lost Fortune: Landfill Containing $750M in Bitcoin to Be Sealed Forever, Score: 683, URL: https://news.bitcoin.com/lost-fortune-landfill-containing-750m-in-bitcoin-to-be-sealed-forever/\n",
      "Title: Odds of Kanye West Launching Token Plummet After He Says ‘Coins Prey on Fans’, Score: 104, URL: https://www.coindesk.com/markets/2025/02/08/odds-of-kanye-west-launching-token-plummet-after-he-says-coins-prey-on-fans\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    subreddit = reddit.subreddit(\"CryptoCurrency\")\n",
    "    for post in subreddit.hot(limit=5):\n",
    "        print(f\"Title: {post.title}, Score: {post.score}, URL: {post.url}\")\n",
    "except Exception as e:\n",
    "    print(f\"Fehler beim Abrufen der Posts: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieser Codeblock stellt sicher, dass die Verbindung zur Reddit API erfolgreich funktioniert. Falls es zu Fehlern kommt, sollten die API-Zugangsdaten überprüft werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition der Kryptowährungen und Subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die relevante Diskussion zu Kryptowährungen zu analysieren, wird eine Liste von Kryptowährungen mit ihren Symbolen sowie eine Auswahl an Subreddits definiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende Liste enthält die wichtigsten Kryptowährungen, die in der Analyse berücksichtigt werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cryptos = [\n",
    "    (\"Ethereum\", \"ETH\"),\n",
    "    (\"Solana\", \"SOL\"),\n",
    "    (\"Avalanche\", \"AVAX\"),\n",
    "    (\"Polkadot\", \"DOT\"),\n",
    "    (\"Near Protocol\", \"NEAR\"),\n",
    "    (\"Polygon\", \"MATIC\"),\n",
    "    (\"XRP\", \"XRP\"),\n",
    "    (\"Cardano\", \"ADA\"),\n",
    "    (\"Chainlink\", \"LINK\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur Analyse der Diskussionen werden Subreddits verwendet, die sich intensiv mit Kryptowährungen und deren Marktbewegungen beschäftigen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = [\n",
    "    \"CryptoCurrency\",  # Allgemeine Diskussionen über Kryptowährungen\n",
    "    \"CryptoMarkets\",   # Diskussionen über den Kryptomarkt und Preisbewegungen\n",
    "    \"CryptoTrading\",   # Fokus auf Trading-Strategien und Analysen\n",
    "    \"Altcoin\",         # Diskussionen über Altcoins (alle Kryptowährungen außer Bitcoin)\n",
    "    \"DeFi\",            # Decentralized Finance (DeFi) und Projekte\n",
    "    \"BitcoinBeginners\",# Für Anfänger in der Krypto-Welt\n",
    "    \"cryptotechnology\", # Fokus auf die zugrunde liegende Blockchain-Technologie\n",
    "    \"cryptocurrencies\", # Allgemeine Diskussionen über Kryptowährungen\n",
    "    \"Satoshistreetsbets\", # Krypto-Wetten und Spekulationen\n",
    "    \"Binance\"        # Diskussionen über die Binance-Plattform  \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Listen werden später verwendet, um relevante Beiträge und Kommentare aus diesen Subreddits zu extrahieren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping von Posts und Kommentaren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende Funktion ermöglicht es, gezielt Reddit-Posts und deren Kommentare für bestimmte Kryptowährungen zu extrahieren. Dabei werden sowohl der vollständige Name als auch das Kürzel (case-insesitive) der Kryptowährung als Suchbegriffe genutzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_reddit(start_date, end_date, mode = 'initial'):\n",
    "    start_timestamp = int(start_date.timestamp()) # Startdatum in Unix-Zeitstempel umwandeln\n",
    "    end_timestamp = int(end_date.timestamp())     # Enddatum in Unix-Zeitstempel umwandeln\n",
    "\n",
    "    posts = []  # Liste für die gesammelten Posts\n",
    "    comments = []  # Liste für die gesammelten Kommentare\n",
    "    post_ids = set()  # Menge für die Post-IDs, um Duplikate zu vermeiden\n",
    "\n",
    "    for crypto_name, crypto_symbol in cryptos:\n",
    "        for subreddit_name in subreddits:\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        print(f\"Scuhe nach {crypto_name} in r/{subreddit_name}\")\n",
    "\n",
    "        # Suchbegriffe in Kleinbuchstaben umwandeln\n",
    "        search_terms = [crypto_name.lower(), crypto_symbol.lower()]\n",
    "\n",
    "        for search_term in search_terms:\n",
    "            for post in subreddit.search(query=search_term, sort=\"new\", limit=None):\n",
    "                if start_timestamp <= post.created_utc <= end_timestamp and post.id not in post_ids:\n",
    "                    post.ids.add(post.id)\n",
    "\n",
    "                    # Titel und Text in Kleinbuchstaben umwandeln\n",
    "                    post_title = post.title.lower()\n",
    "                    post_selftext = post.selftext.lower()\n",
    "\n",
    "                    posts.append({\n",
    "                            'crypto': crypto_name,\n",
    "                            'search_term': search_term,\n",
    "                            'subreddit': subreddit_name,\n",
    "                            'post_id': post.id,\n",
    "                            'title': post_title,\n",
    "                            'author': str(post.author),\n",
    "                            'created_utc': datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                            'score': post.score,\n",
    "                            'num_comments': post.num_comments,\n",
    "                            'selftext': post_selftext\n",
    "                        })\n",
    "\n",
    "                    print(f\"Post gefunden: {post_title} (Suchbegriff: {search_term})\")\n",
    "\n",
    "                    # 🔹 Kommentare sammeln\n",
    "                    post.comments.replace_more(limit=0)\n",
    "                    for comment in post.comments.list():\n",
    "                        comments.append({\n",
    "                            'post_id': post.id,\n",
    "                            'comment_id': comment.id,\n",
    "                            'author': str(comment.author),\n",
    "                            'created_utc': datetime.utcfromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                            'score': comment.score,\n",
    "                            'body': (comment.body or \"\").lower()  # 🔹 Case-Insensitive Kommentartext\n",
    "                        })\n",
    "    # 🔹 Daten in DataFrames umwandeln\n",
    "    df_posts = pd.DataFrame(posts)\n",
    "    df_comments = pd.DataFrame(comments)\n",
    "\n",
    "    print(f\"Scrape abgeschlossen: {len(df_posts)} Posts und {len(df_comments)} Kommentare gefunden.\")\n",
    "    return df_posts, df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Durchführung des Scrapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Beispielaufruf für die Funktion über die letzten drei Monate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einmaliger Scrape für die letzten 3 Monate \n",
    "start_of_period = datetime(2024, 11, 1) # Startdatum für den Scrape\n",
    "now = datetime.now()  # Aktuelles Datum\n",
    "print('Starte den einmaligen Scrape für die letzten 3 Monate...')\n",
    "df_posts_initial, df_comments_initial = scrape_reddit(start_of_period, now, mode='initial')\n",
    "# Beispiel: Lokale Weiterverarbeitung\n",
    "print(\"Daten können jetzt bereinigt werden...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei dem wöchentlichen Scrape sieht der Aufruf wie folgt aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Wöchentlicher Scrape**\n",
    "now = int(datetime.now().timestamp())  # Aktueller Zeitstempel\n",
    "last_week = now - 7 * 24 * 60 * 60  # 7 Tage zurück\n",
    "print(\"🕵️ Starte den wöchentlichen Scrape...\")\n",
    "df_posts_weekly, df_comments_weekly = scrape_reddit(last_week, now, mode=\"weekly\")\n",
    "\n",
    "# Beispiel: Lokale Weiterverarbeitung\n",
    "print(\"Daten können jetzt bereinigt werden...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ich habe zunächst den einmaligen Scrape durchlaufen lassen und anschliessend den wöchentlichen automatisiert, dazu kommen wir später."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenbereinigung und Vorverarbeitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Funktion:\n",
    "- Entfernt Duplikate in Posts und Kommentaren.\n",
    "- Handhabt fehlende Werte, um Datenverluste zu vermeiden.\n",
    "- Konvertiert und strukturiert Zeitstempel für bessere Nachvollziehbarkeit.\n",
    "- Filtert Beiträge und Kommentare basierend auf Qualität (z. B. Score).\n",
    "- Entfernt Kommentare von Accounts mit  übermäßig vielen Kommentaren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df_posts, df_comments, comment_threshold=500):# Anpassbarer Schwellenwert für Kommentare pro Nutzer\n",
    "    # 1. Duplikate entfernen\n",
    "    df_posts = df_posts.drop_duplicates(subset=[\"post_id\"])\n",
    "    df_comments = df_comments.drop_duplicates(subset=[\"comment_id\"])\n",
    "    \n",
    "    # 2. Fehlende Werte behandeln\n",
    "    df_posts['selftext'] = df_posts['selftext'].fillna('')  # Fehlende Posttexte auffüllen\n",
    "    df_comments['body'] = df_comments['body'].fillna('')  # Fehlende Kommentare auffüllen\n",
    "    \n",
    "    # 3. Zeitstempel konvertieren\n",
    "    df_posts['created_utc'] = pd.to_datetime(df_posts['created_utc'])\n",
    "    df_comments['created_utc'] = pd.to_datetime(df_comments['created_utc'])\n",
    "\n",
    "    # 4. Datum & Uhrzeit in separate Spalten aufteilen (Daten normalisieren)\n",
    "    df_posts[\"date\"] = df_posts[\"created_utc\"].dt.date  # YYYY-MM-DD\n",
    "    df_posts[\"time\"] = df_posts[\"created_utc\"].dt.time  # HH:MM:SS\n",
    "\n",
    "    df_comments[\"date\"] = df_comments[\"created_utc\"].dt.date\n",
    "    df_comments[\"time\"] = df_comments[\"created_utc\"].dt.time\n",
    "\n",
    "    # 5. Original `created_utc`-Spalte entfernen\n",
    "    df_posts.drop(columns=[\"created_utc\"], inplace=True)\n",
    "    df_comments.drop(columns=[\"created_utc\"], inplace=True)\n",
    "\n",
    "    # 6. Filterung nach Qualität (Spam oder irrelevante Daten entfernen)\n",
    "    df_posts = df_posts[df_posts['score'] > 0]  # Posts mit negativem Score entfernen\n",
    "    df_comments = df_comments[df_comments['score'] > 0]  # Kommentare mit negativem Score entfernen\n",
    "\n",
    "    # 7. Entferne Nutzer (bots) mit übermäßigen Kommentaren\n",
    "    comment_counts = df_comments[\"author\"].value_counts()\n",
    "    frequent_users = comment_counts[comment_counts > comment_threshold].index  # Nutzer über Grenze\n",
    "    df_comments = df_comments[~df_comments[\"author\"].isin(frequent_users)]\n",
    "\n",
    "    print(f\"Daten bereinigt: {df_comments.shape[0]} Kommentare übrig (nach Spam-Filter).\")\n",
    "\n",
    "    return df_posts, df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anwendung der Bereinigungsfunktion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der Extraktion der Daten wird die Bereinigungsfunktion auf die Posts und Kommentare angewendet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bereinigen der Daten\n",
    "df_posts_clean, df_comments_clean = clean_data(df_posts_initial, df_comments_initial, comment_threshold=300) # Anpassbarer Schwellenwert für Kommentare pro Nutzer\n",
    "\n",
    "\n",
    "# Überprüfen, wie viele Einträge übrig sind\n",
    "print(f\"Bereinigte Posts: {len(df_posts_clean)}\")\n",
    "print(f\"Bereinigte Kommentare: {len(df_comments_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Schritte stellen sicher, dass nur relevante, qualitativ hochwertige Daten in der Pipeline weiterverarbeitet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment-Analyse der Kommentare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der Bereinigung der Daten wird das Sentiment für die gesammelten Kommentare mithilfe eines vortrainierten Modells analysiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verwendetes Sentiment-Analyse-Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Modell \"ElKulako/crypto-bert\" stammt von ElKulako und wurde speziell für die Analyse von Kryptowährungs-Diskussionen entwickelt. Es basiert auf der BERT-Architektur und klassifiziert Texte als bullish (positiv), neutral oder bearish (negativ). Das Modell wurde auf umfangreichen Finanz- und Krypto-spezifischen Daten trainiert, wodurch es sich besonders gut für die Analyse von Reddit-Kommentaren eignet, die sich mit dem Krypto-Markt befassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CryptoBERT-Modell laden\n",
    "MODEL_NAME = \"ElKulako/crypto-bert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 🔹 Sicherstellen, dass die Spalte \"body\" existiert\n",
    "if \"body\" not in df_comments_clean.columns:\n",
    "    raise ValueError(\"Fehler: Die CSV-Datei enthält keine 'body'-Spalte mit Kommentaren!\")\n",
    "\n",
    "# 🔹 Funktion zur Sentiment-Analyse mit CryptoBERT\n",
    "def analyze_sentiment(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return \"neutral\", 0.0  # Leere Kommentare sind neutral\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    scores = F.softmax(outputs.logits, dim=1)[0]\n",
    "    labels = [\"bearish\", \"neutral\", \"bullish\"]  # CryptoBERT nutzt diese Labels\n",
    "    sentiment = labels[torch.argmax(scores).item()]\n",
    "    confidence = scores.max().item()\n",
    "\n",
    "    return sentiment, confidence\n",
    "\n",
    "# 🔹 Sentiment für alle Kommentare berechnen\n",
    "df_comments_clean[\"sentiment\"], df_comments_clean[\"sentiment_confidence\"] = zip(*df_comments_clean[\"body\"].map(analyze_sentiment))\n",
    "\n",
    "# 🔹 Debug-Ausgabe: Zeigt die ersten 5 Ergebnisse zur Überprüfung\n",
    "print(df_comments_clean[[\"body\", \"sentiment\", \"sentiment_confidence\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem die Sentiment-Analyse durchgeführt wurde, werden die bereinigten Posts und Kommentare zusammengeführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging der Posts & Kommentare direkt nach der Bereinigung\n",
    "df_merged = df_comments_clean.merge(df_posts_clean, on=\"post_id\", how=\"left\")\n",
    "\n",
    "# Fehlende Werte entfernen (optional)\n",
    "df_merged.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieser Schritt stellt sicher, dass alle relevanten Kommentare mit ihren zugehörigen Posts verknüpft sind sodass wir neben den normalisierten Post- und Kommentar-Tabelle eine fertige Tabelle für Analysen haben.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export und Speicherung der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zweck der Speicherung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die exportierten Daten enthalten die gesammelten Reddit-Posts und Kommentare sowie deren Sentiment-Analyse. Durch die Speicherung in **Google Drive** wird eine einfache Automatisierung ermöglicht, sodass die Daten regelmäßig aktualisiert und für nachfolgende Analysen oder Machine-Learning-Prozesse verfügbar sind.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementierung des Exports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den Pfad definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setze den Pfad zu deinem Google Drive Ordner\n",
    "DRIVE_PATH = \"G:/Meine Ablage/reddit/\"\n",
    "POSTS_CSV = os.path.join(DRIVE_PATH, \"reddit_posts.csv\")\n",
    "COMMENTS_CSV = os.path.join(DRIVE_PATH, \"reddit_comments.csv\")\n",
    "MERGED_CSV = os.path.join(DRIVE_PATH, \"reddit_merged.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende Funktion speichert die extrahierten und verarbeiteten Daten als CSV-Dateien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_initial_csv(df_new, filename):\n",
    "    \"\"\"Speichert die erste CSV-Datei ohne Anhängen oder Duplikat-Prüfung.\"\"\"\n",
    "    file_path = os.path.join(DRIVE_PATH, filename)\n",
    "    try:\n",
    "        df_new.to_csv(file_path, index=False, sep=\"|\", encoding=\"utf-8-sig\", lineterminator=\"\\n\")\n",
    "        print(f\"✅ Datei erfolgreich gespeichert: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Speichern der Datei {filename}: {e}\")\n",
    "\n",
    "def export_initial_data(df_posts, df_comments, df_merged):\n",
    "    \"\"\"Speichert die initialen Posts, Kommentare & gemergten Daten.\"\"\"\n",
    "    try:\n",
    "        save_initial_csv(df_posts, \"reddit_posts.csv\")\n",
    "        save_initial_csv(df_comments, \"reddit_comments.csv\")\n",
    "        save_initial_csv(df_merged, \"reddit_merged.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Export: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export-Funktion für den ersten Scrape aufrufen\n",
    "export_initial_data(df_posts_clean, df_comments_clean, df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vorteile der Speicherung in Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Automatisierung**: Die exportierten Daten können regelmäßig durch Skripte oder Jenkins-Jobs aktualisiert werden.\n",
    "- **Zugänglichkeit**: Die gespeicherten Dateien können von verschiedenen Systemen oder Nutzern für Analysen oder Machine-Learning-Modelle verwendet werden.\n",
    "- **Versionskontrolle**: Historische Daten können gespeichert werden, um Entwicklungen über die Zeit hinweg zu analysieren.\n",
    "\n",
    "Dieser Schritt stellt sicher, dass die verarbeiteten Daten langfristig verfügbar bleiben und kontinuierlich für weitere Analysen genutzt werden können.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Automatisierung mit Jenkins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einrichtung von Jenkins auf Ubuntu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um den Scraping- und Analyseprozess zu automatisieren, wird Jenkins auf einem Ubuntu-Server eingerichtet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Installation von Jenkins auf Ubuntu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ```bash\n",
    "   sudo apt update\n",
    "   sudo apt install openjdk-11-jre\n",
    "   wget -q -O - https://pkg.jenkins.io/debian/jenkins.io.key | sudo apt-key add -\n",
    "   sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ > /etc/apt/sources.list.d/jenkins.list'\n",
    "   sudo apt update\n",
    "   sudo apt install jenkins\n",
    "   sudo systemctl start jenkins\n",
    "   sudo systemctl enable jenkins\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der Installation ist Jenkins unter `http://localhost:8080` erreichbar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Erstellen eines neuen Jobs in Jenkins**\n",
    "   - Öffne die Jenkins Web-Oberfläche.\n",
    "   - Erstelle einen neuen **Freestyle-Projekt**-Job.\n",
    "   - Füge den **GitHub Personal Access Token** ein, um Zugriff auf das Repository zu erhalten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Konfiguration des Build-Schritts**\n",
    "   Der folgende Shell-Befehl wird in den Build-Schritten des Jenkins-Jobs hinzugefügt, um das Notebook auszuführen:\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aktiviere das Python-Environment\n",
    ". /var/lib/jenkins/venv/bin/activate\n",
    "\n",
    "# Lade die .env-Datei und setze die Variablen\n",
    "set -a\n",
    ". /var/lib/jenkins/workspace/reddit_crypto_scraper/.env\n",
    "set +a\n",
    "\n",
    "# Debugging: Prüfe, ob CLIENT_ID gesetzt wurde\n",
    "echo \"CLIENT_ID = $CLIENT_ID\"\n",
    "\n",
    "# Installiere Abhängigkeiten aus requirements.txt\n",
    "pip install -r /var/lib/jenkins/workspace/reddit_crypto_scraper/requirements.txt\n",
    "\n",
    "# Führe das Notebook mit papermill aus\n",
    "papermill /var/lib/jenkins/workspace/reddit_crypto_scraper/notebooks/reddit-skript.ipynb /var/lib/jenkins/workspace/reddit_crypto_scraper/notebooks/output.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorteile der Jenkins-Automatisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Regelmäßige Ausführung**: Jenkins kann so konfiguriert werden, dass das Skript automatisch täglich oder wöchentlich ausgeführt wird.\n",
    "- **Monitoring & Logging**: Jenkins speichert Logs jeder Ausführung und ermöglicht eine Fehleranalyse.\n",
    "- **Reproduzierbarkeit**: Durch das Laden der `.env`-Datei und die Installation aller Abhängigkeiten ist jede Ausführung identisch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit dieser Konfiguration ist die gesamte Sentiment-Analyse vollständig automatisiert und kann kontinuierlich aktualisiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Analyse und Dashboard mit Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einrichtung der Streamlit-App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streamlit ist ein einfaches Framework zur Erstellung interaktiver Dashboards mit Python. Die folgende Anwendung visualisiert die analysierten Sentiment-Daten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation der benötigten Pakete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```bash\n",
    "pip install streamlit pandas gdown matplotlib seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufbau der Streamlit-App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende **`app.py`** Datei enthält das vollständige Dashboard für die Visualisierung der analysierten Reddit-Daten:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import gdown\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "st.set_page_config(page_title=\"Krypto-Sentiment Dashboard\", layout=\"centered\")\n",
    "\n",
    "MERGED_CSV_ID = \"102W-f_u58Jvx9xBAv4IaYrOY6txk-XKL\"\n",
    "MERGED_CSV = \"reddit_merged.csv\"\n",
    "\n",
    "@st.cache_data\n",
    "def download_csv(file_id, output):\n",
    "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "    gdown.download(url, output, quiet=False)\n",
    "\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    if not os.path.exists(MERGED_CSV):\n",
    "        download_csv(MERGED_CSV_ID, MERGED_CSV)\n",
    "    df = pd.read_csv(MERGED_CSV, sep=\"|\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df[\"sentiment_score\"] = df[\"sentiment\"].map({\"positive\": 1, \"neutral\": 0, \"negative\": -1})\n",
    "    return df\n",
    "\n",
    "df_merged = load_data()\n",
    "\n",
    "st.title(\"📊 Krypto-Sentiment Dashboard\")\n",
    "\n",
    "if df_merged.empty:\n",
    "    st.warning(\"⚠️ Keine Daten verfügbar. Überprüfe Google Drive oder lade neue Daten hoch.\")\n",
    "else:\n",
    "    st.subheader(\"🔥 Top 10 meist erwähnte Kryptowährungen\")\n",
    "    crypto_counts = df_merged[\"crypto\"].value_counts().head(10)\n",
    "    st.bar_chart(crypto_counts)\n",
    "\n",
    "    st.subheader(\"💡 Sentiment-Verteilung der Coins\")\n",
    "    sentiment_distribution = df_merged.groupby([\"crypto\", \"sentiment\"]).size().unstack(fill_value=0)\n",
    "    st.bar_chart(sentiment_distribution)\n",
    "\n",
    "    st.subheader(\"📈 Verhältnis Positiv vs. Negativ\")\n",
    "    sentiment_ratio = df_merged[df_merged[\"sentiment\"] != \"neutral\"].groupby(\"sentiment\").size()\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.pie(sentiment_ratio, labels=sentiment_ratio.index, autopct=\"%1.1f%%\", startangle=90, colors=[\"green\", \"red\"])\n",
    "    ax.axis(\"equal\")\n",
    "    st.pyplot(fig)\n",
    "\n",
    "    st.subheader(\"📅 Sentiment-Entwicklung über Zeit\")\n",
    "    crypto_options = df_merged[\"crypto\"].unique().tolist()\n",
    "    selected_crypto = st.selectbox(\"Wähle eine Kryptowährung:\", crypto_options, index=0)\n",
    "    df_filtered = df_merged[(df_merged[\"crypto\"] == selected_crypto) & (df_merged[\"sentiment\"] != \"neutral\")]\n",
    "    df_time = df_filtered.groupby([\"date\", \"sentiment\"]).size().unstack(fill_value=0)\n",
    "    st.line_chart(df_time)\n",
    "\n",
    "st.write(\"🔄 Dashboard wird regelmäßig mit neuen Daten aktualisiert!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starten der Streamlit-App\n",
    "Die App kann mit folgendem Befehl gestartet werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```bash\n",
    "streamlit run app.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Möglichkeiten zur Erweiterung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Anwendung kann um weitere **Visualisierungen und Features** ergänzt werden:\n",
    "- **Erweiterte Sentiment-Trends**: Liniendiagramme für Langzeitanalysen.\n",
    "- **Korrelationen zwischen Coins**: Heatmaps zur Analyse von Korrelationen zwischen Kryptowährungen.\n",
    "- **Interaktive Benutzersteuerung**: Mehr Auswahlmöglichkeiten für den Nutzer, wie Filter für bestimmte Zeiträume oder Sentiment-Typen.\n",
    "- **Integration weiterer Datenquellen**: Kombination mit Twitter-Daten oder Finanzmarktdaten zur besseren Analyse der Marktentwicklung.\n",
    "\n",
    "Durch die Nutzung von **Matplotlib, Seaborn und Streamlit** stehen zahlreiche Möglichkeiten für kreative Datenvisualisierung zur Verfügung. Entwickler können das Dashboard kontinuierlich verbessern und an neue Anforderungen anpassen.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dennis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

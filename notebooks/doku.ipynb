{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Einleitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Dokumentation beschreibt die Entwicklung einer datengetriebenen Pipeline zur Analyse der Stimmungslage (Sentiment) von Kryptow√§hrungen basierend auf Reddit-Daten. Ziel ist es, durch automatisierte Datenerfassung, Verarbeitung und Analyse wertvolle Einblicke in Markttrends zu gewinnen. Die Implementierung umfasst die w√∂chentliche Erfassung von Reddit-Posts und -Kommentaren, deren Bereinigung und anschlie√üende Sentiment-Analyse. Die Ergebnisse werden durch kontinuierliche Integrationsprozesse mittels Jenkins automatisiert verarbeitet und in einem interaktiven Dashboard mit Streamlit visualisiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevanz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kryptow√§hrungen unterliegen starken Kursschwankungen, die oft durch √∂ffentliche Meinungen und Diskussionen in sozialen Netzwerken beeinflusst werden. Eine systematische Analyse dieser Stimmungen kann helfen:\n",
    "- Markttrends fr√ºhzeitig zu erkennen,\n",
    "- Investitionsentscheidungen zu unterst√ºtzen,\n",
    "- Risiken besser zu bewerten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zielsetzung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Automatisierte Erfassung und Speicherung von relevanten Reddit-Diskussionen √ºber Kryptow√§hrungen.\n",
    "- Bereinigung und Vorverarbeitung der gesammelten Daten, um eine hohe Datenqualit√§t zu gew√§hrleisten.\n",
    "- Anwendung von Sentiment-Analyseverfahren zur Kategorisierung und Quantifizierung von Stimmungen.\n",
    "- Automatisierte Bereitstellung der Ergebnisse in einem Dashboard zur kontinuierlichen √úberwachung."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Einrichtung des Git-Repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorgehensweise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Erstellung eines √∂ffentlichen Repositories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git init\n",
    "git remote add origin <URL>\n",
    "git add .\n",
    "git commit -m \"Initial commit\"\n",
    "git push -u origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. **Erzeugung eines Personal Access Tokens f√ºr Jenkins**\n",
    "   - Navigiere zu [GitHub Personal Access Tokens](https://github.com/settings/tokens)\n",
    "   - Erstelle ein Token mit den erforderlichen Berechtigungen f√ºr **Repo** und **Workflows**\n",
    "   - Speichere das Token sicher und hinterlege es, du brauchst es sp√§ter um Authentifizierungsprozesse f√ºr CI/CD zu erm√∂glichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Einrichtung der Reddit API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorgehensweise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Registrierung einer Anwendung auf Reddit**\n",
    "   - Besuche die [Reddit Developer Console](https://www.reddit.com/prefs/apps)\n",
    "   - Erstelle eine neue Anwendung und w√§hle den Typ **script**\n",
    "   - Trage eine beliebige **App-Name**, **Beschreibung** und eine **Redirect URL** (z. B. `http://localhost:8080`) ein\n",
    "   - Notiere dir die generierte **Client ID** und **Client Secret**\n",
    "\n",
    "2. **Umgebungsvariablen setzen**\n",
    "   - Erstelle eine `.env` Datei und speichere die Anmeldeinformationen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID='deine_client_id'\n",
    "CLIENT_SECRET='dein_client_secret'\n",
    "USER_AGENT='dein_user_agent'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Einrichtung des Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Datenverarbeitung und Analyse interaktiv durchzuf√ºhren, wird ein **Jupyter Notebook** verwendet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation der ben√∂tigten Python-Bibliotheken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zun√§chst m√ºssen alle relevanten Pakete installiert werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install praw python-dotenv psaw transformers torch jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laden und Initialisieren der Reddit API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das folgende Skript l√§dt die API-Zugangsdaten aus der `.env` Datei und stellt eine Verbindung zur Reddit API her:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import psaw as ps\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".env geladen? True\n"
     ]
    }
   ],
   "source": [
    "# Lade die .env-Datei\n",
    "dotenv_loaded = load_dotenv(\"zugang_reddit.env\")  # Falls die Datei anders hei√üt, anpassen\n",
    "print(f\".env geladen? {dotenv_loaded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit API erfolgreich verbunden!\n"
     ]
    }
   ],
   "source": [
    "# Verbindung zur Reddit API\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"USER_AGENT\")\n",
    ")\n",
    "\n",
    "print(\"Reddit API erfolgreich verbunden!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testabfrage von Reddit-Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Verbindung zu √ºberpr√ºfen, werden die f√ºnf hei√üesten Posts aus dem Subreddit `CryptoCurrency` abgerufen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Moon Week 58, Score: 8, URL: https://www.reddit.com/r/CryptoCurrency/comments/1iifsjr/moon_week_58/\n",
      "Title: Daily Crypto Discussion - February 10, 2025 (GMT+0), Score: 12, URL: https://www.reddit.com/r/CryptoCurrency/comments/1ilt27n/daily_crypto_discussion_february_10_2025_gmt0/\n",
      "Title: Just give it a minute, Score: 1713, URL: https://i.redd.it/2sz9fm4b17ie1.png\n",
      "Title: Lost Fortune: Landfill Containing $750M in Bitcoin to Be Sealed Forever, Score: 683, URL: https://news.bitcoin.com/lost-fortune-landfill-containing-750m-in-bitcoin-to-be-sealed-forever/\n",
      "Title: Odds of Kanye West Launching Token Plummet After He Says ‚ÄòCoins Prey on Fans‚Äô, Score: 104, URL: https://www.coindesk.com/markets/2025/02/08/odds-of-kanye-west-launching-token-plummet-after-he-says-coins-prey-on-fans\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    subreddit = reddit.subreddit(\"CryptoCurrency\")\n",
    "    for post in subreddit.hot(limit=5):\n",
    "        print(f\"Title: {post.title}, Score: {post.score}, URL: {post.url}\")\n",
    "except Exception as e:\n",
    "    print(f\"Fehler beim Abrufen der Posts: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieser Codeblock stellt sicher, dass die Verbindung zur Reddit API erfolgreich funktioniert. Falls es zu Fehlern kommt, sollten die API-Zugangsdaten √ºberpr√ºft werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition der Kryptow√§hrungen und Subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die relevante Diskussion zu Kryptow√§hrungen zu analysieren, wird eine Liste von Kryptow√§hrungen mit ihren Symbolen sowie eine Auswahl an Subreddits definiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende Liste enth√§lt die wichtigsten Kryptow√§hrungen, die in der Analyse ber√ºcksichtigt werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cryptos = [\n",
    "    (\"Ethereum\", \"ETH\"),\n",
    "    (\"Solana\", \"SOL\"),\n",
    "    (\"Avalanche\", \"AVAX\"),\n",
    "    (\"Polkadot\", \"DOT\"),\n",
    "    (\"Near Protocol\", \"NEAR\"),\n",
    "    (\"Polygon\", \"MATIC\"),\n",
    "    (\"XRP\", \"XRP\"),\n",
    "    (\"Cardano\", \"ADA\"),\n",
    "    (\"Chainlink\", \"LINK\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur Analyse der Diskussionen werden Subreddits verwendet, die sich intensiv mit Kryptow√§hrungen und deren Marktbewegungen besch√§ftigen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = [\n",
    "    \"CryptoCurrency\",  # Allgemeine Diskussionen √ºber Kryptow√§hrungen\n",
    "    \"CryptoMarkets\",   # Diskussionen √ºber den Kryptomarkt und Preisbewegungen\n",
    "    \"CryptoTrading\",   # Fokus auf Trading-Strategien und Analysen\n",
    "    \"Altcoin\",         # Diskussionen √ºber Altcoins (alle Kryptow√§hrungen au√üer Bitcoin)\n",
    "    \"DeFi\",            # Decentralized Finance (DeFi) und Projekte\n",
    "    \"BitcoinBeginners\",# F√ºr Anf√§nger in der Krypto-Welt\n",
    "    \"cryptotechnology\", # Fokus auf die zugrunde liegende Blockchain-Technologie\n",
    "    \"cryptocurrencies\", # Allgemeine Diskussionen √ºber Kryptow√§hrungen\n",
    "    \"Satoshistreetsbets\", # Krypto-Wetten und Spekulationen\n",
    "    \"Binance\"        # Diskussionen √ºber die Binance-Plattform  \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Listen werden sp√§ter verwendet, um relevante Beitr√§ge und Kommentare aus diesen Subreddits zu extrahieren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping von Posts und Kommentaren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende Funktion erm√∂glicht es, gezielt Reddit-Posts und deren Kommentare f√ºr bestimmte Kryptow√§hrungen zu extrahieren. Dabei werden sowohl der vollst√§ndige Name als auch das K√ºrzel (case-insesitive) der Kryptow√§hrung als Suchbegriffe genutzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_reddit(start_date, end_date, mode = 'initial'):\n",
    "    start_timestamp = int(start_date.timestamp()) # Startdatum in Unix-Zeitstempel umwandeln\n",
    "    end_timestamp = int(end_date.timestamp())     # Enddatum in Unix-Zeitstempel umwandeln\n",
    "\n",
    "    posts = []  # Liste f√ºr die gesammelten Posts\n",
    "    comments = []  # Liste f√ºr die gesammelten Kommentare\n",
    "    post_ids = set()  # Menge f√ºr die Post-IDs, um Duplikate zu vermeiden\n",
    "\n",
    "    for crypto_name, crypto_symbol in cryptos:\n",
    "        for subreddit_name in subreddits:\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        print(f\"Scuhe nach {crypto_name} in r/{subreddit_name}\")\n",
    "\n",
    "        # Suchbegriffe in Kleinbuchstaben umwandeln\n",
    "        search_terms = [crypto_name.lower(), crypto_symbol.lower()]\n",
    "\n",
    "        for search_term in search_terms:\n",
    "            for post in subreddit.search(query=search_term, sort=\"new\", limit=None):\n",
    "                if start_timestamp <= post.created_utc <= end_timestamp and post.id not in post_ids:\n",
    "                    post.ids.add(post.id)\n",
    "\n",
    "                    # Titel und Text in Kleinbuchstaben umwandeln\n",
    "                    post_title = post.title.lower()\n",
    "                    post_selftext = post.selftext.lower()\n",
    "\n",
    "                    posts.append({\n",
    "                            'crypto': crypto_name,\n",
    "                            'search_term': search_term,\n",
    "                            'subreddit': subreddit_name,\n",
    "                            'post_id': post.id,\n",
    "                            'title': post_title,\n",
    "                            'author': str(post.author),\n",
    "                            'created_utc': datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                            'score': post.score,\n",
    "                            'num_comments': post.num_comments,\n",
    "                            'selftext': post_selftext\n",
    "                        })\n",
    "\n",
    "                    print(f\"Post gefunden: {post_title} (Suchbegriff: {search_term})\")\n",
    "\n",
    "                    # üîπ Kommentare sammeln\n",
    "                    post.comments.replace_more(limit=0)\n",
    "                    for comment in post.comments.list():\n",
    "                        comments.append({\n",
    "                            'post_id': post.id,\n",
    "                            'comment_id': comment.id,\n",
    "                            'author': str(comment.author),\n",
    "                            'created_utc': datetime.utcfromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                            'score': comment.score,\n",
    "                            'body': (comment.body or \"\").lower()  # üîπ Case-Insensitive Kommentartext\n",
    "                        })\n",
    "    # üîπ Daten in DataFrames umwandeln\n",
    "    df_posts = pd.DataFrame(posts)\n",
    "    df_comments = pd.DataFrame(comments)\n",
    "\n",
    "    print(f\"Scrape abgeschlossen: {len(df_posts)} Posts und {len(df_comments)} Kommentare gefunden.\")\n",
    "    return df_posts, df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Durchf√ºhrung des Scrapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Beispielaufruf f√ºr die Funktion √ºber die letzten drei Monate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einmaliger Scrape f√ºr die letzten 3 Monate \n",
    "start_of_period = datetime(2024, 11, 1) # Startdatum f√ºr den Scrape\n",
    "now = datetime.now()  # Aktuelles Datum\n",
    "print('Starte den einmaligen Scrape f√ºr die letzten 3 Monate...')\n",
    "df_posts_initial, df_comments_initial = scrape_reddit(start_of_period, now, mode='initial')\n",
    "# Beispiel: Lokale Weiterverarbeitung\n",
    "print(\"Daten k√∂nnen jetzt bereinigt werden...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei dem w√∂chentlichen Scrape sieht der Aufruf wie folgt aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **W√∂chentlicher Scrape**\n",
    "now = int(datetime.now().timestamp())  # Aktueller Zeitstempel\n",
    "last_week = now - 7 * 24 * 60 * 60  # 7 Tage zur√ºck\n",
    "print(\"üïµÔ∏è Starte den w√∂chentlichen Scrape...\")\n",
    "df_posts_weekly, df_comments_weekly = scrape_reddit(last_week, now, mode=\"weekly\")\n",
    "\n",
    "# Beispiel: Lokale Weiterverarbeitung\n",
    "print(\"Daten k√∂nnen jetzt bereinigt werden...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ich habe zun√§chst den einmaligen Scrape durchlaufen lassen und anschliessend den w√∂chentlichen automatisiert, dazu kommen wir sp√§ter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenbereinigung und Vorverarbeitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Funktion:\n",
    "- Entfernt Duplikate in Posts und Kommentaren.\n",
    "- Handhabt fehlende Werte, um Datenverluste zu vermeiden.\n",
    "- Konvertiert und strukturiert Zeitstempel f√ºr bessere Nachvollziehbarkeit.\n",
    "- Filtert Beitr√§ge und Kommentare basierend auf Qualit√§t (z. B. Score).\n",
    "- Entfernt Kommentare von Accounts mit  √ºberm√§√üig vielen Kommentaren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df_posts, df_comments, comment_threshold=500):# Anpassbarer Schwellenwert f√ºr Kommentare pro Nutzer\n",
    "    # 1. Duplikate entfernen\n",
    "    df_posts = df_posts.drop_duplicates(subset=[\"post_id\"])\n",
    "    df_comments = df_comments.drop_duplicates(subset=[\"comment_id\"])\n",
    "    \n",
    "    # 2. Fehlende Werte behandeln\n",
    "    df_posts['selftext'] = df_posts['selftext'].fillna('')  # Fehlende Posttexte auff√ºllen\n",
    "    df_comments['body'] = df_comments['body'].fillna('')  # Fehlende Kommentare auff√ºllen\n",
    "    \n",
    "    # 3. Zeitstempel konvertieren\n",
    "    df_posts['created_utc'] = pd.to_datetime(df_posts['created_utc'])\n",
    "    df_comments['created_utc'] = pd.to_datetime(df_comments['created_utc'])\n",
    "\n",
    "    # 4. Datum & Uhrzeit in separate Spalten aufteilen (Daten normalisieren)\n",
    "    df_posts[\"date\"] = df_posts[\"created_utc\"].dt.date  # YYYY-MM-DD\n",
    "    df_posts[\"time\"] = df_posts[\"created_utc\"].dt.time  # HH:MM:SS\n",
    "\n",
    "    df_comments[\"date\"] = df_comments[\"created_utc\"].dt.date\n",
    "    df_comments[\"time\"] = df_comments[\"created_utc\"].dt.time\n",
    "\n",
    "    # 5. Original `created_utc`-Spalte entfernen\n",
    "    df_posts.drop(columns=[\"created_utc\"], inplace=True)\n",
    "    df_comments.drop(columns=[\"created_utc\"], inplace=True)\n",
    "\n",
    "    # 6. Filterung nach Qualit√§t (Spam oder irrelevante Daten entfernen)\n",
    "    df_posts = df_posts[df_posts['score'] > 0]  # Posts mit negativem Score entfernen\n",
    "    df_comments = df_comments[df_comments['score'] > 0]  # Kommentare mit negativem Score entfernen\n",
    "\n",
    "    # 7. Entferne Nutzer (bots) mit √ºberm√§√üigen Kommentaren\n",
    "    comment_counts = df_comments[\"author\"].value_counts()\n",
    "    frequent_users = comment_counts[comment_counts > comment_threshold].index  # Nutzer √ºber Grenze\n",
    "    df_comments = df_comments[~df_comments[\"author\"].isin(frequent_users)]\n",
    "\n",
    "    print(f\"Daten bereinigt: {df_comments.shape[0]} Kommentare √ºbrig (nach Spam-Filter).\")\n",
    "\n",
    "    return df_posts, df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anwendung der Bereinigungsfunktion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der Extraktion der Daten wird die Bereinigungsfunktion auf die Posts und Kommentare angewendet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bereinigen der Daten\n",
    "df_posts_clean, df_comments_clean = clean_data(df_posts_initial, df_comments_initial, comment_threshold=300) # Anpassbarer Schwellenwert f√ºr Kommentare pro Nutzer\n",
    "\n",
    "\n",
    "# √úberpr√ºfen, wie viele Eintr√§ge √ºbrig sind\n",
    "print(f\"Bereinigte Posts: {len(df_posts_clean)}\")\n",
    "print(f\"Bereinigte Kommentare: {len(df_comments_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Schritte stellen sicher, dass nur relevante, qualitativ hochwertige Daten in der Pipeline weiterverarbeitet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment-Analyse der Kommentare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der Bereinigung der Daten wird das Sentiment f√ºr die gesammelten Kommentare mithilfe eines vortrainierten Modells analysiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verwendetes Sentiment-Analyse-Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Modell \"ElKulako/crypto-bert\" stammt von ElKulako und wurde speziell f√ºr die Analyse von Kryptow√§hrungs-Diskussionen entwickelt. Es basiert auf der BERT-Architektur und klassifiziert Texte als bullish (positiv), neutral oder bearish (negativ). Das Modell wurde auf umfangreichen Finanz- und Krypto-spezifischen Daten trainiert, wodurch es sich besonders gut f√ºr die Analyse von Reddit-Kommentaren eignet, die sich mit dem Krypto-Markt befassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CryptoBERT-Modell laden\n",
    "MODEL_NAME = \"ElKulako/crypto-bert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# üîπ Sicherstellen, dass die Spalte \"body\" existiert\n",
    "if \"body\" not in df_comments_clean.columns:\n",
    "    raise ValueError(\"Fehler: Die CSV-Datei enth√§lt keine 'body'-Spalte mit Kommentaren!\")\n",
    "\n",
    "# üîπ Funktion zur Sentiment-Analyse mit CryptoBERT\n",
    "def analyze_sentiment(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return \"neutral\", 0.0  # Leere Kommentare sind neutral\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    scores = F.softmax(outputs.logits, dim=1)[0]\n",
    "    labels = [\"bearish\", \"neutral\", \"bullish\"]  # CryptoBERT nutzt diese Labels\n",
    "    sentiment = labels[torch.argmax(scores).item()]\n",
    "    confidence = scores.max().item()\n",
    "\n",
    "    return sentiment, confidence\n",
    "\n",
    "# üîπ Sentiment f√ºr alle Kommentare berechnen\n",
    "df_comments_clean[\"sentiment\"], df_comments_clean[\"sentiment_confidence\"] = zip(*df_comments_clean[\"body\"].map(analyze_sentiment))\n",
    "\n",
    "# üîπ Debug-Ausgabe: Zeigt die ersten 5 Ergebnisse zur √úberpr√ºfung\n",
    "print(df_comments_clean[[\"body\", \"sentiment\", \"sentiment_confidence\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem die Sentiment-Analyse durchgef√ºhrt wurde, werden die bereinigten Posts und Kommentare zusammengef√ºhrt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging der Posts & Kommentare direkt nach der Bereinigung\n",
    "df_merged = df_comments_clean.merge(df_posts_clean, on=\"post_id\", how=\"left\")\n",
    "\n",
    "# Fehlende Werte entfernen (optional)\n",
    "df_merged.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieser Schritt stellt sicher, dass alle relevanten Kommentare mit ihren zugeh√∂rigen Posts verkn√ºpft sind sodass wir neben den normalisierten Post- und Kommentar-Tabelle eine fertige Tabelle f√ºr Analysen haben.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export und Speicherung der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zweck der Speicherung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die exportierten Daten enthalten die gesammelten Reddit-Posts und Kommentare sowie deren Sentiment-Analyse. Durch die Speicherung in **Google Drive** wird eine einfache Automatisierung erm√∂glicht, sodass die Daten regelm√§√üig aktualisiert und f√ºr nachfolgende Analysen oder Machine-Learning-Prozesse verf√ºgbar sind.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementierung des Exports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den Pfad definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setze den Pfad zu deinem Google Drive Ordner\n",
    "DRIVE_PATH = \"G:/Meine Ablage/reddit/\"\n",
    "POSTS_CSV = os.path.join(DRIVE_PATH, \"reddit_posts.csv\")\n",
    "COMMENTS_CSV = os.path.join(DRIVE_PATH, \"reddit_comments.csv\")\n",
    "MERGED_CSV = os.path.join(DRIVE_PATH, \"reddit_merged.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende Funktion speichert die extrahierten und verarbeiteten Daten als CSV-Dateien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_initial_csv(df_new, filename):\n",
    "    \"\"\"Speichert die erste CSV-Datei ohne Anh√§ngen oder Duplikat-Pr√ºfung.\"\"\"\n",
    "    file_path = os.path.join(DRIVE_PATH, filename)\n",
    "    try:\n",
    "        df_new.to_csv(file_path, index=False, sep=\"|\", encoding=\"utf-8-sig\", lineterminator=\"\\n\")\n",
    "        print(f\"‚úÖ Datei erfolgreich gespeichert: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Speichern der Datei {filename}: {e}\")\n",
    "\n",
    "def export_initial_data(df_posts, df_comments, df_merged):\n",
    "    \"\"\"Speichert die initialen Posts, Kommentare & gemergten Daten.\"\"\"\n",
    "    try:\n",
    "        save_initial_csv(df_posts, \"reddit_posts.csv\")\n",
    "        save_initial_csv(df_comments, \"reddit_comments.csv\")\n",
    "        save_initial_csv(df_merged, \"reddit_merged.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Export: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export-Funktion f√ºr den ersten Scrape aufrufen\n",
    "export_initial_data(df_posts_clean, df_comments_clean, df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vorteile der Speicherung in Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Automatisierung**: Die exportierten Daten k√∂nnen regelm√§√üig durch Skripte oder Jenkins-Jobs aktualisiert werden.\n",
    "- **Zug√§nglichkeit**: Die gespeicherten Dateien k√∂nnen von verschiedenen Systemen oder Nutzern f√ºr Analysen oder Machine-Learning-Modelle verwendet werden.\n",
    "- **Versionskontrolle**: Historische Daten k√∂nnen gespeichert werden, um Entwicklungen √ºber die Zeit hinweg zu analysieren.\n",
    "\n",
    "Dieser Schritt stellt sicher, dass die verarbeiteten Daten langfristig verf√ºgbar bleiben und kontinuierlich f√ºr weitere Analysen genutzt werden k√∂nnen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Automatisierung mit Jenkins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einrichtung von Jenkins auf Ubuntu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um den Scraping- und Analyseprozess zu automatisieren, wird Jenkins auf einem Ubuntu-Server eingerichtet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Installation von Jenkins auf Ubuntu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ```bash\n",
    "   sudo apt update\n",
    "   sudo apt install openjdk-11-jre\n",
    "   wget -q -O - https://pkg.jenkins.io/debian/jenkins.io.key | sudo apt-key add -\n",
    "   sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ > /etc/apt/sources.list.d/jenkins.list'\n",
    "   sudo apt update\n",
    "   sudo apt install jenkins\n",
    "   sudo systemctl start jenkins\n",
    "   sudo systemctl enable jenkins\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der Installation ist Jenkins unter `http://localhost:8080` erreichbar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Erstellen eines neuen Jobs in Jenkins**\n",
    "   - √ñffne die Jenkins Web-Oberfl√§che.\n",
    "   - Erstelle einen neuen **Freestyle-Projekt**-Job.\n",
    "   - F√ºge den **GitHub Personal Access Token** ein, um Zugriff auf das Repository zu erhalten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Konfiguration des Build-Schritts**\n",
    "   Der folgende Shell-Befehl wird in den Build-Schritten des Jenkins-Jobs hinzugef√ºgt, um das Notebook auszuf√ºhren:\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aktiviere das Python-Environment\n",
    ". /var/lib/jenkins/venv/bin/activate\n",
    "\n",
    "# Lade die .env-Datei und setze die Variablen\n",
    "set -a\n",
    ". /var/lib/jenkins/workspace/reddit_crypto_scraper/.env\n",
    "set +a\n",
    "\n",
    "# Debugging: Pr√ºfe, ob CLIENT_ID gesetzt wurde\n",
    "echo \"CLIENT_ID = $CLIENT_ID\"\n",
    "\n",
    "# Installiere Abh√§ngigkeiten aus requirements.txt\n",
    "pip install -r /var/lib/jenkins/workspace/reddit_crypto_scraper/requirements.txt\n",
    "\n",
    "# F√ºhre das Notebook mit papermill aus\n",
    "papermill /var/lib/jenkins/workspace/reddit_crypto_scraper/notebooks/reddit-skript.ipynb /var/lib/jenkins/workspace/reddit_crypto_scraper/notebooks/output.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorteile der Jenkins-Automatisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Regelm√§√üige Ausf√ºhrung**: Jenkins kann so konfiguriert werden, dass das Skript automatisch t√§glich oder w√∂chentlich ausgef√ºhrt wird.\n",
    "- **Monitoring & Logging**: Jenkins speichert Logs jeder Ausf√ºhrung und erm√∂glicht eine Fehleranalyse.\n",
    "- **Reproduzierbarkeit**: Durch das Laden der `.env`-Datei und die Installation aller Abh√§ngigkeiten ist jede Ausf√ºhrung identisch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit dieser Konfiguration ist die gesamte Sentiment-Analyse vollst√§ndig automatisiert und kann kontinuierlich aktualisiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Analyse und Dashboard mit Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einrichtung der Streamlit-App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streamlit ist ein einfaches Framework zur Erstellung interaktiver Dashboards mit Python. Die folgende Anwendung visualisiert die analysierten Sentiment-Daten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation der ben√∂tigten Pakete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```bash\n",
    "pip install streamlit pandas gdown matplotlib seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufbau der Streamlit-App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende **`app.py`** Datei enth√§lt das vollst√§ndige Dashboard f√ºr die Visualisierung der analysierten Reddit-Daten:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import gdown\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "st.set_page_config(page_title=\"Krypto-Sentiment Dashboard\", layout=\"centered\")\n",
    "\n",
    "MERGED_CSV_ID = \"102W-f_u58Jvx9xBAv4IaYrOY6txk-XKL\"\n",
    "MERGED_CSV = \"reddit_merged.csv\"\n",
    "\n",
    "@st.cache_data\n",
    "def download_csv(file_id, output):\n",
    "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "    gdown.download(url, output, quiet=False)\n",
    "\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    if not os.path.exists(MERGED_CSV):\n",
    "        download_csv(MERGED_CSV_ID, MERGED_CSV)\n",
    "    df = pd.read_csv(MERGED_CSV, sep=\"|\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df[\"sentiment_score\"] = df[\"sentiment\"].map({\"positive\": 1, \"neutral\": 0, \"negative\": -1})\n",
    "    return df\n",
    "\n",
    "df_merged = load_data()\n",
    "\n",
    "st.title(\"üìä Krypto-Sentiment Dashboard\")\n",
    "\n",
    "if df_merged.empty:\n",
    "    st.warning(\"‚ö†Ô∏è Keine Daten verf√ºgbar. √úberpr√ºfe Google Drive oder lade neue Daten hoch.\")\n",
    "else:\n",
    "    st.subheader(\"üî• Top 10 meist erw√§hnte Kryptow√§hrungen\")\n",
    "    crypto_counts = df_merged[\"crypto\"].value_counts().head(10)\n",
    "    st.bar_chart(crypto_counts)\n",
    "\n",
    "    st.subheader(\"üí° Sentiment-Verteilung der Coins\")\n",
    "    sentiment_distribution = df_merged.groupby([\"crypto\", \"sentiment\"]).size().unstack(fill_value=0)\n",
    "    st.bar_chart(sentiment_distribution)\n",
    "\n",
    "    st.subheader(\"üìà Verh√§ltnis Positiv vs. Negativ\")\n",
    "    sentiment_ratio = df_merged[df_merged[\"sentiment\"] != \"neutral\"].groupby(\"sentiment\").size()\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.pie(sentiment_ratio, labels=sentiment_ratio.index, autopct=\"%1.1f%%\", startangle=90, colors=[\"green\", \"red\"])\n",
    "    ax.axis(\"equal\")\n",
    "    st.pyplot(fig)\n",
    "\n",
    "    st.subheader(\"üìÖ Sentiment-Entwicklung √ºber Zeit\")\n",
    "    crypto_options = df_merged[\"crypto\"].unique().tolist()\n",
    "    selected_crypto = st.selectbox(\"W√§hle eine Kryptow√§hrung:\", crypto_options, index=0)\n",
    "    df_filtered = df_merged[(df_merged[\"crypto\"] == selected_crypto) & (df_merged[\"sentiment\"] != \"neutral\")]\n",
    "    df_time = df_filtered.groupby([\"date\", \"sentiment\"]).size().unstack(fill_value=0)\n",
    "    st.line_chart(df_time)\n",
    "\n",
    "st.write(\"üîÑ Dashboard wird regelm√§√üig mit neuen Daten aktualisiert!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starten der Streamlit-App\n",
    "Die App kann mit folgendem Befehl gestartet werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```bash\n",
    "streamlit run app.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M√∂glichkeiten zur Erweiterung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Anwendung kann um weitere **Visualisierungen und Features** erg√§nzt werden:\n",
    "- **Erweiterte Sentiment-Trends**: Liniendiagramme f√ºr Langzeitanalysen.\n",
    "- **Korrelationen zwischen Coins**: Heatmaps zur Analyse von Korrelationen zwischen Kryptow√§hrungen.\n",
    "- **Interaktive Benutzersteuerung**: Mehr Auswahlm√∂glichkeiten f√ºr den Nutzer, wie Filter f√ºr bestimmte Zeitr√§ume oder Sentiment-Typen.\n",
    "- **Integration weiterer Datenquellen**: Kombination mit Twitter-Daten oder Finanzmarktdaten zur besseren Analyse der Marktentwicklung.\n",
    "\n",
    "Durch die Nutzung von **Matplotlib, Seaborn und Streamlit** stehen zahlreiche M√∂glichkeiten f√ºr kreative Datenvisualisierung zur Verf√ºgung. Entwickler k√∂nnen das Dashboard kontinuierlich verbessern und an neue Anforderungen anpassen.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dennis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

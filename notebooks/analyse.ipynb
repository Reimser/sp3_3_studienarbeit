{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Datei geladen: G:/Meine Ablage/reddit/reddit_posts_original.csv\n",
      "üîπ Spalten: ['post_id', 'crypto', 'search_term', 'subreddit', 'title', 'author', 'date', 'time', 'score', 'num_comments', 'selftext']\n",
      "post_id         object\n",
      "crypto          object\n",
      "search_term     object\n",
      "subreddit       object\n",
      "title           object\n",
      "author          object\n",
      "date            object\n",
      "time            object\n",
      "score            int64\n",
      "num_comments     int64\n",
      "selftext        object\n",
      "dtype: object\n",
      "   post_id    crypto search_term       subreddit  \\\n",
      "0  1irsm07  Ethereum    ethereum  CryptoCurrency   \n",
      "1  1irr9h8  Ethereum    ethereum  CryptoCurrency   \n",
      "2  1ir3avz  Ethereum    ethereum  CryptoCurrency   \n",
      "3  1ir2i1r  Ethereum    ethereum  CryptoCurrency   \n",
      "4  1ir0uz0  Ethereum    ethereum  CryptoCurrency   \n",
      "\n",
      "                                               title                author  \\\n",
      "0  Urgent Ethereum Geth patch addresses Merge ove...             Afonsoo99   \n",
      "1  Banking Giant JPMorgan Chase Holds $1,016,728 ...                 KIG45   \n",
      "2  World Liberty Finance Wallet ‚Äì Stacking ETH & ...  Educational-Hand6427   \n",
      "3  BlackRock holding over $60 Billion in Crypto -...        CriticalCobraz   \n",
      "4       Ethereum (ETH) Hodler Finally Sleeps In 2025             kirtash93   \n",
      "\n",
      "         date      time  score  num_comments  \\\n",
      "0  2025-02-17  19:36:44      5             2   \n",
      "1  2025-02-17  18:43:56    192            45   \n",
      "2  2025-02-16  21:36:19      6             8   \n",
      "3  2025-02-16  21:02:09    348            34   \n",
      "4  2025-02-16  19:52:54    228            21   \n",
      "\n",
      "                                            selftext  \n",
      "0                                                NaN  \n",
      "1                                                NaN  \n",
      "2  World Liberty Fi has been stacking a lot of Et...  \n",
      "3                                                NaN  \n",
      "4                                                NaN  \n",
      "\n",
      "üìå Datei geladen: G:/Meine Ablage/reddit/reddit_comments_original.csv\n",
      "üîπ Spalten: ['post_id', 'comment_id', 'author', 'date', 'time', 'score', 'selftext']\n",
      "post_id       object\n",
      "comment_id    object\n",
      "author        object\n",
      "date          object\n",
      "time          object\n",
      "score          int64\n",
      "selftext      object\n",
      "dtype: object\n",
      "   post_id comment_id           author        date      time  score  \\\n",
      "0  1irsm07    mdbfesi  CipherScarlatti  2025-02-17  21:15:08      2   \n",
      "1  1irsm07    mdax641    coinfeeds-bot  2025-02-17  19:50:04      1   \n",
      "2  1irr9h8    mdalcx4        Nighmarez  2025-02-17  18:55:58    195   \n",
      "3  1irr9h8    mdapdan     athomasflynn  2025-02-17  19:14:13     66   \n",
      "4  1irr9h8    mdas66c         partymsl  2025-02-17  19:27:00     31   \n",
      "\n",
      "                                            selftext  \n",
      "0                                 Price goes up now?  \n",
      "1  tldr; Geth version 1.15.2 has been released to...  \n",
      "2                               A whole million wow‚Ä¶  \n",
      "3  $1M isn't statistically much more than $0 when...  \n",
      "4             $1M is like transaction fees for them.  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import os\n",
    "import psaw as ps\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from psaw import PushshiftAPI\n",
    "from praw.exceptions import APIException\n",
    "\n",
    "# üìå Pfade zu den CSV-Dateien\n",
    "DRIVE_PATH = \"G:/Meine Ablage/reddit/\"\n",
    "POSTS_CSV = os.path.join(DRIVE_PATH, \"reddit_posts_original.csv\")\n",
    "COMMENTS_CSV = os.path.join(DRIVE_PATH, \"reddit_comments_original.csv\")\n",
    "\n",
    "# üîπ **Funktion zum Laden der CSV-Dateien**\n",
    "def load_csv(filepath):\n",
    "    \"\"\"L√§dt eine CSV-Datei mit `|` als Trennzeichen und Debugging-Infos\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"‚ùå Datei nicht gefunden: {filepath}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.read_csv(filepath, sep=\"|\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "\n",
    "    print(f\"\\nüìå Datei geladen: {filepath}\")\n",
    "    print(f\"üîπ Spalten: {df.columns.tolist()}\")\n",
    "    print(df.dtypes)\n",
    "    print(df.head())\n",
    "\n",
    "    return df\n",
    "\n",
    "# üìå **Daten laden**\n",
    "df_posts = load_csv(POSTS_CSV)\n",
    "df_comments = load_csv(COMMENTS_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Daten aufbereitet: 6754 Posts & 264029 Kommentare √ºbrig.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Erstelle die \"full_text\"-Spalte f√ºr Posts & Kommentare\n",
    "df_posts[\"full_text\"] = df_posts[\"title\"] + \" \" + df_posts[\"selftext\"].fillna(\"\")\n",
    "df_comments[\"full_text\"] = df_comments[\"selftext\"].fillna(\"\")\n",
    "\n",
    "# Entferne sehr kurze Texte (< 10 Zeichen) und doppelte Eintr√§ge\n",
    "df_posts = df_posts[df_posts[\"full_text\"].str.len() > 10].drop_duplicates(subset=[\"full_text\"])\n",
    "df_comments = df_comments[df_comments[\"full_text\"].str.len() > 10].drop_duplicates(subset=[\"full_text\"])\n",
    "\n",
    "print(f\"‚úÖ Daten aufbereitet: {len(df_posts)} Posts & {len(df_comments)} Kommentare √ºbrig.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Verwende Ger√§t: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# üîπ GPU nutzen, falls verf√ºgbar\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Verwende Ger√§t: {device}\")\n",
    "\n",
    "# üîπ CryptoBERT-Modell laden\n",
    "MODEL_NAME = \"ElKulako/cryptobert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()  # Setzt das Modell in den Evaluationsmodus\n",
    "\n",
    "# üîπ Funktion zur Sentiment-Analyse (Optimiert f√ºr Batch-Prozesse)\n",
    "def analyze_sentiment_batch(texts, batch_size=32):\n",
    "    \"\"\"Effiziente GPU-gest√ºtzte Sentiment-Analyse mit CryptoBERT f√ºr eine Liste von Texten.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Ersetze leere Eintr√§ge durch \"neutral\"\n",
    "    texts = [t if isinstance(t, str) and t.strip() != \"\" else \"neutral\" for t in texts]\n",
    "\n",
    "    # Batchweise Verarbeitung\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"üîç Analysiere Sentiments\"):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "\n",
    "        # Tokenisierung (mit Padding f√ºr Performance)\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, max_length=512, padding=True).to(device)\n",
    "\n",
    "        # Vorhersage mit CryptoBERT\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        scores = F.softmax(outputs.logits, dim=1)\n",
    "        labels = [\"bearish\", \"neutral\", \"bullish\"] \n",
    "\n",
    "        # Ergebnisse speichern\n",
    "        for i in range(len(batch_texts)):\n",
    "            sentiment = labels[torch.argmax(scores[i]).item()]\n",
    "            confidence = scores[i].max().item()\n",
    "            results.append((sentiment, confidence))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîç Analysiere Sentiments: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 212/212 [01:54<00:00,  1.85it/s]\n",
      "üîç Analysiere Sentiments: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8251/8251 [42:59<00:00,  3.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sentiment-Analyse abgeschlossen!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sentiment f√ºr Posts berechnen\n",
    "df_posts[[\"sentiment\", \"sentiment_confidence\"]] = pd.DataFrame(\n",
    "    analyze_sentiment_batch(df_posts[\"full_text\"].tolist()), index=df_posts.index\n",
    ")\n",
    "\n",
    "# Sentiment f√ºr Kommentare berechnen\n",
    "df_comments[[\"sentiment\", \"sentiment_confidence\"]] = pd.DataFrame(\n",
    "    analyze_sentiment_batch(df_comments[\"full_text\"].tolist()), index=df_comments.index\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Sentiment-Analyse abgeschlossen!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bereinigung abgeschlossen: 3884 Posts & 156345 Kommentare nach Filtering.\n"
     ]
    }
   ],
   "source": [
    "df_posts = df_posts[df_posts[\"sentiment_confidence\"] >= 0.6]\n",
    "df_comments = df_comments[df_comments[\"sentiment_confidence\"] >= 0.6]\n",
    "\n",
    "print(f\"‚úÖ Bereinigung abgeschlossen: {len(df_posts)} Posts & {len(df_comments)} Kommentare nach Filtering.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Daten erfolgreich vorbereitet und tokenisiert!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Labels in numerische Werte umwandeln\n",
    "label_mapping = {\"bearish\": 0, \"neutral\": 1, \"bullish\": 2}\n",
    "df_posts[\"label\"] = df_posts[\"sentiment\"].map(label_mapping)\n",
    "df_comments[\"label\"] = df_comments[\"sentiment\"].map(label_mapping)\n",
    "\n",
    "# Trainings- und Validierungssets erstellen (80% Train, 20% Test)\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df_posts[\"full_text\"].tolist(), df_posts[\"label\"].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenizer laden\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ElKulako/cryptobert\")\n",
    "\n",
    "# Tokenisierung der Daten\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "print(\"‚úÖ Daten erfolgreich vorbereitet und tokenisiert!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PyTorch Dataset erfolgreich erstellt!\n"
     ]
    }
   ],
   "source": [
    "class CryptoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# Erstelle Trainings- und Validierungs-Datasets\n",
    "train_dataset = CryptoDataset(train_encodings, train_labels)\n",
    "val_dataset = CryptoDataset(val_encodings, val_labels)\n",
    "\n",
    "print(\"‚úÖ PyTorch Dataset erfolgreich erstellt!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElKulako/cryptobert\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Trainingsparameter\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./logs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[0;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Modell und Trainingsparameter gesetzt!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m<string>:134\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\dennis\\lib\\site-packages\\transformers\\training_args.py:1772\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1770\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[0;32m   1771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[1;32m-> 1772\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_tokens_across_devices:\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\dennis\\lib\\site-packages\\transformers\\training_args.py:2294\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2290\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2291\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   2292\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2293\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 2294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\dennis\\lib\\site-packages\\transformers\\utils\\generic.py:62\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     60\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\dennis\\lib\\site-packages\\transformers\\training_args.py:2167\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   2166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 2167\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   2168\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2169\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2170\u001b[0m         )\n\u001b[0;32m   2171\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[0;32m   2172\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# CryptoBERT mit drei Klassifikations-Labels laden\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ElKulako/cryptobert\", num_labels=3).to(\"cuda\")\n",
    "\n",
    "# Trainingsparameter\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Modell und Trainingsparameter gesetzt!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available\n",
      "GPU is available: NVIDIA RTX A5000 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU available\")\n",
    "else:\n",
    "    print(\"GPU not available\")\n",
    "\n",
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"GPU is available:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"GPU not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA verf√ºgbar: True\n",
      "CUDA Version: 12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA verf√ºgbar:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dennis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Datei geladen: G:/Meine Ablage/reddit/reddit_posts_original.csv\n",
      "üîπ Spalten: ['post_id', 'crypto', 'search_term', 'subreddit', 'title', 'author', 'date', 'time', 'score', 'num_comments', 'selftext']\n",
      "post_id         object\n",
      "crypto          object\n",
      "search_term     object\n",
      "subreddit       object\n",
      "title           object\n",
      "author          object\n",
      "date            object\n",
      "time            object\n",
      "score            int64\n",
      "num_comments     int64\n",
      "selftext        object\n",
      "dtype: object\n",
      "   post_id    crypto search_term       subreddit  \\\n",
      "0  1irsm07  Ethereum    ethereum  CryptoCurrency   \n",
      "1  1irr9h8  Ethereum    ethereum  CryptoCurrency   \n",
      "2  1ir3avz  Ethereum    ethereum  CryptoCurrency   \n",
      "3  1ir2i1r  Ethereum    ethereum  CryptoCurrency   \n",
      "4  1ir0uz0  Ethereum    ethereum  CryptoCurrency   \n",
      "\n",
      "                                               title                author  \\\n",
      "0  Urgent Ethereum Geth patch addresses Merge ove...             Afonsoo99   \n",
      "1  Banking Giant JPMorgan Chase Holds $1,016,728 ...                 KIG45   \n",
      "2  World Liberty Finance Wallet ‚Äì Stacking ETH & ...  Educational-Hand6427   \n",
      "3  BlackRock holding over $60 Billion in Crypto -...        CriticalCobraz   \n",
      "4       Ethereum (ETH) Hodler Finally Sleeps In 2025             kirtash93   \n",
      "\n",
      "         date      time  score  num_comments  \\\n",
      "0  2025-02-17  19:36:44      5             2   \n",
      "1  2025-02-17  18:43:56    192            45   \n",
      "2  2025-02-16  21:36:19      6             8   \n",
      "3  2025-02-16  21:02:09    348            34   \n",
      "4  2025-02-16  19:52:54    228            21   \n",
      "\n",
      "                                            selftext  \n",
      "0                                                NaN  \n",
      "1                                                NaN  \n",
      "2  World Liberty Fi has been stacking a lot of Et...  \n",
      "3                                                NaN  \n",
      "4                                                NaN  \n",
      "\n",
      "üìå Datei geladen: G:/Meine Ablage/reddit/reddit_comments_original.csv\n",
      "üîπ Spalten: ['post_id', 'comment_id', 'author', 'date', 'time', 'score', 'selftext']\n",
      "post_id       object\n",
      "comment_id    object\n",
      "author        object\n",
      "date          object\n",
      "time          object\n",
      "score          int64\n",
      "selftext      object\n",
      "dtype: object\n",
      "   post_id comment_id           author        date      time  score  \\\n",
      "0  1irsm07    mdbfesi  CipherScarlatti  2025-02-17  21:15:08      2   \n",
      "1  1irsm07    mdax641    coinfeeds-bot  2025-02-17  19:50:04      1   \n",
      "2  1irr9h8    mdalcx4        Nighmarez  2025-02-17  18:55:58    195   \n",
      "3  1irr9h8    mdapdan     athomasflynn  2025-02-17  19:14:13     66   \n",
      "4  1irr9h8    mdas66c         partymsl  2025-02-17  19:27:00     31   \n",
      "\n",
      "                                            selftext  \n",
      "0                                 Price goes up now?  \n",
      "1  tldr; Geth version 1.15.2 has been released to...  \n",
      "2                               A whole million wow‚Ä¶  \n",
      "3  $1M isn't statistically much more than $0 when...  \n",
      "4             $1M is like transaction fees for them.  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import os\n",
    "import psaw as ps\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from psaw import PushshiftAPI\n",
    "from praw.exceptions import APIException\n",
    "\n",
    "# üìå Pfade zu den CSV-Dateien\n",
    "DRIVE_PATH = \"G:/Meine Ablage/reddit/\"\n",
    "POSTS_CSV = os.path.join(DRIVE_PATH, \"reddit_posts_original.csv\")\n",
    "COMMENTS_CSV = os.path.join(DRIVE_PATH, \"reddit_comments_original.csv\")\n",
    "\n",
    "# üîπ **Funktion zum Laden der CSV-Dateien**\n",
    "def load_csv(filepath):\n",
    "    \"\"\"L√§dt eine CSV-Datei mit `|` als Trennzeichen und Debugging-Infos\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"‚ùå Datei nicht gefunden: {filepath}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.read_csv(filepath, sep=\"|\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "\n",
    "    print(f\"\\nüìå Datei geladen: {filepath}\")\n",
    "    print(f\"üîπ Spalten: {df.columns.tolist()}\")\n",
    "    print(df.dtypes)\n",
    "    print(df.head())\n",
    "\n",
    "    return df\n",
    "\n",
    "# üìå **Daten laden**\n",
    "df_posts = load_csv(POSTS_CSV)\n",
    "df_comments = load_csv(COMMENTS_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Daten aufbereitet: 6754 Posts & 264029 Kommentare √ºbrig.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Erstelle die \"full_text\"-Spalte f√ºr Posts & Kommentare\n",
    "df_posts[\"full_text\"] = df_posts[\"title\"] + \" \" + df_posts[\"selftext\"].fillna(\"\")\n",
    "df_comments[\"full_text\"] = df_comments[\"selftext\"].fillna(\"\")\n",
    "\n",
    "# Entferne sehr kurze Texte (< 10 Zeichen) und doppelte Eintr√§ge\n",
    "df_posts = df_posts[df_posts[\"full_text\"].str.len() > 10].drop_duplicates(subset=[\"full_text\"])\n",
    "df_comments = df_comments[df_comments[\"full_text\"].str.len() > 10].drop_duplicates(subset=[\"full_text\"])\n",
    "\n",
    "print(f\"‚úÖ Daten aufbereitet: {len(df_posts)} Posts & {len(df_comments)} Kommentare √ºbrig.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Verwende Ger√§t: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# üîπ GPU nutzen, falls verf√ºgbar\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Verwende Ger√§t: {device}\")\n",
    "\n",
    "# üîπ CryptoBERT-Modell laden\n",
    "MODEL_NAME = \"ElKulako/cryptobert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()  # Setzt das Modell in den Evaluationsmodus\n",
    "\n",
    "# üîπ Funktion zur Sentiment-Analyse (Optimiert f√ºr Batch-Prozesse)\n",
    "def analyze_sentiment_batch(texts, batch_size=32):\n",
    "    \"\"\"Effiziente GPU-gest√ºtzte Sentiment-Analyse mit CryptoBERT f√ºr eine Liste von Texten.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Ersetze leere Eintr√§ge durch \"neutral\"\n",
    "    texts = [t if isinstance(t, str) and t.strip() != \"\" else \"neutral\" for t in texts]\n",
    "\n",
    "    # Batchweise Verarbeitung\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"üîç Analysiere Sentiments\"):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "\n",
    "        # Tokenisierung (mit Padding f√ºr Performance)\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, max_length=512, padding=True).to(device)\n",
    "\n",
    "        # Vorhersage mit CryptoBERT\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        scores = F.softmax(outputs.logits, dim=1)\n",
    "        labels = [\"bearish\", \"neutral\", \"bullish\"] \n",
    "\n",
    "        # Ergebnisse speichern\n",
    "        for i in range(len(batch_texts)):\n",
    "            sentiment = labels[torch.argmax(scores[i]).item()]\n",
    "            confidence = scores[i].max().item()\n",
    "            results.append((sentiment, confidence))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîç Analysiere Sentiments: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 212/212 [01:54<00:00,  1.85it/s]\n",
      "üîç Analysiere Sentiments: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8251/8251 [42:59<00:00,  3.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sentiment-Analyse abgeschlossen!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sentiment f√ºr Posts berechnen\n",
    "df_posts[[\"sentiment\", \"sentiment_confidence\"]] = pd.DataFrame(\n",
    "    analyze_sentiment_batch(df_posts[\"full_text\"].tolist()), index=df_posts.index\n",
    ")\n",
    "\n",
    "# Sentiment f√ºr Kommentare berechnen\n",
    "df_comments[[\"sentiment\", \"sentiment_confidence\"]] = pd.DataFrame(\n",
    "    analyze_sentiment_batch(df_comments[\"full_text\"].tolist()), index=df_comments.index\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Sentiment-Analyse abgeschlossen!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available\n",
      "GPU is available: NVIDIA RTX A5000 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU available\")\n",
    "else:\n",
    "    print(\"GPU not available\")\n",
    "\n",
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"GPU is available:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"GPU not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dennis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

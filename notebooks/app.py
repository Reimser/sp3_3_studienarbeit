import streamlit as st
import pandas as pd
import gdown
import os
import matplotlib.pyplot as plt
import seaborn as sns

# ğŸ“Œ Streamlit Page Configuration
st.set_page_config(page_title="Reddit Crypto Dashboard", layout="wide")

# ğŸ“¥ **Google Drive File IDs**
MERGED_CRYPTO_CSV_ID = "12ugApKWh1cJYONcLanpHND9gIdobh-wA"
CRYPTO_PRICES_CSV_ID = "11k9wiflOkqg2DayEgn7iPqNPHC5Qatht"

# ğŸ“Œ **Lokale Dateinamen**
MERGED_CRYPTO_CSV = "app.csv"
CRYPTO_PRICES_CSV = "crypto_prices.csv"

# ğŸ”¥ **Daten herunterladen**
def download_csv(file_id, output):
    url = f"https://drive.google.com/uc?export=download&id={file_id}"
    try:
        gdown.download(url, output, quiet=False, fuzzy=True)  # âœ… Alternative Download-Methode
        if not os.path.exists(output) or os.path.getsize(output) == 0:
            st.error(f"âŒ Fehler: {output} wurde nicht korrekt heruntergeladen!")
    except Exception as e:
        st.error(f"âŒ Download fehlgeschlagen: {str(e)}")

# ğŸ“¥ **Daten laden**
download_csv(MERGED_CRYPTO_CSV_ID, MERGED_CRYPTO_CSV)
download_csv(CRYPTO_PRICES_CSV_ID, CRYPTO_PRICES_CSV)

# ğŸ“Œ **CSV-Dateien einlesen**
def load_csv(filepath):
    if not os.path.exists(filepath):
        st.error(f"âŒ Datei nicht gefunden: {filepath}")
        return pd.DataFrame()
    try:
        return pd.read_csv(filepath, sep="|", encoding="utf-8-sig", on_bad_lines="skip")
    except Exception as e:
        st.error(f"âŒ Fehler beim Laden von {filepath}: {str(e)}")
        return pd.DataFrame()

df_crypto = load_csv(MERGED_CRYPTO_CSV)
df_prices = load_csv(CRYPTO_PRICES_CSV)

# ğŸ”¹ Datentypen korrigieren
df_crypto["date"] = pd.to_datetime(df_crypto["date"], format="%Y-%m-%d", errors="coerce")
df_crypto["crypto"] = df_crypto["crypto"].astype(str)
df_crypto["sentiment"] = df_crypto["sentiment"].astype(str)

# ğŸ” Debugging: Dtypes prÃ¼fen
print(df_crypto.dtypes)
print(df_crypto.head())


# ğŸ“Š **Multi-Tab Navigation**
tab_home, tab_crypto, tab_stocks = st.tabs(["ğŸ  Home", "ğŸ“ˆ Crypto Data", "ğŸ’¹ Stock Data"])

# ğŸ”¹ **ğŸ  HOME (README)**
with tab_home:
    st.title("ğŸ“Š Reddit Financial Sentiment Dashboard")
    st.markdown("""
        ## ğŸ” Project Overview
        This dashboard provides a **data-driven analysis of cryptocurrency sentiment** using **Reddit discussions** and **historical price data** starting from November 2024. The project integrates multiple data sources to explore the relationship between social sentiment and market trends.

        ### ğŸ“Š **Data Sources & Processing**
        - **Reddit Comments & Posts:** Scraped weekly from multiple subreddits using a **custom Reddit scraper**.  
        - **Sentiment Analysis:** Applied **CryptoBERT** for a **bullish-bearish-neutral classification** with confidence scores.  
        - **Historical Price Data:** Collected from **CoinGecko API** for major cryptocurrencies.  
        - **Data Storage:** Merged sentiment and price data is stored and updated weekly in **Google Drive**.

        ### ğŸ” **Key Features**
        - **ğŸ“ˆ Crypto Sentiment Analysis:**  
          - Top mentioned cryptocurrencies & sentiment distribution  
          - Sentiment trends over time (overall & high-confidence)  
          - Word count trends for selected cryptos
          - Combined analysis of sentiment & price dynamics    
        - **ğŸ’¹ Stock Market Analysis (Coming Soon)**  

        ### ğŸ”„ **Update Frequency**
        - **Reddit data & sentiment analysis:** Weekly  
        - **Crypto price data:** Weekly  

        ---
        ğŸ”¥ **Use the navigation tabs above to explore sentiment trends & price dynamics!**
    """)

# **Word Count Over Time**
st.subheader("ğŸ“ Word Count Evolution Over Time")

# Multi-Select fÃ¼r mehrere KryptowÃ¤hrungen
selected_cryptos_wordcount = st.multiselect(
    "Choose Cryptos to Compare Word Frequency:",
    df_crypto["crypto"].unique().tolist(),
    default=df_crypto["crypto"].unique()[:3]
)

if selected_cryptos_wordcount:
    df_wordcount_filtered = df_crypto[df_crypto["crypto"].isin(selected_cryptos_wordcount)]
    wordcount_per_day = df_wordcount_filtered.groupby(["date", "crypto"]).size().unstack(fill_value=0)  # âœ… FIXED
    st.line_chart(wordcount_per_day)


# **Sentiment Trend Over Time**
st.subheader("ğŸ“… Sentiment Trend Over Time")
selected_crypto = st.selectbox("Choose a Cryptocurrency for Sentiment:", df_crypto["crypto"].unique(), key="sentiment_crypto")

df_filtered = df_crypto[(df_crypto["crypto"] == selected_crypto) & (df_crypto["sentiment"] != "neutral")]

if df_filtered.empty:
    st.warning("âš ï¸ No sentiment data available for the selected cryptocurrency.")
else:
    df_time = df_filtered.groupby(["date", "sentiment"]).size().unstack(fill_value=0)  # âœ… FIXED
    st.line_chart(df_time)


# **Word Count & Price Over Time**
st.subheader("ğŸ“Š Word Count & Price Over Time")
selected_crypto_dual = st.selectbox("Choose a Cryptocurrency for Word Count & Price:", df_prices["crypto"].unique(), key="dual_axis_crypto")

df_wordcount_filtered = df_crypto[df_crypto["crypto"] == selected_crypto_dual].groupby("date").size().reset_index(name="word_count")  # âœ… FIXED
df_price_filtered = df_prices[df_prices["crypto"] == selected_crypto_dual]

# Sicherstellen, dass beide DataFrames die gleiche Zeitachse haben
df_combined_dual = df_wordcount_filtered.merge(df_price_filtered, on="date", how="inner")  # âœ… FIXED

# Visualisierung mit zwei Y-Achsen
fig, ax1 = plt.subplots(figsize=(10, 5))

ax1.set_xlabel("Date")
ax1.set_ylabel("Word Count", color="blue")
ax1.plot(df_combined_dual["date"], df_combined_dual["word_count"], color="blue", label="Word Count", alpha=0.7)
ax1.tick_params(axis="y", labelcolor="blue")

ax2 = ax1.twinx()
ax2.set_ylabel("Price (USD)", color="red")
ax2.plot(df_combined_dual["date"], df_combined_dual["price"], color="red", label="Price", alpha=0.7)
ax2.tick_params(axis="y", labelcolor="red")

fig.suptitle(f"Word Count & Price for {selected_crypto_dual} Over Time")
fig.tight_layout()
st.pyplot(fig)
        
# ğŸ”¹ **ğŸ’¹ STOCK MARKET ANALYSIS**
with tab_stocks:
    st.title("ğŸ’¹ Stock Market Analysis (Coming Soon)")
    st.warning("ğŸš§ This section is under development. Stock data will be integrated soon!")